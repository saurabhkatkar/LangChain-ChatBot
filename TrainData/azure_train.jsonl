{"id": "ef4ae6fa7b61-0", "text": "Tell us about y our PDF experience.\nOVERVIEW\nWhat is Azur e OpenAI Ser vice?QUICKSTART\nQuickstar ts\nHOW-TO GUIDE\nCreate a resour ceTUTORIAL\nEmbeddings\nHOW-TO GUIDE\nCompletionsTRAINING\nIntro to Azur e OpenAI training\nCONCEPT\nAzur e OpenAI Models\nREFERENCE\nSuppor t and help options\nAdditional resources\nAzur e OpenAI\nAzure OpenAI S tudio \nRegion support \nQuotas and limits\nApply for access to Azure OpenAI \nVideo\nCombining OpenAI models with the power of AzureAzure OpenAI Service Documentation\nLearn how to use Azure OpenAI's powerful language models including the GPT-3, Codex and\nEmbeddings model series for content generation, summarization, semantic search, and natural\nlanguage to code translation.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "772f27016adf-0", "text": "Reference\nREST API\nTerms of use \nTools\nAzure CLI\nPowerShell", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "82ce48be6561-0", "text": "What is Azure OpenAI Service?\nArticle \u202206/28/2023\nAzure OpenAI Service provides REST API access to OpenAI's powerful language models\nincluding the GPT-3, Codex and Embeddings model series. In addition, the new GPT-4\nand ChatGPT (gpt-35-turbo) model series have now reached general availability. These\nmodels can be easily adapted to your specific task including but not limited to content\ngeneration, summarization, semantic search, and natural language to code translation.\nUsers can access the service through REST APIs, Python SDK, or our web-based interface\nin the Azure OpenAI S tudio.\nFeatur e Azur e OpenAI\nModels available NEW GPT-4 series   \nGPT-3 base series  \nNEW ChatGPT (gpt -35-turbo)  \nCodex series  \nEmbeddings series  \nLearn more in our Models  page.\nFine-tuning Ada  \nBabbage  \nCurie \nCushman  \nDavinci  \nFine-tuning is curr ently unav ailable t o new cust omer s.\nPrice Available here\nVirtual network support\n& private link supportYes, unless using Azure OpenAI on your data .\nManaged Identity Yes, via Azure Active Directory\nUI experience Azur e por tal for account & resource management,  \nAzur e OpenAI Ser vice S tudio  for model exploration and fine tuning\nModel regional\navailabilityModel availability\nContent filtering Prompts and completions are evaluated against our content policy\nwith automated systems. High severity content will be filtered.Features overview", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7a09ab619a73-0", "text": "At Microsoft, we're committed to the advancement of AI driven by principles that put\npeople first. Generative models such as the ones available in Azure OpenAI have\nsignificant potential benefits, but without careful design and thoughtful mitigations,\nsuch models have the potential to generate incorrect or even harmful content. Microsoft\nhas made significant investments to help guard against abuse and unintended harm,\nwhich includes requiring applicants to show well-defined use cases, incorporating\nMicrosoft\u2019s principles for responsible AI use , building content filters to support\ncustomers, and providing responsible AI implementation guidance to onboarded\ncustomers.\nHow do I get access to Azure OpenAI?\nAccess is currently limited as we navigate high demand, upcoming product\nimprovements, and Microsoft\u2019s commitment to responsible AI . For now, we're working\nwith customers with an existing partnership with Microsoft, lower risk use cases, and\nthose committed to incorporating mitigations.\nMore specific information is included in the application form. W e appreciate your\npatience as we work to responsibly enable broader access to Azure OpenAI.\nApply here for access:\nApply now\nAzure OpenAI Service gives customers advanced language AI with OpenAI GPT-4, GPT-\n3, Codex, and D ALL-E models with the security and enterprise promise of Azure. Azure\nOpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth\ntransition from one to the other.\nWith Azure OpenAI, customers get the security capabilities of Microsoft Azure while\nrunning the same models as OpenAI. Azure OpenAI offers private networking, regional\navailability, and responsible AI content filtering.Responsible AI\nHow do I get access to Azure OpenAI?\nComparing Azure OpenAI and OpenAI\nKey concepts", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8dc1e4cb2b3a-0", "text": "The completions endpoint is the core component of the API service. This API provides\naccess to the model's text-in, text-out interface. Users simply need to provide an input\nprompt  containing the English text command, and the model will generate a text\ncompletion .\nHere's an example of a simple prompt and completion:\nPrompt : \"\"\" count to 5 in a for loop \"\"\"\nCompletion : for i in range(1, 6): print(i)\nAzure OpenAI processes text by breaking it down into tokens. T okens can be words or\njust chunks of characters. For example, the word \u201chamburger\u201d gets broken up into the\ntokens \u201cham\u201d, \u201cbur\u201d and \u201cger\u201d, while a short and common word like \u201cpear\u201d is a single\ntoken. Many tokens start with a whitespace, for example \u201c hello\u201d and \u201c bye\u201d.\nThe total number of tokens processed in a given request depends on the length of your\ninput, output and request parameters. The quantity of tokens being processed will also\naffect your response latency and throughput for the models.\nAzure OpenAI is a new product offering on Azure. Y ou can get started with Azure\nOpenAI the same way as any other Azure product where you create a resource , or\ninstance of the service, in your Azure Subscription. Y ou can read more about Azure's\nresource management design .\nOnce you create an Azure OpenAI R esource, you must deploy a model before you can\nstart making API calls and generating text. This action can be done using the\nDeployment APIs. These APIs allow you to specify the model you wish to use.\nGPT-3, GPT-3.5, and GPT-4 models from OpenAI are prompt-based. With prompt-based\nmodels, the user interacts with the model by entering a text prompt, to which the modelPrompts & completions\nTokens\nResources\nDeployments", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8dc1e4cb2b3a-1", "text": "Tokens\nResources\nDeployments\nPrompt engineering", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "04440bdb5cf7-0", "text": "responds with a text completion. This completion is the model\u2019s continuation of the\ninput text.\nWhile these models are extremely powerful, their behavior is also very sensitive to the\nprompt. This makes prompt engineering  an important skill to develop.\nPrompt construction can be difficult. In practice, the prompt acts to configure the model\nweights to complete the desired task, but it's more of an art than a science, often\nrequiring experience and intuition to craft a successful prompt.\nThe service provides users access to several different models. Each model provides a\ndifferent capability and price point.\nGPT-4 models are the latest available models. Due to high demand access to this model\nseries is currently only available by request. T o request access, existing Azure OpenAI\ncustomers can apply by filling out this form\nThe GPT-3 base models are known as Davinci, Curie, Babbage, and Ada in decreasing\norder of capability and increasing order of speed.\nThe Codex series of models is a descendant of GPT-3 and has been trained on both\nnatural language and code to power natural language to code use cases. Learn more\nabout each model on our models concept page .\nThe D ALL-E models, currently in preview, generate images from text prompts that the\nuser provides.\nLearn more about the underlying models that power Azure OpenAI .Models\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "03f991c34ce7-0", "text": "Azure OpenAI Service quotas and limits\nArticle \u202206/21/2023\nThis article contains a quick reference and a detailed description of the quotas and limits\nfor Azure OpenAI in Azure Cognitive Services.\nThe following sections provide you with a quick guide to the default quotas and limits\nthat apply to Azure OpenAI:\nLimit Name Limit V alue\nOpenAI resources per region per Azure\nsubscription30\nDefault quota per model and region (in\ntokens-per-minute)Text-Davinci-003: 120 K  \nGPT-4: 20 K  \nGPT-4-32K: 60 K \nAll others: 240 K\nDefault D ALL-E quota limits 2 concurrent requests\nMaximum prompt tokens per request Varies per model. For more information, see Azure\nOpenAI Service models\nMax fine-tuned model deployments 2\nTotal number of training jobs per resource 100\nMax simultaneous running training jobs\nper resource1\nMax training jobs queued 20\nMax Files per resource 30\nTotal size of all files per resource 1 GB\nMax training job time (job will fail if\nexceeded)720 hours\nMax training job size (tokens in training\nfile) x (# of epochs)2 Billion\n Default quota limits are subject to change.Quotas and limits reference\n1\n1", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6a96cdc7e85b-0", "text": "To minimize issues related to rate limits, it's a good idea to use the following techniques:\nImplement retry logic in your application.\nAvoid sharp changes in the workload. Increase the workload gradually.\nTest different load increase patterns.\nIncrease the quota assigned to your deployment. Move quota from another\ndeployment, if necessary.\nQuota increase requests can be submitted from the Quotas  page of Azure OpenAI\nStudio. Please note that due to overwhelming demand, we are not currently approving\nnew quota increase requests. Y our request will be queued until it can be filled at a later\ntime.\nFor other rate limits, please submit a service request .\nExplore how to manage quota  for your Azure OpenAI deployments. Learn more about\nthe underlying models that power Azure OpenAI .General best practices to remain within rate limits\nHow to request increases to the default quotas and limits\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2bf8808a433a-0", "text": "Azure OpenAI Service models\nArticle \u202206/30/2023\nAzure OpenAI provides access to many different models, grouped by family and\ncapability. A model family typically associates models by their intended task. The\nfollowing table describes model families currently available in Azure OpenAI. Not all\nmodels are available in all regions currently. R efer to the model capability table  in this\narticle for a full breakdown.\nModel\nfamilyDescr iption\nGPT-4 A set of models that improve on GPT-3.5 and can understand as well as generate\nnatural language and code.\nGPT-3 A series of models that can understand and generate natural language. This\nincludes the new ChatGPT model .\nDALL-E\n(Preview)A series of models in preview that can generate original images from natural\nlanguage.\nCodex A series of models that can understand and generate code, including translating\nnatural language to code.\nEmbeddings A set of models that can understand and use embeddings. An embedding is a\nspecial format of data representation that can be easily utilized by machine\nlearning models and algorithms. The embedding is an information dense\nrepresentation of the semantic meaning of a piece of text. Currently, we offer three\nfamilies of Embeddings models for different functionalities: similarity, text search,\nand code search.\nEach model family has a series of models that are further distinguished by capability.\nThese capabilities are typically identified by names, and the alphabetical order of these\nnames generally signifies the relative capability and cost of that model within a given\nmodel family. For example, GPT-3 models use names such as Ada, Babbage, Curie, and\nDavinci to indicate relative capability and cost. Davinci is more capable and more\nexpensive than Curie, which in turn is more capable and more expensive than Babbage,\nand so on.Model capabilities\n\uff17 Note", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "076c1e5eb61d-0", "text": "Azure OpenAI model names typically correspond to the following standard naming\nconvention:\n{capability}-{family}[-{input-type}]-{identifier}\nElement Descr iption\n{capability}The model capability of the model. For example, GPT-3 models  uses text, while\nCodex models  use code.\n{family} The relative family of the model. For example, GPT-3 models include ada,\nbabbage, curie, and davinci.\n{input-type}(Embeddings models  only) The input type of the embedding supported by the\nmodel. For example, text search embedding models support doc and query.\n{identifier}The version identifier of the model.\nFor example, our most powerful GPT-3 model is called text-davinci-003, while our\nmost powerful Codex model is called code-davinci-002.\nThe older versions of GPT-3 models named ada, babbage, curie, and davinci that\ndon't follow the standard naming convention are primarily intended for fine tuning.\nFor more information, see Learn how to customize a model for your application .\nYou can get a list of models that are available for both inference and fine-tuning by your\nAzure OpenAI resource by using the Models List API .\nAzure OpenAI now supports automatic updates for select model deployments. On\nmodels where automatic update support is available, a model version drop-down will be\nvisible in Azure OpenAI S tudio under Create new deployment  and Edit deployment :Any task that can be performed by a less capable model like Ada can be performed\nby a more capable model like Curie or Davinci.\nNaming convention\nFinding what models are available\nModel updates", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "baea1a8e0973-0", "text": "When Auto-updat e to default  is selected your model deployment will be automatically\nupdated within two weeks of a new version being released.\nIf you are still in the early testing phases for completion and chat completion based\nmodels we recommend deploying models with auto-updat e to default  set whenever it\nis available. For embeddings models while we recommend using the latest model\nversion, you should choose when you want to upgrade since embeddings generated\nwith an earlier model version will not be interchangeable with the new version.\nAs your use of Azure OpenAI evolves, and you start to build and integrate with\napplications you will likely want to manually control model updates so that you can first\ntest and validate that model performance is remaining consistent for your use case prior\nto upgrade.\n\uf80a\nAuto update to default\nSpecific model version", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b453b9977b2d-0", "text": "When you select a specific model version for a deployment this version will remain\nselected until you either choose to manually update yourself, or once you reach the\nexpiration date for the model. When the deprecation/expiration date is reached the\nmodel will auto-upgrade to the latest available version.\nThe original gpt-35-turbo (0301) and both gpt-4 (0314) models will expire no earlier\nthan October 15th, 2023. Upon expiration, deployments will automatically be upgraded\nto the default version. If you would like your deployment to stop accepting completion\nrequests rather than upgrading, then you will be able to set the model upgrade option\nto expire through the API. W e will publish guidelines on this by September 1.\nFor currently deployed models, from Azure OpenAI S tudio select Deployments :\nTo view deprecation/expiration dates for all available models in a given region from\nAzure OpenAI S tudio select Models  > Column options  > Select Depr ecation fine tune\nand Depr ecation infer ence :GPT-35-Turbo 0301 and GPT-4 0314 expiration\nViewing deprecation dates\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e1f5b70729f0-0", "text": "HTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nacountname string Required The name of your Azure OpenAI R esource.\ndeploymentName string Required The deployment name you chose when you deployed\nan existing model or the name you would like a new\nmodel deployment to have.\nresourceGroupNamestring Required The name of the associated resource group for this\nmodel deployment.\nsubscriptionId string Required Subscription ID for the associated subscription.\napi-version string Required The API version to use for this operation. This follows\nthe YYYY-MM-DD format.\nSuppor ted versions\n2023-05-01 Swagger spec\nRequest body\nThis is only a subset of the available request body parameters. For the full list of the\nparameters you can refer to the REST API spec .\nParamet er Type Descr iption\nversionUpgradeOption String Deployment model version upgrade options:  \nOnceNewDefaultVersionAvailable \nOnceCurrentVersionExpired \nNoAutoUpgrade\ncapacity integer This represents the amount of quota  you are assigning to this\ndeployment. A value of 1 equals 1,000 T okens per Minute\n(TPM)Update & deploy models via the API\nPUT \nhttps://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{\nresourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountNa\nme}/deployments/{deploymentName}?api-version=2023-05-01", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "412f3a6fb161-0", "text": "Bash\nJSONExample request\ncurl -X PUT https://management.azure.com/subscriptions/00000000-0000-0000-\n0000-000000000000/resourceGroups/resource-group-\ntemp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-\n001/deployments/text-embedding-ada-002-test-1 \" \\ \n  -H \"Content-Type: application/json \" \\ \n  -H 'Authorization: Bearer YOUR_AUTH_TOKEN' \\  \n  -d '{\" sku\":{\"name\":\"Standard \",\"capacity \":1},\"properties \": {\"model\": \n{\"format\": \"OpenAI\",\"name\": \"text-embedding-ada-002 \",\"version\": \n\"2\"},\"versionUpgradeOption \":\"OnceCurrentVersionExpired \"}}' \n\uff17 Note\nThere are multiple ways to generate an authorization token. The easiest method for\ninitial testing is to launch the Cloud Shell from https://por tal.azur e.com . Then\nrun az account get -access-t oken. You can use this token as your temporary\nauthorization token for API testing.\nExample response\n{ \n  \"id\": \"/subscriptions/{subscription-id}/resourceGroups/resource-group-\ntemp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-\n001/deployments/text-embedding-ada-002-test-1\" , \n  \"type\": \"Microsoft.CognitiveServices/accounts/deployments\" , \n  \"name\": \"text-embedding-ada-002-test-1\" , \n  \"sku\": { \n    \"name\": \"Standard\" ,\n    \"capacity\" : 1 \n  }, \n  \"properties\" : { \n    \"model\": { \n      \"format\" : \"OpenAI\" , \n      \"name\": \"text-embedding-ada-002\" , \n      \"version\" : \"2\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "412f3a6fb161-1", "text": "\"version\" : \"2\" \n    }, \n    \"versionUpgradeOption\" : \"OnceCurrentVersionExpired\" ,\n    \"capabilities\" : { \n      \"embeddings\" : \"true\", \n      \"embeddingsMaxInputs\" : \"1\" \n    }, \n    \"provisioningState\" : \"Succeeded\" , \n    \"ratelimits\" : [", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "79ed2c059bb3-0", "text": "We recommend starting with the most capable model in a model family to confirm\nwhether the model capabilities meet your requirements. Then you can stay with that\nmodel or move to a model with lower capability and cost, optimizing around that\nmodel's capabilities.\nGPT-4 can solve difficult problems with greater accuracy than any of OpenAI's previous\nmodels. Like gpt-35-turbo, GPT-4 is optimized for chat but works well for traditional\ncompletions tasks.\nDue to high demand access to this model series is currently only available by request. T o\nrequest access, existing Azure OpenAI customers can apply by filling out this form\ngpt-4\ngpt-4-32k\nThe gpt-4 supports 8192 max input tokens and the gpt-4-32k supports up to 32,768\ntokens.      { \n        \"key\": \"request\" , \n        \"renewalPeriod\" : 10, \n        \"count\": 2 \n      },  \n      { \n        \"key\": \"token\", \n        \"renewalPeriod\" : 60, \n        \"count\": 1000 \n      } \n    ] \n  }, \n  \"systemData\" : { \n    \"createdBy\" : \"docs@contoso.com\" , \n    \"createdByType\" : \"User\", \n    \"createdAt\" : \"2023-06-13T00:12:38.885937Z\" , \n    \"lastModifiedBy\" : \"docs@contoso.com\" , \n    \"lastModifiedByType\" : \"User\", \n    \"lastModifiedAt\" : \"2023-06-13T02:41:04.8410965Z\"  \n  },", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "79ed2c059bb3-1", "text": "}, \n  \"etag\": \"\\\"{GUID}\\\"\"  \n} \nFinding the right model\nGPT-4 models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "92057ba8663d-0", "text": "The GPT-3 models can understand and generate natural language. The service offers\nfour model capabilities, each with different levels of power and speed suitable for\ndifferent tasks. Davinci is the most capable model, while Ada is the fastest. In the order\nof greater to lesser capability, the models are:\ntext-davinci-003\ntext-curie-001\ntext-babbage-001\ntext-ada-001\nWhile Davinci is the most capable, the other models provide significant speed\nadvantages. Our recommendation is for users to start with Davinci while experimenting,\nbecause it produces the best results and validate the value that Azure OpenAI can\nprovide. Once you have a prototype working, you can then optimize your model choice\nwith the best latency/performance balance for your application.\nDavinci is the most capable model and can perform any task the other models can\nperform, often with less instruction. For applications requiring deep understanding of\nthe content, like summarization for a specific audience and creative content generation,\nDavinci produces the best results. The increased capabilities provided by Davinci require\nmore compute resources, so Davinci costs more and isn't as fast as other models.\nAnother area where Davinci excels is in understanding the intent of text. Davinci is\nexcellent at solving many kinds of logic problems and explaining the motives of\ncharacters. Davinci has been able to solve some of the most challenging AI problems\ninvolving cause and effect.\nUse for : Complex intent, cause and effect, summarization for audience\nCurie is powerful, yet fast. While Davinci is stronger when it comes to analyzing\ncomplicated text, Curie is capable for many nuanced tasks like sentiment classification\nand summarization. Curie is also good at answering questions and performing Q&A and\nas a general service chatbot.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "92057ba8663d-1", "text": "as a general service chatbot.\nUse for : Language translation, complex classification, text sentiment, summarizationGPT-3 models\nDavinci\nCurie", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1524e75c8f55-0", "text": "Babbage can perform straightforward tasks like simple classification. It\u2019s also capable\nwhen it comes to semantic search, ranking how well documents match up with search\nqueries.\nUse for : Moderate classification, semantic search classification\nAda is usually the fastest model and can perform tasks like parsing text, address\ncorrection and certain kinds of classification tasks that don\u2019t require too much nuance.\nAda\u2019s performance can often be improved by providing more context.\nUse for : Parsing text, simple classification, address correction, keywords\nThe ChatGPT model (gpt-35-turbo) is a language model designed for conversational\ninterfaces and the model behaves differently than previous GPT-3 models. Previous\nmodels were text-in and text-out, meaning they accepted a prompt string and returned\na completion to append to the prompt. However, the ChatGPT model is conversation-in\nand message-out. The model expects a prompt string formatted in a specific chat-like\ntranscript format, and returns a completion that represents a model-written message in\nthe chat.\nTo learn more about the ChatGPT model and how to interact with the Chat API check\nout our in-depth how-to .\nThe D ALL-E models, currently in preview, generate images from text prompts that the\nuser provides.\nThe Codex models are descendants of our base GPT-3 models that can understand and\ngenerate code. Their training data contains both natural language and billions of lines of\npublic code from GitHub.Babbage\nAda\nChatGPT (gpt-35-turbo)\nDALL-E models (Preview)\nCodex models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e3cebdddd237-0", "text": "They\u2019re most capable in Python and proficient in over a dozen languages, including C#,\nJavaScript, Go, P erl, PHP, Ruby, S wift, T ypeScript, SQL, and Shell. In the order of greater\nto lesser capability, the Codex models are:\ncode-davinci-002\ncode-cushman-001\nSimilar to GPT-3, Davinci is the most capable Codex model and can perform any task the\nother models can perform, often with less instruction. For applications requiring deep\nunderstanding of the content, Davinci produces the best results. Greater capabilities\nrequire more compute resources, so Davinci costs more and isn't as fast as other\nmodels.\nCushman is powerful, yet fast. While Davinci is stronger when it comes to analyzing\ncomplicated tasks, Cushman is a capable model for many code generation tasks.\nCushman typically runs faster and cheaper than Davinci, as well.\nCurrently, we offer three families of Embeddings models for different functionalities:\nSimilarity\nText search\nCode searchDavinci\nCushman\nEmbeddings models\n\uff09 Impor tant\nWe strongly recommend using text-embedding-ada-002 (Version 2). This\nmodel/version provides parity with OpenAI's text-embedding-ada-002. To learn\nmore about the improvements offered by this model, please refer to OpenAI' s blog\npost . Even if you are currently using V ersion 1 you should migrate to V ersion 2 to\ntake advantage of the latest weights/updated token limit. V ersion 1 and V ersion 2\nare not interchangeable, so document embedding and document search must be\ndone using the same version of the model.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9f7788977e44-0", "text": "Each family includes models across a range of capability. The following list indicates the\nlength of the numerical vector returned by the service, based on model capability:\nBase Model Model(s) Dimensions\nAda models ending in -001 (V ersion 1) 1024\nAda text-embedding-ada-002 (V ersion 2) 1536\nBabbage 2048\nCurie 4096\nDavinci 12288\nDavinci is the most capable, but is slower and more expensive than the other models.\nAda is the least capable, but is both faster and cheaper.\nThese models are good at capturing semantic similarity between two or more pieces of\ntext.\nUse cases Models\nClustering, regression, anomaly detection, visualization text-similarity-ada-001  \ntext-similarity-babbage-001  \ntext-similarity-curie-001  \ntext-similarity-davinci-001  \nThese models help measure whether long documents are relevant to a short search\nquery. There are two input types supported by this family: doc, for embedding the\ndocuments to be retrieved, and query, for embedding the search query.\nUse cases ModelsSimilarity embedding\nText search embedding", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6459e70beb70-0", "text": "Use cases Models\nSearch, context relevance, information retrieval text-search-ada-doc-001  \ntext-search-ada-query-001  \ntext-search-babbage-doc-001  \ntext-search-babbage-query-001  \ntext-search-curie-doc-001  \ntext-search-curie-query-001  \ntext-search-davinci-doc-001  \ntext-search-davinci-query-001  \nSimilar to text search embedding models, there are two input types supported by this\nfamily: code, for embedding code snippets to be retrieved, and text, for embedding\nnatural language search queries.\nUse cases Models\nCode search and relevance code-search-ada-code-001  \ncode-search-ada-text-001  \ncode-search-babbage-code-001  \ncode-search-babbage-text-001\nWhen using our embeddings models, keep in mind their limitations and risks.\nThese models can be used with Completion API requests. gpt-35-turbo is the only\nmodel that can be used with both Completion API requests and the Chat Completion\nAPI.Code search embedding\nModel Summary table and region availability\n\uff09 Impor tant\nSouth Central US is temporarily unavailable for creating new resources due to high\ndemand.\nGPT-3 Models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "37057293faf9-0", "text": "Model ID Base model R egions Fine-\nTuning\nRegionsMax\nRequest\n(tokens)Training\nData (up\nto)Model ID Base model R egions Fine-\nTuning\nRegionsMax\nRequest\n(tokens)Training\nData (up\nto)\nada N/A N/A 2,049 Oct 2019\ntext-ada-001 East US, South Central US, W est\nEuropeN/A 2,049 Oct 2019\nbabbage N/A N/A 2,049 Oct 2019\ntext-\nbabbage-001East US, South Central US, W est\nEuropeN/A 2,049 Oct 2019\ncurie N/A N/A 2,049 Oct 2019\ntext-curie-001 East US, South Central US, W est\nEuropeN/A 2,049 Oct 2019\ndavinci N/A N/A 2,049 Oct 2019\ntext-davinci-\n001South Central US, W est Europe N/A\ntext-davinci-\n002East US, South Central US, W est\nEuropeN/A 4,097 Jun 2021\ntext-davinci-\n003East US, W est Europe N/A 4,097 Jun 2021\ntext-davinci-\nfine-tune-002N/A N/A\ngpt-35-turbo\n(0301)East US, France Central, South\nCentral US, UK South, W est EuropeN/A 4,096 Sep 2021\ngpt-35-turbo\n(0613)East US, France Central, UK South N/A 4,096 Sep 2021\ngpt-35-turbo-\n16k (0613)East US, France Central, UK South N/A 16,384 Sep 2021", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "37057293faf9-1", "text": "Version 0301 of gpt-35-turbo will be deprecated no earlier than October 15th, 2023 in\nfavor of version 0613.\nThese models can only be used with the Chat Completion API.1\n1\nGPT-4 Models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "686aae07e75b-0", "text": "Model ID Base model\nRegionsFine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)Model ID Base model\nRegionsFine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\ngpt-4 \n(0314)East US, France\nCentralN/A 8,192 September 2021\ngpt-4-32k \n(0314)East US, France\nCentralN/A 32,768 September 2021\ngpt-4  (0613) East US, France\nCentralN/A 8,192 September 2021\ngpt-4-32k \n(0613)East US, France\nCentralN/A 32,768 September 2021\n The model is only available by request . \n Version 0314 of gpt-4 and gpt-4-32k will be deprecated no earlier than October 15th,\n2023 in favor of version 0613.\nModel\nIDBase model\nRegionsFine-T uning\nRegionsMax R equest\n(charact ers)Training Data (up\nto)\ndalle2 East US N/A 1000 N/A\nThese models can only be used with Completions API requests.\nModel ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\ncode-\ncushman-001South Central US,\nWest EuropeCurrently\nunavailable2,048\ncode-davinci-\n002East US, West Europe N/A 8,001 Jun 2021\n The model is available for fine-tuning by request only. Currently we aren't accepting\nnew requests to fine-tune the model.\nThese models can only be used with Embedding API requests.1,2\n1,2\n1\n1\n1", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "686aae07e75b-1", "text": "1,2\n1\n1\n1\n2\nDall-E Models\nCodex Models\n1\n1\nEmbeddings Models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a1a663724679-0", "text": "Model ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training\nData (up t o)\ntext-embedding-ada-\n002 (version 2)East US, South Central\nUSN/A 8,191 Sep 2021\ntext-embedding-ada-\n002 (version 1)East US, South Central\nUS, W est EuropeN/A 2,046 Sep 2021\ntext-similarity-ada-\n001East US, South Central\nUS, W est EuropeN/A 2,046 Aug 2020\ntext-similarity-\nbabbage-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-similarity-curie-\n001East US, South Central\nUS, W est EuropeN/A 2046 Aug 2020\ntext-similarity-davinci-\n001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-ada-doc-\n001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-ada-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-babbage-\ndoc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-babbage-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-curie-doc-\n001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-curie-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\uff17 Note\nWe strongly recommend using text-embedding-ada-002 (Version 2). This", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a1a663724679-1", "text": "We strongly recommend using text-embedding-ada-002 (Version 2). This\nmodel/version provides parity with OpenAI's text-embedding-ada-002. To learn\nmore about the improvements offered by this model, please refer to OpenAI' s blog\npost . Even if you are currently using V ersion 1 you should migrate to V ersion 2 to\ntake advantage of the latest weights/updated token limit. V ersion 1 and V ersion 2\nare not interchangeable, so document embedding and document search must be\ndone using the same version of the model.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cf1df387a5cf-0", "text": "Model ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training\nData (up t o)\ntext-search-davinci-\ndoc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-davinci-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-ada-\ncode-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-ada-text-\n001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-\nbabbage-code-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-\nbabbage-text-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\nLearn more about Azure OpenAI\nLearn more about fine-tuning Azure OpenAI modelsNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1c7b64db1288-0", "text": "Azure OpenAI Service legacy models\nArticle \u202207/07/2023\nAzure OpenAI Service offers a variety of models for different use cases. The following\nmodels are not available for new deployments beginning July 6, 2023. Deployments\ncreated prior to July 6, 2023 remain available to customers until July 5, 2024. W e\nrecommend customers migrate to the replacement models prior to the July 5, 2024\nretirement.\nThe impacted GPT-3.5 models are the following. The replacement for the GPT-3.5\nmodels is GPT-3.5 Turbo Instruct when that model becomes available.\ntext-davinci-002\ntext-davinci-003\ncode-davinci-002\nThe impacted GPT-3 models are the following. The replacement for the GPT-3 models is\nGPT-3.5 Turbo Instruct when that model becomes available.\ntext-ada-001\ntext-babbage-001\ntext-curie-001\ntext-davinci-001\ncode-cushman-001\nThe embedding models below will be retired effective July 5, 2024. Customers should\nmigrate to text-embedding-ada-002 (version 2).\nSimilarity\nText search\nCode searchGPT-3.5\nGPT-3\nEmbedding models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "15b963f3bea8-0", "text": "Each family includes models across a range of capability. The following list indicates the\nlength of the numerical vector returned by the service, based on model capability:\nBase Model Model(s) Dimensions\nAda 1024\nBabbage 2048\nCurie 4096\nDavinci 12288\nThese models are good at capturing semantic similarity between two or more pieces of\ntext.\nUse cases Models\nClustering, regression, anomaly detection, visualization text-similarity-ada-001  \ntext-similarity-babbage-001  \ntext-similarity-curie-001  \ntext-similarity-davinci-001  \nThese models help measure whether long documents are relevant to a short search\nquery. There are two input types supported by this family: doc, for embedding the\ndocuments to be retrieved, and query, for embedding the search query.\nUse cases Models\nSearch, context relevance, information retrieval text-search-ada-doc-001  \ntext-search-ada-query-001  \ntext-search-babbage-doc-001  \ntext-search-babbage-query-001  \ntext-search-curie-doc-001  \ntext-search-curie-query-001  \ntext-search-davinci-doc-001  \ntext-search-davinci-query-001  Similarity embedding\nText search embedding\nCode search embedding", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "36fd1ed21c5a-0", "text": "Similar to text search embedding models, there are two input types supported by this\nfamily: code, for embedding code snippets to be retrieved, and text, for embedding\nnatural language search queries.\nUse cases Models\nCode search and relevance code-search-ada-code-001  \ncode-search-ada-text-001  \ncode-search-babbage-code-001  \ncode-search-babbage-text-001\nRegion availability is for customers with deployments of the models prior to July 6, 2023.\nModel ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\ntext-\ndavinci-002East US, South Central US,\nWest EuropeN/A 4,097 Jun 2021\ntext-\ndavinci-003East US, W est Europe N/A 4,097 Jun 2021\ncode-\ndavinci-002East US, West Europe N/A 8,001 Jun 2021\nModel ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\nada N/A N/A 2,049 Oct 2019\ntext-ada-\n001East US, South Central US,\nWest EuropeN/A 2,049 Oct 2019\nbabbage N/A N/A 2,049 Oct 2019\ntext-\nbabbage-\n001East US, South Central US,\nWest EuropeN/A 2,049 Oct 2019Model summary table and region availability\nGPT-3.5 models\nGPT-3 models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "02ede6fa86e2-0", "text": "Model ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\ncurie N/A N/A 2,049 Oct 2019\ntext-curie-\n001East US, South Central US,\nWest EuropeN/A 2,049 Oct 2019\ndavinci N/A N/A 2,049 Oct 2019\ntext-\ndavinci-001South Central US, W est\nEuropeN/A\nModel ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training Data\n(up t o)\ncode-\ncushman-001South Central US, W est\nEuropeN/A 2,048\nModel ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training\nData (up t o)\ntext-similarity-ada-\n001East US, South Central\nUS, W est EuropeN/A 2,046 Aug 2020\ntext-similarity-\nbabbage-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-similarity-curie-\n001East US, South Central\nUS, W est EuropeN/A 2046 Aug 2020\ntext-similarity-\ndavinci-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-ada-\ndoc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-ada-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-\nbabbage-doc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020Codex models\nEmbedding models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "75870b311d4b-0", "text": "Model ID Base model R egions Fine-T uning\nRegionsMax R equest\n(tokens)Training\nData (up t o)\ntext-search-\nbabbage-query-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-curie-\ndoc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-curie-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-davinci-\ndoc-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ntext-search-davinci-\nquery-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-ada-\ncode-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-ada-\ntext-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-\nbabbage-code-001South Central US, W est\nEuropeN/A 2,046 Aug 2020\ncode-search-\nbabbage-text-001South Central US, W est\nEuropeN/A 2,046 Aug 2020", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1021c81b8c6b-0", "text": "What's new in Azure OpenAI Service\nArticle \u202206/30/2023\nAzure OpenAI on your data  is now available in preview, enabling you to chat with\nOpenAI models such as ChatGPT and GPT-4 and receive responses based on your\ndata.\ngpt-35-turbo (version 0613)\ngpt-35-turbo-16k (version 0613)\ngpt-4 (version 0613)\ngpt-4-32k (version 0613)\nAzure OpenAI is now available in the UK South region. Check the models page , for\nthe latest information on model availability in each region.\nHow to configure content filters  with Azure OpenAI Service.\nEnable annotations  to view content filtering category and severity information as\npart of your GPT based Completion and Chat Completion calls.\nQuota provides the flexibility to actively manage the allocation of rate limits across\nthe deployments  within your subscription.June 2023\nUse Azure OpenAI on your own data (preview)\nNew versions of gpt-35-turbo and gpt-4 models\nUK South\nContent filtering & annotations (Preview)\nQuota\nMay 2023\nJava & JavaScript SDK support", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "146e599804ce-0", "text": "NEW Azure OpenAI preview SDKs offering support for JavaScript  and Java.\nGeneral availability support for:\nChat Completion API version 2023-05-15.\nGPT-35-Turbo models.\nGPT-4 model series. Due to high demand access to this model series is currently\nonly available by request. T o request access, existing Azure OpenAI customers\ncan apply by filling out this form\nIf you are currently using the 2023-03-15-preview API, we recommend migrating to the\nGA 2023-05-15 API. If you are currently using API version 2022-12-01 this API remains\nGA, but does not include the latest Chat Completion capabilities.\nAzure OpenAI is now available in the France Central region. Check the models\npage , for the latest information on model availability in each region.\nDALL-E 2 public pr eview . Azure OpenAI Service now supports image generation\nAPIs powered by OpenAI's D ALL-E 2 model. Get AI-generated images based on the\ndescriptive text you provide. T o learn more, check out the quickstart . To request\naccess, existing Azure OpenAI customers can apply by filling out this form .\nInactiv e deployments o f cust omized models will now be delet ed af ter 15 days;\nmodels will r emain av ailable for r edeployment.  If a customized (fine-tuned)\nmodel is deployed for more than fifteen (15) days during which no completions or\nchat completions calls are made to it, the deployment will automatically be deleted\n(and no further hosting charges will be incurred for that deployment). The\nunderlying customized model will remain available and can be redeployed at any\ntime. T o learn more check out the how-to-article .Azure OpenAI Chat Completion General Availability (GA)\n\uff09 Impor tant", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "146e599804ce-1", "text": "\uff09 Impor tant\nUsing the current versions of the GPT-35-Turbo models with the completion\nendpoint remains in preview.\nFrance Central\nApril 2023", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "dec70478b031-0", "text": "GPT-4 series models ar e now av ailable in pr eview on Azur e OpenAI . To request\naccess, existing Azure OpenAI customers can apply by filling out this form . These\nmodels are currently available in the East US and South Central US regions.\nNew Chat Completion API for ChatGPT and GPT-4 models r eleased in pr eview\non 3/21 . To learn more checkout the updated quickstarts  and how-to article .\nChatGPT (gpt -35-turbo) pr eview . To learn more checkout the how-to article .\nIncreased training limits for fine-tuning: The max training job size (tokens in\ntraining file) x (# of epochs) is 2 Billion tokens for all models. W e have also\nincreased the max training job from 120 to 720 hours.\nAdding additional use cases to your existing access. \u00a0Previously, the process for\nadding new use cases required customers to reapply to the service. Now, we're\nreleasing a new process that allows you to quickly add new use cases to your use\nof the service. This process follows the established Limited Access process within\nAzure Cognitive Services. Existing customers can attest to any and all new use\ncases here . Please note that this is required anytime you would like to use the\nservice for a new use case you did not originally apply for.\n.NET SDK(inference) preview release  | Samples\nTerraform SDK update  to support Azure OpenAI management operations.\nInserting text at the end of a completion is now supported with the suffix\nparameter.\nContent filtering is on by default.\nNew articles on:\nMonitoring an Azure OpenAI Service\nPlan and manage costs for Azure OpenAI\nNew training course:March 2023\nFebruary 2023\nNew Features\nUpdates", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e96a1e1f92f8-0", "text": "Intro to Azure OpenAI\nService GA . Azure OpenAI Service is now generally available. \nNew models : Addition of the latest text model, text-davinci-003 (East US, W est\nEurope), text-ada-embeddings-002 (East US, South Central US, W est Europe)\nThe lat est models fr om OpenAI.  Azure OpenAI provides access to all the latest\nmodels including the GPT-3.5 series .\nNew API v ersion (2022-12-01).  This update includes several requested\nenhancements including token usage information in the API response, improved\nerror messages for files, alignment with OpenAI on fine-tuning creation data\nstructure, and support for the suffix parameter to allow custom naming of fine-\ntuned jobs. \nHigher r equest per second limits.  50 for non-Davinci models. 20 for Davinci\nmodels. \nFaster fine-tune deployments.  Deploy an Ada and Curie fine-tuned models in\nunder 10 minutes. \nHigher training limits:  40M training tokens for Ada, Babbage, and Curie. 10M for\nDavinci. \nProcess for r equesting modifications t o the abuse & miss-use data logging &\nhuman r eview . Today, the service logs request/response data for the purposes of\nabuse and misuse detection to ensure that these powerful models aren't abused.\nHowever, many customers have strict data privacy and security requirements that\nrequire greater control over their data. T o support these use cases, we're releasing\na new process for customers to modify the content filtering policies or turn off the\nabuse logging for low-risk use cases. This process follows the established LimitedJanuary 2023\nNew Features\nDecember 2022\nNew features", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e9be54a78cbf-0", "text": "Access process within Azure Cognitive Services and existing OpenAI customers can\napply here . \nCustomer managed k ey (CMK) encr yption.  CMK provides customers greater\ncontrol over managing their data in Azure OpenAI by providing their own\nencryption keys used for storing training data and customized models. Customer-\nmanaged keys (CMK), also known as bring your own key (B YOK), offer greater\nflexibility to create, rotate, disable, and revoke access controls. Y ou can also audit\nthe encryption keys used to protect your data. Learn more from our encryption at\nrest documentation .\nLockbo x suppor t \nSOC-2 compliance  \nLogging and diagnostics  through Azure R esource Health, Cost Analysis, and\nMetrics & Diagnostic settings .\nStudio impr ovements.  Numerous usability improvements to the S tudio workflow\nincluding Azure AD role support to control who in the team has access to create\nfine-tuned models and deploy.\nFine-tuning  create API request has been updated to match OpenAI\u2019s schema.\nPreview API v ersions:\nJSON\nAPI v ersion 2022-12-01:\nJSON\nChanges (breaking)\n{  \n    \"training_file\" : \"file-XGinujblHPwGLSztz8cPS8XY\" ,  \n    \"hyperparams\" : {   \n        \"batch_size\" : 4,  \n        \"learning_rate_multiplier\" : 0.1,  \n        \"n_epochs\" : 4,  \n        \"prompt_loss_weight\" : 0.1,  \n    }  \n} \n{  \n    \"training_file\" : \"file-XGinujblHPwGLSztz8cPS8XY\" ,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2de0a939ae38-0", "text": "Cont ent filt ering is t emporarily o ff by default. Azure content moderation works\ndifferently than OpenAI. Azure OpenAI runs content filters during the generation call to\ndetect harmful or abusive content and filters them from the response. Learn More \n These models will be re-enabled in Q1 2023 and be on by default.  \nCustomer actions  \nContact Azure Support  if you would like these turned on for your subscription .\nApply for filtering modifications , if you would like to have them remain off. (This\noption will be for low-risk use cases only.) \nLearn more about the underlying models that power Azure OpenAI .    \"batch_size\" : 4,  \n    \"learning_rate_multiplier\" : 0.1,  \n    \"n_epochs\" : 4,  \n    \"prompt_loss_weight\" : 0.1,  \n} \nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "71040705a54d-0", "text": "Azur e OpenAI Ser vice fr equently\nasked questions\nFAQ\nIf you can't find answers to your questions in this document, and still need help check\nthe Cognitive Services support options guide . Azure OpenAI is part of Azure Cognitive\nServices.\nAzure OpenAI doesn't use customer data to retrain models. For more information, see\nthe Azure OpenAI data, privacy, and security guide .\nAzure OpenAI supports the latest GPT-4 models. These models are currently only\navailable by request. For access, existing Azure OpenAI customers can apply by filling\nout this form .\nAzure OpenAI Service gives customers advanced language AI with OpenAI GPT-3,\nCodex, and D ALL-E models with the security and enterprise promise of Azure. Azure\nOpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth\ntransition from one to the other.\nWith Azure OpenAI, customers get the security capabilities of Microsoft Azure while\nrunning the same models as OpenAI.Data and Priv acy\nDo y ou use my comp any data t o train any o f the\nmodels?\nGeneral\nDoes Azur e OpenAI suppor t GPT-4?\nHow do the cap abilities o f Azur e OpenAI\ncomp are to OpenAI?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "5567d06c8d82-0", "text": "Yes, as part of Azure Cognitive Services, Azure OpenAI supports VNET s and Private\nEndpoints. T o learn more consult the Cognitive Services virtual networking guidance\nNo, GPT-4 is designed by OpenAI to be multimodal, but currently only text input and\noutput are supported.\nPreviously, the process for adding new use cases required customers to reapply to the\nservice. Now, we're releasing a new process that allows you to quickly add new use\ncases to your use of the service. This process follows the established Limited Access\nprocess within Azure Cognitive Services. Existing customers can attest to any and all new\nuse cases here . Please note that this is required anytime you would like to use the\nservice for a new use case you did not originally apply for.\nThis error typically occurs when you try to send a batch of text to embed in a single API\nrequest as an array. Currently Azure OpenAI does not support batching with embedding\nrequests. Embeddings API calls should consist of a single string input per request. The\nstring can be up to 8191 tokens in length when using the text-embedding-ada-002\n(Version 2) model.\nCheck out our introduction to prompt engineering  While these models are extremely\npowerful, their behavior is also very sensitive to the prompts they receive from the user.\nThis makes prompt construction an important skill to develop. After you've masteredDoes Azur e OpenAI suppor t VNE Ts and Priv ate\nEndpoints?\nDo the GPT-4 models curr ently suppor t image\ninput ?\nHow do I apply for new use cases?\nI am tr ying t o use embeddings and r eceiv ed the\nerror \"Inv alidR equestErr or: Too many inputs. The\nmax number o f inputs is 1.\" How do I fix this?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "5567d06c8d82-1", "text": "max number o f inputs is 1.\" How do I fix this?\nWher e can I r ead about bett er ways t o use Azur e\nOpenAI t o get the r esponses I want fr om the\nservice?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1de2b9d3180b-0", "text": "the introduction, check out our article on more advanced prompt engineering\ntechniques .\nAccess is currently limited as we navigate high demand, upcoming product\nimprovements, and Microsoft's commitment to responsible AI. For now, we're working\nwith customers with an existing partnership with Microsoft, lower risk use cases, and\nthose committed to incorporating mitigations. In addition to applying for initial access,\nall solutions using Azure OpenAI are required to go through a use case review before\nthey can be released for production use. Apply here for initial access or for a production\nreview: Apply now\nWe don't currently provide a timeline for access approval.\nFor monthly updates, see our what's new page .\nCheck out our introduction to Azure OpenAI training course .\nWe recommend posting questions on Microsoft Q&AGetting access t o Azur e OpenAI Ser vice\nHow do I get access t o Azur e OpenAI?\nAfter I apply for access, how long will I hav e to\nwait t o get appr oved?\nLearning mor e and wher e to ask\nquestions\nWher e can I r ead about the lat est updat es to\nAzur e OpenAI?\nWher e can I get training t o get star ted learning\nand build my skills ar ound Azur e OpenAI?\nWher e can I post questions and see answ ers to\nother common questions?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "5a363bfe333f-0", "text": "Alternatively, you can post questions on Stack Overflow\nAzure OpenAI is part of Azure Cognitive Services. Y ou can learn about all the support\noptions for Azure Cognitive Services in the support and help options guide .\nConsult the Azure OpenAI model availability guide .\nConsult the Azure OpenAI model availability guide  for region availability.\nA base model is a model that hasn't been customized or fine-tuned for a specific use\ncase. Fine-tuned models are customized versions of base models where a model's\nweights are trained on a unique set of prompts. Fine-tuned models let you achieve\nbetter results on a wider number of tasks without needing to provide detailed examples\nfor in-context learning as part of your completion prompt. T o learn more, review our\nfine-tuning guide .\n100\nWher e do I go for Azur e OpenAI cust omer\nsuppor t?\nModels and fine-tuning\nWhat models ar e available?\nWher e can I find out what r egion a model is\navailable in?\nWhat is the differ ence betw een a b ase model\nand a fine-tuned model?\nWhat is the maximum number o f fine-tuned\nmodels I can cr eate?\nWhat ar e the SL As for API r esponses in Azur e\nOpenAI?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3231e8396e44-0", "text": "We don't have a defined API response time Service Level Agreement (SL A) at this time.\nThe overall SL A for Azure OpenAI Service is the same as for other Azure Cognitive\nServices. For more information, see the Cognitive Services section of the Service Level\nAgreements (SL A) for Online Services page .\nIf a customized (fine-tuned) model is deployed for more than fifteen (15) days during\nwhich no completions or chat completions calls are made to it, the deployment will\nautomatically be deleted (and no further hosting charges will be incurred for that\ndeployment). The underlying customized model will remain available and can be\nredeployed at any time. T o learn more check out the how-to-article .\nThere are currently two different REST APIs that allow model deployment. For the latest\nmodel deployment features such as the ability to specify a model version during\ndeployment for models like text-embedding-ada-002 V ersion 2, use the Cognitive\nServices Create or Update REST API call .\nNo, quota T okens-P er-Minute (TPM) allocation is not related to the max input token\nlimit of a model. Model input token limits are defined in the models table  and are not\nimpacted by changes made to TPM.\nYou can customize your published web app in the Azure portal. The source code for the\npublished web app is available on GitHub , where you can find information on\nchanging the app frontend, as well as instructions for building and deploying the app.\nWhy was my fine-tuned model deployment\ndelet ed?\nHow do I deploy a model with the REST API?\nCan I use quota t o incr ease the max t oken limit\nof a model?\nWeb app\nHow can I cust omize my published w eb app?\nWill my w eb app be ov erwritt en when I deploy", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3231e8396e44-1", "text": "Will my w eb app be ov erwritt en when I deploy\nthe app again fr om the Azur e AI S tudio?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d2249ab09fcc-0", "text": "Your app code will not be overwritten when you update your app. The app will be\nupdated to use the Azure OpenAI resource, Azure Cognitive Search index (if you're\nusing Azure OpenAI on your data), and model settings selected in the Azure OpenAI\nStudio without any change to the appearance or functionality.\nAzure OpenAI on your data is a feature of the Azure OpenAI Services that helps\norganizations to generate customized insights, content, and searches using their\ndesignated data sources. It works with the capabilities of the OpenAI models in Azure\nOpenAI to provide more accurate and relevant responses to user queries in natural\nlanguage. Azure OpenAI on your data can be integrated with customer's existing\napplications and workflows, offers insights into key performance indicators, and can\ninteract with users seamlessly.\nAll Azure OpenAI customers can use Azure OpenAI on your data via the Azure AI studio\nand R est API.\nAzure OpenAI on your data supports ingestion from Azure Cognitive Search, Azure Blob\nStorage, and uploading local files. Y ou can learn more about Azure OpenAI on your data\nfrom the conceptual article  and quickstart .\nWhen using Azure OpenAI on your data, you incur costs when you use Azure Cognitive\nSearch, Azure Blob S torage, Azure W eb App Service, semantic search and OpenAI\nmodels. There is no additional cost for using the \"your data\" feature in the Azure AI\nStudio.Using y our data\nWhat is Azur e OpenAI on y our data?\nHow can I access Azur e OpenAI on y our data?\nWhat data sour ces does Azur e OpenAI on y our\ndata suppor t?\nHow much does it cost t o use Azur e OpenAI on\nyour data?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c046353ae86f-0", "text": "You can prepare the index yourself using a script provided on GitHub . Using this script\nwill create an Azure Cognitive Search index with all the information needed to better\nleverage your data, with your documents broken down into manageable chunks. Please\nsee the README file  with the data preparation code for details on how to run it.\nYou can upload additional data to your Azure Blob Container and use it as your data\nsource when you create a new index. The new index will include all of the data in your\ncontainer.\nSee Using your data  for more information on supported file types.\nYes, Azure OpenAI on your data  is part of the Azure OpenAI service and works with the\nmodels  available in Azure OpenAI. The content filtering  and abuse monitoring features\nof Azure OpenAI still apply. For more information, see the overview of R esponsible AI\npractices for Azure OpenAI models  and the Transparency Note for Azure OpenAI  for\nadditional guidance on using Azure OpenAI on your data responsibly.\nYes, the token limit on the system message is 200. If the system message is more than\n200 tokens, the rest of the tokens beyond the first 200 will be ignored.\nYou must send queries in the same language of your data. Y our data can be in any of the\nlanguages supported by Azure Cognitive Search .How can I cust omize or aut omat e the index\ncreation pr ocess?\nHow can I updat e my index?\nWhat file types does Azur e OpenAI on y our data\nsuppor t?\nIs responsible AI suppor ted by Azur e OpenAI on\nyour data?\nIs ther e a token limit on the syst em message?\nDo the quer y language and the data sour ce\nlanguage need t o be the same?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bcc053d6cfb9-0", "text": "Azure OpenAI quotas and limits\nAzure OpenAI what's new\nAzure OpenAI quickstartsNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "83c4bbaf8eb7-0", "text": "Quickstart: Get started generating text\nusing Azure OpenAI Service\nArticle \u202205/24/2023\nUse this article to get started making your first calls to Azure OpenAI.\nAn Azure subscription - Create one for free .\nAccess granted to Azure OpenAI in the desired Azure subscription.\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nAn Azure OpenAI resource with a model deployed. For more information about\nmodel deployment, see the resource deployment guide .\nNavigate to Azure OpenAI S tudio at https://oai.azure.com/  and sign-in with\ncredentials that have access to your OpenAI resource. During or after the sign-in\nworkflow, select the appropriate directory, Azure subscription, and Azure OpenAI\nresource.\nFrom the Azure OpenAI S tudio landing page navigate further to explore examples for\nprompt completion, manage your deployments and models, and find learning resources\nsuch as documentation and community forums.Prerequisites\nI ran int o an issue with the pr erequisit es.\nGo to the Azure OpenAI Studio", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3ff77fd6f2ff-0", "text": "Go to the Playground  for experimentation and fine-tuning workflow.\nStart exploring Azure OpenAI capabilities with a no-code approach through the GPT-3\nPlayground. It's simply a text box where you can submit a prompt to generate a\ncompletion. From this page, you can quickly iterate and experiment with the capabilities.\n\uf80a\nPlayground\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ee88147b9766-0", "text": "You can select a deployment and choose from a few pre-loaded examples to get started.\nIf your resource doesn't have a deployment, select Create a deployment  and follow the\ninstructions provided by the wizard. For more information about model deployment, see\nthe resource deployment guide .\nYou can experiment with the configuration settings such as temperature and pre-\nresponse text to improve the performance of your task. Y ou can read more about each\nparameter in the REST API .\nSelecting the Generat e button will send the entered text to the completions API\nand stream the results back to the text box.\nSelect the Undo  button to undo the prior generation call.\nSelect the Regenerat e button to complete an undo and generation call together.\nAzure OpenAI also performs content moderation on the prompt inputs and generated\noutputs. The prompts or responses may be filtered if harmful content is detected. For\nmore information, see the content filter  article.\nIn the GPT-3 playground you can also view Python and curl code samples pre-filled\naccording to your selected settings. Just select View code  next to the examples\ndropdown. Y ou can write an application to complete the same task with the OpenAI\nPython SDK, curl, or other REST API client.\nTo use the Azure OpenAI for text summarization in the GPT-3 Playground, follow these\nsteps:\n1. Sign in to Azure OpenAI S tudio .\n2. Select the subscription and OpenAI resource to work with.\n3. Select GPT-3 Play ground  at the top of the landing page.\n4. Select your deployment from the Deployments  dropdown. If your resource doesn't\nhave a deployment, select Create a deployment  and then revisit this step.\n5. Select Summarize T ext from the Examples  dropdown.Try text summarization", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1a9e341b1e07-0", "text": "6. Select Generate. Azure OpenAI will attempt to capture the context of text and\nrephrase it succinctly. Y ou should get a result that resembles the following text:\nThe accuracy of the response can vary per model. The Davinci based model in this\nexample is well-suited to this type of summarization, whereas a Codex based model\nwouldn't perform as well at this particular task.\nIf you want to clean up and remove an OpenAI resource, you can delete the resource or\nresource group. Deleting the resource group also deletes any other resources associated\nwith it.\nPortal\nAzure CLI\n\uf80a\nTl;dr A neutron star is the collapsed core of a supergiant star. These  \nincredibly dense objects are incredibly fascinating due to their  \nstrange properties and their potential for phenomena such as extreme  \ngravitational forces and a strong magnetic field.  \nI ran int o an issue with the play ground.\nClean up resources\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "02d70e973112-0", "text": "Learn more about how to generate the best completion in our How-to guide on\ncompletions .\nFor more examples check out the Azure OpenAI Samples GitHub repository .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a93ebbd1ebee-0", "text": "Quickstart: Get started using ChatGPT\nand GPT-4 with Azure OpenAI Service\nArticle \u202205/24/2023\nUse this article to get started using Azure OpenAI.\nAn Azure subscription - Create one for free .\nAccess granted to Azure OpenAI in the desired Azure subscription.\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nAn Azure OpenAI Service resource with either the gpt-35-turbo or the gpt-4\nmodels deployed. For more information about model deployment, see the\nresource deployment guide .\n GPT-4 models ar e curr ently only av ailable by r equest.  To access these models,\nexisting Azure OpenAI customers can apply for access by filling out this form .\nNavigate to Azure OpenAI S tudio at https://oai.azure.com/  and sign-in with\ncredentials that have access to your OpenAI resource. During or after the sign-in\nworkflow, select the appropriate directory, Azure subscription, and Azure OpenAI\nresource.\nFrom the Azure OpenAI S tudio landing page, select Chat play ground .Prerequisites\n1\n1\nI ran int o an issue with the pr erequisit es.\nGo to Azure OpenAI Studio", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1ff1e6257d0b-0", "text": "Start exploring OpenAI capabilities with a no-code approach through the Azure OpenAI\nStudio Chat playground. From this page, you can quickly iterate and experiment with the\ncapabilities.\n\uf80a\nPlayground\n\uf80a\nAssistant setup", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b819f5235673-0", "text": "You can use the Assistant setup  dropdown to select a few pre-loaded System message\nexamples to get started.\nSystem messages  give the model instructions about how it should behave and any\ncontext it should reference when generating a response. Y ou can describe the assistant's\npersonality, tell it what it should and shouldn't answer, and tell it how to format\nresponses.\nAdd few-shot examples  allows you to provide conversational examples that are used by\nthe model for in-context learning .\nAt any time while using the Chat playground you can select View code  to see Python,\ncurl, and json code samples pre-populated based on your current chat session and\nsettings selections. Y ou can then take this code and write an application to complete the\nsame task you're currently performing with the playground.\nSelecting the Send  button sends the entered text to the completions API and the results\nare returned back to the text box.\nSelect the Clear chat  button to delete the current conversation history.\nName Description\nDeployments Your deployment name that is associated with a specific model. For ChatGPT, you\nneed to use the gpt-35-turbo model.\nTemperature Controls randomness. Lowering the temperature means that the model produces\nmore repetitive and deterministic responses. Increasing the temperature results in\nmore unexpected or creative responses. T ry adjusting temperature or T op P but\nnot both.\nMax length\n(tokens)Set a limit on the number of tokens per model response. The API supports a\nmaximum of 4096 tokens shared between the prompt (including system message,\nexamples, message history, and user query) and the model's response. One token\nis roughly four characters for typical English text.\nTop\nprobabilitiesSimilar to temperature, this controls randomness but uses a different method.\nLowering T op P narrows the model\u2019s token selection to likelier tokens. Increasing", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b819f5235673-1", "text": "Lowering T op P narrows the model\u2019s token selection to likelier tokens. Increasing\nTop P lets the model choose from tokens with both high and low likelihood. T ry\nadjusting temperature or T op P but not both.Chat session\nSettings", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0f629b505960-0", "text": "Name Description\nMulti-turn\nconversationsSelect the number of past messages to include in each new API request. This\nhelps give the model context for new user queries. Setting this number to 10\nresults in five user queries and five system responses.\nStop\nsequencesStop sequence make the model end its response at a desired point. The model\nresponse ends before the specified sequence, so it won't contain the stop\nsequence text. For ChatGPT, using <|im_end|> ensures that the model response\ndoesn't generate a follow-up user query. Y ou can include as many as four stop\nsequences.\nBy default there are three panels: assistant setup, chat session, and settings. Show\npanels  allows you to add, remove, and rearrange the panels. If you ever close a panel\nand need to get it back, use Show p anels  to restore the lost panel.\n1. From the assistant setup drop-down, select Xbox cust omer suppor t agent\n2. You'll be prompted asking if you want to update the system message, select\nContinue .\n3. In the chat session pane, enter the following question: \"I'm interested in buying a\nnew Xbox\", and select Send .\n4. You'll receive a response similar to:\n5. Enter a follow-up question like: \"which models support 4K?\"Show panels\nStart a chat session\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "600c7ba2b209-0", "text": "6. Now that you have a basic conversation select View code  from under Assistant\nsetup  and you'll have a replay of the code behind the entire conversation so far:\nPython\n\uf80a\n#Note: The openai-python library support for Azure OpenAI is in preview.  \nimport os \nimport openai \nopenai.api_type = \"azure\" \nopenai.api_base = \"https://docs-test-001.openai.azure.com/\"  \nopenai.api_version = \"2023-05-15\"\nopenai.api_key = os.getenv( \"OPENAI_API_KEY\" ) \nresponse = openai.ChatCompletion.create(  \n  engine= \"gpt-35-turbo\" , # replace this value with the deployment name you  \nchose when you deployed the associated model.  \n  messages = [{ \"role\":\"system\" ,\"content\" :\"You are an Xbox customer support  \nagent whose primary goal is to help users with issues they are experiencing  \nwith their Xbox devices. You are friendly and concise. You only provide  \nfactual answers to queries, and do not provide answers that are not related  \nto Xbox.\" },{\"role\":\"user\",\"content\" :\"How much is a PS5?\" },\n{\"role\":\"assistant\" ,\"content\" :\"I apologize, but I do not have information  \nabout the prices of other gaming devices such as the PS5. My primary focus  \nis to assist with issues regarding Xbox devices. Is there a specific issue  \nyou are having with your Xbox device that I may be able to help with?\" },\n{\"role\":\"user\",\"content\" :\"I am interested in buying an Xbox?\" },\n{\"role\":\"assistant\" ,\"content\" :\"Great! There are several Xbox models  \navailable in the market, including the Xbox Series X, Xbox Series S, and", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "600c7ba2b209-1", "text": "available in the market, including the Xbox Series X, Xbox Series S, and  \nXbox One. The Xbox Series X is the latest and most powerful console, while  \nthe Xbox Series S is a more affordable option that still offers great  \nperformance. The Xbox One is the previous generation console, but it still  \nhas a large library of games available to play.\\n\\nBefore purchasing an  \nXbox, you may want to consider factors such as your budget, the types of  \ngames you want to play, and whether you want to play games in 4K resolution.  \nOnce you have decided on a console, you can purchase it from a retailer or  \ndirectly from the Xbox website. Let me know if you have any other questions  \nor concerns!\" },{\"role\":\"user\",\"content\" :\"Which models are 4K?\" },\n{\"role\":\"assistant\" ,\"content\" :\"Both the Xbox Series X and Xbox One X support  \n4K resolution gaming. The Xbox Series S, on the other hand, supports up to  \n1440p resolution gaming, but it can upscale games to 4K. It's worth noting  \nthat in order to take advantage of 4K gaming, you'll need a 4K TV or monitor  \nthat supports HDR (High Dynamic Range) and HDMI 2.0 or higher.\" }],\n  temperature= 0,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "932cdf43ad40-0", "text": "If you examine the sample from View code  you'll notice some unique tokens that\nweren't part of a typical GPT completion call. ChatGPT was trained to use special tokens\nto delineate different parts of the prompt. Content is provided to the model in between\n<|im_start|> and <|im_end|> tokens. The prompt begins with a system message that\ncan be used to prime the model by including context or instructions for the model. After\nthat, the prompt contains a series of messages between the user and the assistant.\nThe assistant's response to the prompt will then be returned below the\n<|im_start|>assistant token and will end with <|im_end|> denoting that the assistant\nhas finished its response. Y ou can also use the Show raw syntax  toggle button to\ndisplay these tokens within the chat session panel.\nThe ChatGPT how-to guide  provides an in-depth introduction into the new prompt\nstructure and how to use the gpt-35-turbo model effectively.\nOnce you're satisfied with the experience in Azure OpenAI studio, you can deploy a web\napp directly from the S tudio by selecting the Deploy t o button.  max_tokens= 350, \n  top_p= 0.95, \n  frequency_penalty= 0, \n  presence_penalty= 0, \n  stop=None) \nI ran int o an issue with the play ground.\nUnderstanding the prompt structure\nDeploy a web app", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e94efae39146-0", "text": "The first time you deploy a web app, you should select Create a new w eb app . Choose a\nname for the app, which will become part of the app URL. For example,\nhttps://<appname>.azurewebsites.net.\nSelect your subscription, resource group, location, and pricing plan for the published\napp. T o update an existing app, select Publish t o an existing w eb app  and choose the\nname of your previous app from the dropdown menu.\nOnce you're done testing out the Chat playground, if you want to clean up and remove\nan OpenAI resource, you can delete the resource or resource group. Deleting the\nresource group also deletes any other resources associated with it.\nPortal\nAzure CLI\nLearn more about how to work with ChatGPT and the new gpt-35-turbo model\nwith the ChatGPT how-to guide .\nFor more examples check out the Azure OpenAI Samples GitHub repository\n\uf80a\nI ran int o an issue with the w eb app.\nClean up resources\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "edc20f5fea8c-0", "text": "Quickstart: Get started generating\nimages using Azure OpenAI Service\nArticle \u202206/12/2023\nUse this guide to get started generating images with Azure OpenAI in your browser.\nAn Azure subscription - Create one for free .\nAccess granted to D ALL-E in the desired Azure subscription. Currently, access to\nthis service is granted only by application. Y ou can apply for access to Azure\nOpenAI by completing the form at https://aka.ms/oai/access . Existing Azure\nOpenAI customers need to re-enter the form to get access to D ALL-E. Open an\nissue on this repo to contact us if you have an issue.\nAn Azure OpenAI resource created in the East US region. For more information\nabout model deployment, see the resource deployment guide .\nNavigate to Azure OpenAI S tudio at https://oai.azure.com/  and sign in with the\ncredentials associated with your OpenAI resource. During or after the sign-in workflow,\nselect the appropriate directory, Azure subscription, and Azure OpenAI resource.\nFrom the Azure OpenAI S tudio landing page, select DALL\u00b7E 2 play ground  to use the\nimage generation APIs.\nStart exploring Azure OpenAI capabilities with a no-code approach through the D ALL-E\n2 playground. Enter your image prompt into the text box and select Generat e. When the\nAI-generated image is ready, it will appear on the page.\uff17 Note\nThe image generation API creates an image from a text prompt. It does not edit\nexisting images or create variations.\nPrerequisites\nGo to the Azure OpenAI Studio\nTry out image generation", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8c38f35e0474-0", "text": "In the D ALL-E 2 playground, you can also view Python and cURL code samples, which\nare pre-filled according to your settings. Select View code  near the top of the page. Y ou\ncan use this code to write an application that completes the same task.\nIf you want to clean up and remove an OpenAI resource, you can delete the resource or\nresource group. Deleting the resource group also deletes any other resources associated\nwith it.\nPortal\nAzure CLI\nAzure OpenAI Overview\uff17 Note\nThe image generation APIs come with a content moderation filter. If the service\nrecognizes your prompt as harmful content, it won't return a generated image. For\nmore information, see the content filt er guide.\n\uf80a\nClean up resources\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cae01c392cb0-0", "text": "For more examples check out the Azure OpenAI Samples GitHub repository .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "51dddad8a24b-0", "text": "Quickstart: Chat with Azure OpenAI\nmodels using your own data\nArticle \u202206/28/2023\nIn this quickstart you can use your own data with Azure OpenAI models. Using Azure\nOpenAI's models on your data can provide you with a powerful conversational AI\nplatform that enables faster and more accurate communication.\nAn Azure subscription - Create one for free .\nAccess granted to Azure OpenAI in the desired Azure subscription.\nAzure OpenAI requires registration and is currently only available to approved\nenterprise customers and partners. See Limited access to Azure OpenAI Service  for\nmore information. Y ou can apply for access to Azure OpenAI by completing the\nform at https://aka.ms/oai/access . Open an issue on this repo to contact us if\nyou have an issue.\nAn Azure OpenAI resource with a chat model deployed (for example, GPT-3 or\nGPT-4). For more information about model deployment, see the resource\ndeployment guide .\nNavigate to Azure OpenAI S tudio  and sign-in with credentials that have access to your\nAzure OpenAI resource. During or after the sign-in workflow, select the appropriate\ndirectory, Azure subscription, and Azure OpenAI resource.\n1. Select the Chat play ground  tile.Prerequisites\nI ran int o an issue with the pr erequisit es.\nAdd your data using Azure OpenAI Studio", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f2f5de742663-0", "text": "2. On the Assistant setup  tile, select Add y our data (pr eview)  > + Add a data\nsource.\n3. In the pane that appears, select Uplo ad files  under Select data sour ce. Select\nUplo ad files . Azure OpenAI needs both a storage resource and a search resource\nto access and index your data.\na. For Azure OpenAI to access your storage account, you will need to turn on\nCross-origin resource sharing (C ORS) . If CORS isn't already turned on for the\nAzure Blob storage resource, select Turn on C ORS.\n\uf80a\n\uf80a\n\uea80 Tip\nFor a list of supported data sources, see Data sour ce options\nFor documents and datasets with long text, we recommend using the\navailable data pr eparation script .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f8f0344efa95-0", "text": "b. Select your Azure Cognitive Search resource, and select the acknowledgment\nthat connecting it will incur usage on your account. Then select Next .\n4. On the Uplo ad files  pane, select Browse for a file  and select the files you want to\nupload. Then select Uplo ad files . Then select Next .\n5. Review the details you entered, and select Save and close . You can now chat with\nthe model and it will use information from your data to construct the response.\nStart exploring Azure OpenAI capabilities with a no-code approach through the chat\nplayground. It's simply a text box where you can submit a prompt to generate a\ncompletion. From this page, you can quickly iterate and experiment with the capabilities.\n\uf80a\nI ran int o an issue adding my data.\nChat playground", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "009736659512-0", "text": "You can experiment with the configuration settings such as temperature and pre-\nresponse text to improve the performance of your task. Y ou can read more about each\nparameter in the REST API .\nSelecting the Generat e button will send the entered text to the completions API\nand stream the results back to the text box.\nSelect the Undo  button to undo the prior generation call.\nSelect the Regenerat e button to complete an undo and generation call together.\nOnce you're satisfied with the experience in Azure OpenAI studio, you can deploy a web\napp directly from the S tudio by selecting the Deploy t o button.\n\uf80a\nDeploy a web app\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "20bcb0a53c25-0", "text": "The first time you deploy a web app, you should select Create a new w eb app . Choose a\nname for the app, which will become part of the app URL. For example,\nhttps://<appname>.azurewebsites.net.\nSelect your subscription, resource group, location, and pricing plan for the published\napp. T o update an existing app, select Publish t o an existing w eb app  and choose the\nname of your previous app from the dropdown menu.\nPublishing creates an Azure App Service in your subscription. It may incur costs\ndepending on the pricing plan  you select. When you're done with your app, you\ncan delete it from the Azure portal.\nBy default, the app will only be accessible to you. T o add authentication (for\nexample, restrict access to the app to members of your Azure tenant):\n1. Go to the Azure portal  and search for the app name you specified during\npublishing. Select the web app, and go to the Authentication  tab on the left\nnavigation menu. Then select Add an identity pr ovider .\n2. Select Microsoft as the identity provider. The default settings on this page will\nrestrict the app to your tenant only, so you don't need to change anything\nelse here. Then select AddI ran int o an issue with the w eb app.\nImportant considerations\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "54089bf94cbf-0", "text": "Now users will be asked to sign in with their Azure Active Directory account to be\nable to access your app. Y ou can follow a similar process to add another identity\nprovider if you prefer. The app doesn't use the user's login information in any\nother way other than verifying they are a member of your tenant.\nIf you want to clean up and remove an OpenAI or Azure Cognitive Search resource, you\ncan delete the resource or resource group. Deleting the resource group also deletes any\nother resources associated with it.\nCognitive Services resources\nAzure Cognitive Search resources\nAzure app service resources\nLearn more about using your data in Azure OpenAI Service\nChat app sample code on GitHub .Clean up resources\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "52634652f301-0", "text": "Abuse Monitoring\nArticle \u202206/19/2023\nAzure OpenAI Service detects and mitigates instances of recurring content and/or\nbehaviors that suggest use of the service in a manner that may violate the Code of\nConduct  or other applicable product terms. Details on how data is handled can be\nfound on the Data, Privacy and Security page .\nThere are several components to abuse monitoring:\nCont ent Classification : Classifier models detect harmful language and/or images in\nuser prompts (inputs) and completions (outputs). The system looks for categories\nof harms as defined in the Content R equirements , and assigns severity levels as\ndescribed in more detail on the Content Filtering page .\nAbuse P attern Captur e: Azure OpenAI Service\u2019s abuse monitoring looks at\ncustomer usage patterns and employs algorithms and heuristics to detect\nindicators of potential abuse. Detected patterns consider, for example, the\nfrequency and severity at which harmful content is detected in a customer\u2019s\nprompts and completions.\nHuman R eview and Decision : When prompts and/or completions are flagged\nthrough content classification and abuse pattern capture as described above,\nauthorized Microsoft employees may assess the flagged content, and either\nconfirm or correct the classification or determination based on predefined\nguidelines and policies. Data can be accessed for human review only by authorized\nMicrosoft employees via Secure Access W orkstations (SA Ws) with Just-In-Time (JIT)\nrequest approval granted by team managers. For Azure OpenAI Service resources\ndeployed in the European Economic Area, the authorized Microsoft employees are\nlocated in the European Economic Area.\nNotification and Action : When a threshold of abusive behavior has been\nconfirmed based on the preceding three steps, the customer is informed of the\ndetermination by email. Except in cases of severe or recurring abuse, customers\ntypically are given an opportunity to explain or remediate\u2014and implement", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "52634652f301-1", "text": "typically are given an opportunity to explain or remediate\u2014and implement\nmechanisms to prevent recurrence of\u2014the abusive behavior. F ailure to address the\nbehavior\u2014or recurring or severe abuse\u2014may result in suspension or termination\nof the customer\u2019s access to Azure OpenAI resources and/or capabilities.Components of abuse monitoring", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b5daf37a9d1b-0", "text": "Learn more about the underlying models that power Azure OpenAI .\nLearn more about understanding and mitigating risks associated with your\napplication: Overview of R esponsible AI practices for Azure OpenAI models .\nLearn more about how data is processed in connection with content filtering and\nabuse monitoring: Data, privacy, and security for Azure OpenAI Service .Next steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f8c0f931e3c0-0", "text": "Content filtering\nArticle \u202206/09/2023\nAzure OpenAI Service includes a content filtering system that works alongside core\nmodels. This system works by running both the prompt and completion through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. V ariations in API configurations and application design may affect\ncompletions and thus filtering behavior. The content filtering system supports the\nfollowing languages: English, German, Japanese, Spanish, French, Italian, P ortuguese,\nand Chinese. It might not be able to detect inappropriate content in languages that it\nhas not been trained or tested to process.\nIn addition to the content filtering system, the Azure OpenAI Service performs\nmonitoring to detect content and/or behaviors that suggest use of the service in a\nmanner that may violate applicable product terms. For more information about\nunderstanding and mitigating risks associated with your application, see the\nTransparency Note for Azure OpenAI . For more information about how data is\nprocessed in connection with content filtering and abuse monitoring, see Data, privacy,\nand security for Azure OpenAI Service .\nThe following sections provide information about the content filtering categories, the\nfiltering severity levels and their configurability, and API scenarios to be considered in\napplication design and implementation.\nThe content filtering system integrated in the Azure OpenAI Service contains neural\nmulti-class classification models aimed at detecting and filtering harmful content; the\nmodels cover four categories (hate, sexual, violence, and self-harm) across four severity\nlevels (safe, low, medium, and high). Content detected at the 'safe' severity level is\nlabeled in annotations but is not subject to filtering and is not configurable.\nCategor yDescr iptionContent filtering categories\nCategories", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "297e324dd5bd-0", "text": "Categor yDescr iption\nHate The hate category describes language attacks or uses that include pejorative or\ndiscriminatory language with reference to a person or identity group on the basis of\ncertain differentiating attributes of these groups including but not limited to race,\nethnicity, nationality, gender identity and expression, sexual orientation, religion,\nimmigration status, ability status, personal appearance, and body size.\nSexual The sexual category describes language related to anatomical organs and genitals,\nromantic relationships, acts portrayed in erotic or affectionate terms, physical sexual\nacts, including those portrayed as an assault or a forced sexual violent act against\none\u2019s will, prostitution, pornography, and abuse.\nViolence The violence category describes language related to physical actions intended to\nhurt, injure, damage, or kill someone or something; describes weapons, etc.\nSelf-\nHarmThe self-harm category describes language related to physical actions intended to\npurposely hurt, injure, or damage one\u2019s body, or kill oneself.\nCategor yDescr iption\nSafe Content may be related to violence, self-harm, sexual, or hate categories but the\nterms are used in general, journalistic, scientific, medical, and similar professional\ncontexts, which are appropriate for most audiences.\nLow Content that expresses prejudiced, judgmental, or opinionated views, includes\noffensive use of language, stereotyping, use cases exploring a fictional world (for\nexample, gaming, literature) and depictions at low intensity.\nMedium Content that uses offensive, insulting, mocking, intimidating, or demeaning language\ntowards specific identity groups, includes depictions of seeking and executing\nharmful instructions, fantasies, glorification, promotion of harm at medium intensity.\nHigh Content that displays explicit and severe harmful instructions, actions, damage, or", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "297e324dd5bd-1", "text": "High Content that displays explicit and severe harmful instructions, actions, damage, or\nabuse; includes endorsement, glorification, or promotion of severe harmful acts,\nextreme or illegal forms of harm, radicalization, or non-consensual power exchange\nor abuse.\nThe default content filtering configuration is set to filter at the medium severity\nthreshold for all four content harm categories for both prompts and completions. That\nmeans that content that is detected at severity level medium or high is filtered, while\ncontent detected at severity level low is not filtered by the content filters. TheSeverity levels\nConfigurability (preview)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "15d378b75935-0", "text": "configurability feature is available in preview and allows customers to adjust the\nsettings, separately for prompts and completions, to filter content for each content\ncategory at different severity levels as described in the table below:\nSeverity\nfilteredConfigurable\nfor pr omptsConfigurable\nfor\ncompletionsDescr iptions\nLow,\nmedium,\nhighYes Yes Strictest filtering configuration. Content detected at\nseverity levels low, medium and high is filtered.\nMedium,\nhighYes Yes Default setting. Content detected at severity level low\nis not filtered, content at medium and high is filtered.\nHigh If approved If approved Content detected at severity levels low and medium\nis not filtered. Only content at severity level high is\nfiltered. R equires approval .\nNo\nfiltersIf approved If approved No content is filtered regardless of severity level\ndetected. R equires approval .\n Only customers who have been approved for modified content filtering have full\ncontent filtering control, including configuring content filters at severity level high only\nor turning content filters off. Apply for modified content filters via this form: Azure\nOpenAI Limited Access R eview: Modified Content Filters and Abuse Monitoring\n(microsoft.com)\nContent filtering configurations are created within a R esource in Azure AI S tudio, and\ncan be associated with Deployments. Learn more about configurability here .* *\n*\n* *\n*\n*", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4c0eb2a67585-0", "text": "When the content filtering system detects harmful content, you'll receive either an error\non the API call if the prompt was deemed inappropriate or the finish_reason on the\nresponse will be content_filter to signify that some of the completion was filtered.\nWhen building your application or system, you'll want to account for these scenarios\nwhere the content returned by the Completions API is filtered, which may result in\ncontent that is incomplete. How you act on this information will be application specific.\nThe behavior can be summarized in the following points:\nPrompts that are classified at a filtered category and severity level will return an\nHTTP 400 error.\nNon-streaming completions calls won't return any content when the content is\nfiltered. The finish_reason value will be set to content_filter. In rare cases with\nlonger responses, a partial result can be returned. In these cases, the\nfinish_reason will be updated.\nFor streaming completions calls, segments will be returned back to the user as\nthey're completed. The service will continue streaming until either reaching a stop\ntoken, length, or when content that is classified at a filtered category and severity\nlevel is detected.\n\uf80a\nScenario details", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "889f1ac158ad-0", "text": "The table below outlines the various ways content filtering can appear:\nHTTP\nresponse\ncodeResponse behavior\n200 In the cases when all generation passes the filters as configured, no content\nmoderation details are added to the response. The finish_reason for each generation\nwill be either stop or length.\nExample r equest p ayload:\nJSON\nExample r esponse JSON:\nJSONScenario: You send a non-streaming completions call\nasking for multiple outputs; no content is classified at a\nfiltered category and severity level\n{ \n    \"prompt\" :\"Text example\" ,  \n    \"n\": 3, \n    \"stream\" : false \n} \n{ \n    \"id\": \"example-id\" ,\n    \"object\" : \"text_completion\" , \n    \"created\" : 1653666286 , \n    \"model\": \"davinci\" ,\n    \"choices\" : [ \n        {  \n            \"text\": \"Response generated text\" , \n            \"index\": 0,\n            \"finish_reason\" : \"stop\", \n            \"logprobs\" : null \n        }  \n    ] \n} \nScenario: Your API call asks for multiple responses (N>1)\nand at least 1 of the responses is filtered", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "5a11820a5e60-0", "text": "HTTP Response\nCodeResponse behavior\n200 The generations that were filtered will have a finish_reason value of\ncontent_filter.\nExample r equest p ayload:\nJSON\nExample r esponse JSON:\nJSON\nHTTP\nResponse\nCodeResponse behavior{ \n    \"prompt\" :\"Text example\" , \n    \"n\": 3, \n    \"stream\" : false \n} \n{ \n    \"id\": \"example\" , \n    \"object\" : \"text_completion\" , \n    \"created\" : 1653666831 , \n    \"model\": \"ada\", \n    \"choices\" : [ \n        {  \n            \"text\": \"returned text 1\" , \n            \"index\": 0,\n            \"finish_reason\" : \"length\" , \n            \"logprobs\" : null \n        },  \n        {  \n            \"text\": \"returned text 2\" , \n            \"index\": 1,\n            \"finish_reason\" : \"content_filter\" , \n            \"logprobs\" : null \n        }  \n    ] \n} \nScenario: An inappropriate input prompt is sent to the\ncompletions API (either for streaming or non-streaming)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "896094707188-0", "text": "HTTP\nResponse\nCodeResponse behavior\n400 The API call will fail when the prompt triggers a content filter as configured.\nModify the prompt and try again.\nExample r equest p ayload:\nJSON\nExample r esponse JSON:\nJSON\nHTTP\nResponse\nCodeResponse behavior\n200 In this case, the call will stream back with the full generation and finish_reason will\nbe either 'length' or 'stop' for each generated response.\nExample r equest p ayload:\nJSON{ \n    \"prompt\" :\"Content that triggered the filtering model\"  \n} \n\"error\": { \n    \"message\" : \"The response was filtered\" , \n    \"type\": null, \n    \"param\": \"prompt\" , \n    \"code\": \"content_filter\" , \n    \"status\" : 400 \n} \nScenario: You make a streaming completions call; no\noutput content is classified at a filtered category and\nseverity level\n{ \n    \"prompt\" :\"Text example\" , \n    \"n\": 3,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e0d03e5329be-0", "text": "Example r esponse JSON:\nJSON\nHTTP\nResponse\nCodeResponse behavior\n200 For a given generation index, the last chunk of the generation will include a non-null\nfinish_reason value. The value will be content_filter when the generation was\nfiltered.\nExample r equest p ayload:\nJSON\nExample r esponse JSON:    \"stream\" : true \n} \n{ \n    \"id\": \"cmpl-example\" , \n    \"object\" : \"text_completion\" , \n    \"created\" : 1653670914 , \n    \"model\": \"ada\", \n    \"choices\" : [ \n        {  \n            \"text\": \"last part of generation\" , \n            \"index\": 2,\n            \"finish_reason\" : \"stop\", \n            \"logprobs\" : null \n        }  \n    ] \n} \nScenario: You make a streaming completions call asking\nfor multiple completions and at least a portion of the\noutput content is filtered\n{ \n    \"prompt\" :\"Text example\" , \n    \"n\": 3, \n    \"stream\" : true \n}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f451a90808ff-0", "text": "JSON\nHTTP\nResponse\nCodeResponse behavior\n200 If the content filtering system is down or otherwise unable to complete the operation\nin time, your request will still complete without content filtering. Y ou can determine\nthat the filtering wasn't applied by looking for an error message in the\ncontent_filter_result object.\nExample r equest p ayload:\nJSON\nExample r esponse JSON:\nJSON { \n    \"id\": \"cmpl-example\" , \n    \"object\" : \"text_completion\" , \n    \"created\" : 1653670515 , \n    \"model\": \"ada\", \n    \"choices\" : [ \n        {  \n            \"text\": \"Last part of generated text streamed back\" , \n            \"index\": 2,\n            \"finish_reason\" : \"content_filter\" , \n            \"logprobs\" : null \n        }  \n    ] \n} \nScenario: Content filtering system doesn't run on the\ncompletion\n{ \n    \"prompt\" :\"Text example\" , \n    \"n\": 1, \n    \"stream\" : false \n} \n{ \n    \"id\": \"cmpl-example\" , \n    \"object\" : \"text_completion\" , \n    \"created\" : 1652294703 , \n    \"model\": \"ada\",", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f761fcfe5846-0", "text": "When annotations are enabled as shown in the code snippet below, the following\ninformation is returned via the API: content filtering category (hate, sexual, violence,\nself-harm); within each content filtering category, the severity level (safe, low, medium or\nhigh); filtering status (true or false).\nAnnotations are currently in preview for Completions and Chat Completions (GPT\nmodels); the following code snippet shows how to use annotations in preview:\nPython    \"choices\" : [ \n        {  \n            \"text\": \"generated text\" , \n            \"index\": 0,\n            \"finish_reason\" : \"length\" , \n            \"logprobs\" : null, \n            \"content_filter_result\" : { \n                \"error\": { \n                    \"code\": \"content_filter_error\" , \n                    \"message\" : \"The contents are not filtered\"  \n                }  \n            }  \n        }  \n    ] \n} \nAnno tations (preview)\n# Note: The openai-python library support for Azure OpenAI is in preview.  \n# os.getenv() for the endpoint and key assumes that you are using  \nenvironment variables.  \nimport os \nimport openai \nopenai.api_type = \"azure\" \nopenai.api_base = os.getenv( \"AZURE_OPENAI_ENDPOINT\" )  \nopenai.api_version = \"2023-06-01-preview\"  # API version required to test out  \nAnnotations preview  \nopenai.api_key = os.getenv( \"AZURE_OPENAI_KEY\" ) \nresponse = openai.Completion.create(  \n    engine= \"text-davinci-003\" , # engine = \"deployment_name\".  \n    prompt= \"{Example prompt where a severity level of low is detected}\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f761fcfe5846-1", "text": "prompt= \"{Example prompt where a severity level of low is detected}\"   \n    # Content that is detected at severity level medium or high is filtered,   \n    # while content detected at severity level low isn't filtered by the  \ncontent filters.  \n) \nprint(response)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a041eddbcf76-0", "text": "JSONOutput\n{ \n  \"choices\" : [ \n    { \n      \"content_filter_results\" : { \n        \"hate\": { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"self_harm\" : { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"sexual\" : { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"violence\" : { \n          \"filtered\" : false, \n          \"severity\" : \"low\" \n        }  \n      },  \n      \"finish_reason\" : \"length\" , \n      \"index\": 0, \n      \"logprobs\" : null,\n      \"text\": {\"\\\")(\\\"Example model response will be returned\\\").}\"  \n    } \n  ], \n  \"created\" : 1685727831 , \n  \"id\": \"cmpl-7N36VZAVBMJtxycrmiHZ12aK76a6v\" , \n  \"model\": \"text-davinci-003\" , \n  \"object\" : \"text_completion\" , \n  \"prompt_annotations\" : [ \n    { \n      \"content_filter_results\" : { \n        \"hate\": { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"self_harm\" : { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"sexual\" : { \n          \"filtered\" : false,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a041eddbcf76-1", "text": "\"sexual\" : { \n          \"filtered\" : false, \n          \"severity\" : \"safe\" \n        },  \n        \"violence\" : { \n          \"filtered\" : false,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "46072ba36274-0", "text": "The following code snippet shows how to retrieve annotations when content was\nfiltered:\nPython\nFor details on the inference REST API endpoints for Azure OpenAI and how to create\nChat and Completions please follow Azure OpenAI Service REST API reference guidance .          \"severity\" : \"safe\" \n        }  \n      },  \n      \"prompt_index\" : 0\n    } \n  ], \n  \"usage\": { \n    \"completion_tokens\" : 16, \n    \"prompt_tokens\" : 5,\n    \"total_tokens\" : 21 \n  } \n} \n# Note: The openai-python library support for Azure OpenAI is in preview.  \n# os.getenv() for the endpoint and key assumes that you are using  \nenvironment variables.  \nimport os \nimport openai \nopenai.api_type = \"azure\" \nopenai.api_base = os.getenv( \"AZURE_OPENAI_ENDPOINT\" )  \nopenai.api_version = \"2023-06-01-preview\"  # API version required to test out  \nAnnotations preview  \nopenai.api_key = os.getenv( \"AZURE_OPENAI_KEY\" ) \ntry: \n    response = openai.Completion.create(  \n        prompt= \"<PROMPT>\" , \n        engine= \"<MODEL_DEPLOYMENT_NAME>\" , \n    ) \n    print(response)  \nexcept openai.error.InvalidRequestError as e: \n    if e.error.code == \"content_filter\"  and e.error.innererror:  \n        content_filter_result = e.error.innererror.content_filter_result  \n        # print the formatted JSON  \n        print(content_filter_result)  \n        # or access the individual categories and details  \n        for category, details in content_filter_result.items():", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "46072ba36274-1", "text": "for category, details in content_filter_result.items():  \n            print( f\"{category} :\\n filtered= {details[ 'filtered' ]}\\n severity=\n{details[ 'severity' ]}\")", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "aaca05d000eb-0", "text": "Annotations are returned for all scenarios when using 2023-06-01-preview.\nJSON\nAs part of your application design, consider the following best practices to deliver a\npositive experience with your application while minimizing potential harms:Example scenario: An input prompt containing content\nthat is classified at a filtered category and severity level is\nsent to the completions API\n{ \n    \"error\": { \n        \"message\" : \"The response was filtered due to the prompt triggering  \nAzure Content management policy. \n                   Please modify your prompt and retry. To learn more about  \nour content filtering policies  \n                   please read our documentation:  \nhttps://go.microsoft.com/fwlink/?linkid=21298766\" , \n        \"type\": null, \n        \"param\": \"prompt\" , \n        \"code\": \"content_filter\" , \n        \"status\" : 400, \n        \"innererror\" : { \n            \"code\": \"ResponsibleAIPolicyViolation\" , \n            \"content_filter_result\" : { \n                \"hate\": { \n                    \"filtered\" : true, \n                    \"severity\" : \"high\" \n                },  \n                \"self-harm\" : { \n                    \"filtered\" : true, \n                    \"severity\" : \"high\" \n                },  \n                \"sexual\" : { \n                    \"filtered\" : false, \n                    \"severity\" : \"safe\" \n                },  \n                \"violence\" : { \n                    \"filtered\" :true, \n                    \"severity\" : \"medium\"  \n                }  \n            }  \n        }  \n    } \n} \nBest practices", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "128997d99bd3-0", "text": "Decide how you want to handle scenarios where your users send prompts\ncontaining content that is classified at a filtered category and severity level or\notherwise misuse your application.\nCheck the finish_reason to see if a completion is filtered.\nCheck that there's no error object in the content_filter_result (indicating that\ncontent filters didn't run).\nLearn more about the underlying models that power Azure OpenAI .\nApply for modified content filters via this form .\nAzure OpenAI content filtering is powered by Azure AI Content Safety .\nLearn more about understanding and mitigating risks associated with your\napplication: Overview of R esponsible AI practices for Azure OpenAI models .\nLearn more about how data is processed in connection with content filtering and\nabuse monitoring: Data, privacy, and security for Azure OpenAI Service .Next steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "30659331225f-0", "text": "Understanding embeddings in Azure\nOpenAI Service\nArticle \u202205/10/2023\nAn embedding is a special format of data representation that can be easily utilized by\nmachine learning models and algorithms. The embedding is an information dense\nrepresentation of the semantic meaning of a piece of text. Each embedding is a vector\nof floating-point numbers, such that the distance between two embeddings in the\nvector space is correlated with semantic similarity between two inputs in the original\nformat. For example, if two texts are similar, then their vector representations should\nalso be similar.\nDifferent Azure OpenAI embedding models are specifically created to be good at a\nparticular task. Similarity embeddings  are good at capturing semantic similarity\nbetween two or more pieces of text. Text sear ch embeddings  help measure whether\nlong documents are relevant to a short query. Code sear ch embeddings  are useful for\nembedding code snippets and embedding natural language search queries.\nEmbeddings make it easier to do machine learning on large inputs representing words\nby capturing the semantic similarities in a vector space. Therefore, we can use\nembeddings to determine if two text chunks are semantically related or similar, and\nprovide a score to assess similarity.\nAzure OpenAI embeddings rely on cosine similarity to compute similarity between\ndocuments and a query.\nFrom a mathematic perspective, cosine similarity measures the cosine of the angle\nbetween two vectors projected in a multi-dimensional space. This is beneficial because if\ntwo documents are far apart by Euclidean distance because of size, they could still have\na smaller angle between them and therefore higher cosine similarity. For more\ninformation about cosine similarity equations, see this article on Wikipedia .\nAn alternative method of identifying similar documents is to count the number of\ncommon words between documents. Unfortunately, this approach doesn't scale since an\nexpansion in document size is likely to lead to a greater number of common wordsEmbedding models\nCosine similarity", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "05abda04af5b-0", "text": "detected even among completely disparate topics. For this reason, cosine similarity can\noffer a more effective alternative.\nLearn more about using Azure OpenAI and embeddings to perform document search\nwith our embeddings tutorial .Next steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fd87a1e9f31a-0", "text": "Introduction to red teaming large\nlanguage mo dels (LLMs)\nArticle \u202205/19/2023\nThe term red teaming  has historically described systematic adversarial attacks for testing\nsecurity vulnerabilities. With the rise of LLMs, the term has extended beyond traditional\ncybersecurity and evolved in common usage to describe many kinds of probing, testing,\nand attacking of AI systems. With LLMs, both benign and adversarial usage can produce\npotentially harmful outputs, which can take many forms, including harmful content such\nas hate speech, incitement or glorification of violence, or sexual content.\nRed teaming is an essential practice in the r esponsible dev elopment o f syst ems and\nfeatur es using LLMs . While not a replacement for systematic measurement and\nmitigation  work, red teamers help to uncover and identify harms and, in turn, enable\nmeasurement strategies to validate the effectiveness of mitigations.\nMicrosoft has conducted red teaming exercises and implemented safety systems\n(including content filters  and other mitigation strategies ) for its Azure OpenAI Service\nmodels (see this Responsible AI Overview ). However, the context of your LLM\napplication will be unique and you also should conduct red teaming to:\nTest the LLM base model and determine whether there are gaps in the existing\nsafety systems, given the context of your application system.\nIdentify and mitigate shortcomings in the existing default filters or mitigation\nstrategies.\nProvide feedback on failures so we can make improvements.\nHere is how you can get started in your process of red teaming LLMs. Advance planning\nis critical to a productive red teaming exercise.\nAssemble a div erse gr oup o f red teamer s.\nLLM red teamers should be a mix of people with diverse social and professional\nbackgrounds, demographic groups, and interdisciplinary expertise that fits the", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fd87a1e9f31a-1", "text": "backgrounds, demographic groups, and interdisciplinary expertise that fits the\ndeployment context of your AI system. For example, if you\u2019re designing a chatbot to\nhelp health care providers, medical experts can help identify risks in that domain.Getting  started\nManaging your red team", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6f4b8eb2b367-0", "text": "Recruit r ed teamer s with both benign and adv ersarial mindsets.\nHaving red teamers with an adversarial mindset and security-testing experience is\nessential for understanding security risks, but red teamers who are ordinary users of\nyour application system and haven\u2019t been involved in its development can bring\nvaluable perspectives on harms that regular users might encounter.\nRemember that handling pot entially harmful cont ent can be mentally taxing.\nYou will need to take care of your red teamers, not only by limiting the amount of time\nthey spend on an assignment, but also by letting them know they can opt out at any\ntime. Also, avoid burnout by switching red teamers\u2019 assignments to different focus\nareas.\nBecause a system is developed using a LLM base model, you may need to test at several\ndifferent layers:\nThe LLM base model with its safety system  in place to identify any gaps that may\nneed to be addressed in the context of your application system. (T esting is usually\nthrough an API endpoint.)\nYour application system. (T esting is usually through a UI.)\nBoth the LLM base model and your application system before and after mitigations\nare in place.\nConsider conducting iterative red teaming in at least two phases:\n1. Open-ended red teaming, where red teamers are encouraged to discover a variety\nof harms. This can help you develop a taxonomy of harms to guide further testing.\nNote that developing a taxonomy of undesired LLM outputs for your application\nsystem is crucial to being able to measure the success of specific mitigation efforts.\n2. Guided red teaming, where red teamers are assigned to focus on specific harms\nlisted in the taxonomy while staying alert for any new harms that may emerge. R ed\nteamers can also be instructed to focus testing on specific features of a system for\nsurfacing potential harms.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6f4b8eb2b367-1", "text": "surfacing potential harms.\nBe sure to:Planning your red teaming\nWhere to test\nHow to test", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bc1c1ddf3e21-0", "text": "Provide your red teamers with clear instructions for what harms or system features\nthey will be testing.\nGive your red teamers a place for recording their findings. For example, this could\nbe a simple spreadsheet specifying the types of data that red teamers should\nprovide, including basics such as:\nThe type of harm that was surfaced.\nThe input prompt that triggered the output.\nAn excerpt from the problematic output.\nComments about why the red teamer considered the output problematic.\nMaximize the effort of responsible AI red teamers who have expertise for testing\nspecific types of harms or undesired outputs. For example, have security subject\nmatter experts focus on jailbreaks, metaprompt extraction, and content related to\naiding cyberattacks.\nYou will want to summarize and report red teaming top findings at regular intervals to\nkey stakeholders, including teams involved in the measurement and mitigation of LLM\nfailures so that the findings can inform critical decision making and prioritizations.\nLearn about other mitigation strategies like prompt engineeringReporting red teaming findings\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cd08f2f92e6f-0", "text": "Introduction to prompt engineering\nArticle \u202203/21/2023\nGPT-3, GPT-3.5, and GPT-4 models from OpenAI are prompt-based. With prompt-based\nmodels, the user interacts with the model by entering a text prompt, to which the model\nresponds with a text completion. This completion is the model\u2019s continuation of the\ninput text.\nWhile these models are extremely powerful, their behavior is also very sensitive to the\nprompt. This makes prompt construction an important skill to develop.\nPrompt construction can be difficult. In practice, the prompt acts to configure the model\nweights to complete the desired task, but it's more of an art than a science, often\nrequiring experience and intuition to craft a successful prompt. The goal of this article is\nto help get you started with this learning process. It attempts to capture general\nconcepts and patterns that apply to all GPT models. However it's important to\nunderstand that each model behaves differently, so the learnings may not apply equally\nto all models.\nThis section covers the basic concepts and elements of GPT prompts.\nText prompts are how users interact with GPT models. As with all generative language\nmodels, GPT models attempt to produce the next series of words that are most likely to\nfollow from the previous text. It's as if we're saying What is the f irst thing that c omes t o\nyour mind when I s ay <prompt>?\u201d. The examples below demonstrate this behavior. Given\nthe first words of famous content, the model is able to accurately continue the text.\nPrompt Completion\nFour score\nand seven\nyears ago\nourfathers brought forth on this continent, a new nation, conceived in Liberty, and\ndedicated to the proposition that all men are created equal. [\u2026]\n\"Call me", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cd08f2f92e6f-1", "text": "dedicated to the proposition that all men are created equal. [\u2026]\n\"Call me\nIshmael.\"\"Some years ago\u2014never mind how long precisely\u2014having little o no money in my\npurse, and nothing particular to interest me on shore, I thought I would sail about\na little and see the watery part of the world.\" [\u2026]\nAs you develop more complex prompts, it's helpful to keep this fundamental behavior in\nmind. R egardless of the prompt that is provided, the model is simply responding withBasics", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9a0514f7d702-0", "text": "what it determines is most likely (given its training data and training targets). If you ask\na question in your prompt, the model isn't following a separate \u201cQ&A\u201d code path, but\nrather it appears to answer the question because an answer is the most likely sort of\nresponse for the given question as input.\nWhen using the Completion API while there's no differentiation between different parts\nof the prompt, it can still be useful for learning and discussion to identify underlying\nprompt components. With the Chat Completion API  there are distinct sections of the\nprompt that are sent to the API in the form of an array of dictionaries with associated\nroles: system, user, and assistant. This guidance focuses more generally on how to think\nabout prompt construction rather than providing prescriptive guidance that is specific to\none API over another.\nIt's also important to understand that while there could be other valid ways to dissect\nprompts, the goal of this breakdown is to provide a relatively simple way to think about\nprompt construction. With the Completion API, all the components are optional, but at\nleast one must be present and most prompts include more than one component. There\ncan be some grey area between components as well. The order presented below\nroughly corresponds to how commonly each component is used, from most to least.\nInstructions are likely the most commonly used prompt component. Instructions are\nsimply that: instructions to the model on what to do. While simple in concept, they can\nbe complex in practice. The following table illustrates this by showing both simple and\nmore complex instructions, in two different forms.\nInstr uction\nTypePrompt Completion\nSimple\ninstructionsWrite an introduction for\na weekly newsletter.Welcome to our weekly newsletter. This week we have\na special focus on productivity. W e'll be sharing tips\nand tricks on how to be productive in all areas of your", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9a0514f7d702-1", "text": "and tricks on how to be productive in all areas of your\nlife. W e hope you enjoy and please share with your\nfriends!Prompt components\nInstructions", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0633fb377e16-0", "text": "Instr uction\nTypePrompt Completion\nComplex\ninstructionsWrite an introduction for\na weekly newsletter to\nmy company named\nContoso. It should\nmention the great all\nhands meeting we had\nlast week and thank the\nteam for all their hard\nwork over the tough past\nfew months.Welcome to Contoso's weekly newsletter! W e had a\ngreat all hands meeting last week and I want to thank\nthe team for all their hard work over the tough past\nfew months. W e're looking forward to a bright future\nand I'm confident that with our team's dedication and\ncommitment, we will achieve great things. Thank you\nfor being a part of our team and I look forward to\nworking with you all in the coming weeks and months.\nSincerely, [Y our name].\nComplex\nInstructionsWrite an introduction for\na weekly newsletter,\nincluding the following:  \n- My company is named\ncontoso  \n- An exuberant greeting  \n- Thanking the team for\ntheir hard work over a\ncouple tough months  \n- Positive outlook for the\ncoming quarter  \n- Signed by the SL THello Contoso team! I wanted to take a moment to\nthank you for all your hard work over the past few\nmonths. It's been a tough couple of months, but you've\nall pulled together and done an amazing job. I'm really\nproud of you all. Looking ahead to the next quarter, I'm\nfeeling really positive. I think we're going to achieve\nsome great things. Thanks again, The Senior Leadership\nTeam.\nPrimary content refers to some sort of text that is being processed or transformed by\nthe model. Primary content is typically used in conjunction with instructions. A simple\nexample would be language translation. In the example below, the English text would be", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0633fb377e16-1", "text": "example would be language translation. In the example below, the English text would be\nconsidered the primary content while \u201cT ranslate to French:\u201d is an instruction.\nPrompt Completion\nCan you please tell me how to get to the\nmuseum?  \nTranslate to French:Pouvez-vous s'il vous pla\u00eet me dire comment aller\nau mus\u00e9e?\nPrimary content can also be much longer. In the following example, the primary content\nis the introduction section from the Wikipedia entry on time complexity , which is\nalmost 300 words long. This content has been abbreviated in our table for ease of\ndisplay.\nPrompt CompletionPrimary Content", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b00eb385174f-0", "text": "Prompt Completion\nIn computer science, the time\ncomplexity is the\ncomputational complexity that\ndescribes the amount of\ncomputer time it takes to run\nan algorithm. Time complexity\nis commonly estimated by [...]  \nSummarize the previous text in\na few sentences:Time complexity is a measure of how long an algorithm takes\nto run. It is typically expressed as a function of the size of the\ninput. The most common measure is worst-case time\ncomplexity, which is the maximum amount of time required for\ninputs of a given size.\nIn computer science, the time\ncomplexity is the\ncomputational complexity that\ndescribes the amount of\ncomputer time it takes to run\nan algorithm. Time complexity\nis commonly estimated by [...]  \nTL;DR (Too long didn't read)The time complexity of an algorithm is the amount of time it\ntakes to run an algorithm. Time complexity is commonly\nestimated by counting the number of elementary operations\nperformed by the algorithm, supposing that each elementary\noperation takes a fixed amount of time to perform. The time\ncomplexity is generally expressed as a function of the size of\nthe input.\nGPT models can also handle primary content that is structured. In the example below, a\nTSV about beers in Belgium (abbreviated for display in this article) is passed to the\nmodel as part of the prompt. It is capable of properly interpreting the content and\nanswering questions about the data.\nPrompt Completion\nBelgian Beer Brands  \nBeer name Beer style color ABV Brewery \n\"Chimay Gold: \"\"Doree\"\"\" trappist pale ale gold 4.80%\nChimay Brewery (official trappist) \n\"Chimay Blue: \"\"Grande Reserve\"\"\" trappist dark ale\ndark 9.00% \n[\u2026]", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b00eb385174f-1", "text": "dark 9.00% \n[\u2026] \nReferencing the table above, how many beers are less than\n6% ABV?There are 10 beers in the table that\nare less than 6% ABV.\nSuccessful prompts often rely on the practice of \u201cone-shot\u201d or \u201cfew-shot\u201d learning. This\nrefers to the inclusion of one or more examples of the desired behavior of the model,\ntypically by including input and output pairs. This is not learning in the sense that the\nmodel is permanently changed, but rather that the examples better condition the model\nto respond as desired for only the current inference. The use of prompts with no\nexamples is sometimes referred to as \u201czero-shot\u201d learning. Note that with the ChatExamples", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6366e469dd60-0", "text": "Completion API few-shot learning examples are typically added to the messages array in\nthe form of example user/assistant interactions after the initial system message.\nLearning\nTypePrompt Completion\nZero-\nshotHeadline: Coach\nconfident injury\nwon't derail\nWarriors  \nTopic:The coach is confident that the injury won't derail the W arriors'\nseason. The team is still focused on their goals and that they\nwill continue to work hard to achieve them.\nFew-shot Headline: T wins'\nCorrea to use opt-\nout, test free agency  \nTopic: Baseball  \nHeadline: Qatar\nWorld Cup to have\nzones for sobering\nup \nTopic: Soccer  \nHeadline: Y ates:\nFantasy football\nintel for W eek 6  \nTopic: Football  \nHeadline: Coach\nconfident injury\nwon't derail\nWarriors  \nTopic:Basketball\nThe example above illustrates the utility of few-shot learning. Without the examples, the\nmodel seems to be guessing at the desired behavior, while the examples cleanly show\nthe model how to operate. This also demonstrates the power of the model, it can infer\nthe category of label that is wanted, even without a \u2018basketball\u2019 label in the examples.\nCues act as the \u201cjumpstart\u201d for the output of the model, helping to direct the model to\nthe desired output. It is often a prefix that the model can build onto. In the example\nbelow, we show the cue directing the story to include key elements. Cues are often used\nalong with instructions, but not always. In the examples below, the model is being used\nto summarize a long announcement email. A cue is used in one case to help focus the\noutput of the model and to suggest a specific output format (bullet points) in the other.Cue", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e6330d80c44f-0", "text": "In the context of the Chat Completion API instructions take the form of the system\nmessage, and few-shot examples are indicated by providing a series of example\nuser/assistant interactions to help prime the model.\nNumber\nof CuesPrompt Completion\n0 Performance\nreviews are a\ncore\ncomponent of\nour\nPerformance &\nDevelopment\napproach at\nContoso. [\u2026]  \nSummarize the\nabove email\nmessage:The purpose of this email is to inform employees of recent changes\nto the performance review experience, which is part of Contoso's\nPerformance & Development approach. The changes include an\nimproved structure, upgraded in-tool user experience, and focused\nprompts. Additionally, there is now an optional Conversation\nStarters section for employees to flag topics they'd like to discuss\nwith their managers.\n1 Performance\nreviews are a\ncore\ncomponent of\nour\nPerformance &\nDevelopment\napproach at\nContoso. [\u2026]  \nSummarize the\nabove email\nmessage:  \nThe key\ntakeaway of the\nmessage isthat Contoso is improving the performance review experience by\nmaking it more focused on core priorities, expectations and impact.\nAdditionally, they are upgrading the in-tool user experience with\ntips, resources and guidance. Finally, they are adding guidance on\nhow to activate company-wide core priorities.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "701634188aba-0", "text": "Number\nof CuesPrompt Completion\n2 Performance\nreviews are a\ncore\ncomponent of\nour\nPerformance &\nDevelopment\napproach at\nContoso. [\u2026]  \nSummarize the\nabove email\nmessage:  \nKey Points:  \n\u2022Contoso is introducing changes to the performance review\nexperience, based on feedback from employees.  \n\u2022 The changes include a new focus on core priorities, expectations,\nand impact; an upgraded user experience; and new guidance on\ncompany-wide core priorities.  \n\u2022 Performance reviews are a time to reflect on past performance and\nlearning as well as to plan for future priorities and growth\nopportunities.  \n\u2022 Employees and managers should write comments in advance of\nthe performance review meeting.  \n\u2022 There should be at least two performance reviews per year.  \n\u2022 The time investment for each review should remain about the\nsame.\nSupporting content is information that the model can utilize to influence the output in\nsome way. It differs from primary content in that it is not the main target of task, but it is\ntypically used along with primary content. Common examples include contextual\ninformation such as the current date, the name of the user, user preferences, and so on.\nThe example below uses supporting content to help organize a set of planned\nworkshops for the user. Without the supporting (important topics) the model simply lists\noff the workshops (truncated for display), when informed of my important topics, the\nmodel is able to accurately group the meetings.\nSuppor ting\nCont entPrompt CompletionSupporting content", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3b283e81371f-0", "text": "Suppor ting\nCont entPrompt Completion\nWithout Workshop list:  \nTitle Description  \nPlanning W orkshop Kickoff Please join us to kick off our workshop!\nHear from our leaders about cross group priorities and learnings for\nthe year ahead. The invitation to this event has already been sent to\neveryone in [\u2026]  \nSummarize the workshops above, grouped by topic:Workshop\nlist:  \nPlanning\nWorkshop\nKickoff  \n-Planning\nWorkshop\nWrap-up  \n- Core\nMeetUp  \n- Customer\nDriven\nPerspectives  \n-Impact and\nKey Focus\nAreas  \n-Best\npractices  \n-\nAccelerating\napp\nadoption\n[\u2026]\nWith Workshop list:  \nTitle Description  \nPlanning W orkshop Kickoff Please join us to kick off our workshop!\nHear from our leaders about cross group priorities and learnings for\nthe year ahead. The invitation to this event has already been sent to\neveryone in [\u2026]  \nMy Important T opics: prompt engineering, search, GPT models  \nSummarize the workshops above, grouped by my important topics:1. Prompt\nEngineering:\n- New\nprompt\nengineering\ntips \n- The art of\nprompt\nengineering  \n2. Search:  \n-Intro to\nvector\nsearch with\nembedding  \n3. GPT\nModels:  \n- Intro to\nGPT-4  \n- GPT-35-\nTurbo in-\ndepth.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "01831436148c-0", "text": "Be Specific . Leave as little to interpretation as possible. R estrict the operational\nspace.\nBe Descriptiv e. Use analogies.\nDouble Down . Sometimes you may need to repeat yourself to the model. Give\ninstructions before and after your primary content, use an instruction and a cue,\netc.\nOrder Matt ers. The order in which you present information to the model may\nimpact the output. Whether you put instructions before your content (\u201csummarize\nthe following\u2026\u201d) or after (\u201csummarize the above\u2026\u201d) can make a difference in\noutput. Even the order of few-shot examples can matter. This is referred to as\nrecency bias.\nGive the model an \u201cout\u201d . It can sometimes be helpful to give the model an\nalternative path if it is unable to complete the assigned task. For example, when\nasking a question over a piece of text you might include something like \"respond\nwith \u2018not found\u2019 if the answer is not present.\" This can help the model avoid\ngenerating false responses.\nWhile the input size increases with each new generation of GPT models, there will\ncontinue to be scenarios that provide more data than the model can handle. GPT\nmodels break words into \u201ctokens\u201d. While common multi-syllable words are often a\nsingle token, less common words are broken in syllables. T okens can sometimes be\ncounter-intuitive, as shown by the example below which demonstrates token boundaries\nfor different date formats. In this case, spelling out the entire month is more space\nefficient than a fully numeric date. The current range of token support goes from 2000\ntokens with earlier GPT-3 models to up to 32,768 tokens with the 32k version of the\nlatest GPT-4 model.\nGiven this limited space, it is important to use it as efficiently as possible.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "01831436148c-1", "text": "Given this limited space, it is important to use it as efficiently as possible.\nTables \u2013 As shown in the examples in the previous section, GPT models can\nunderstand tabular formatted data quite easily. This can be a space efficient way to\ninclude data, rather than preceding every field with name (such as with JSON).Best practices\nSpace efficiency\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cf58e8976dc1-0", "text": "White Space \u2013 Consecutive whitespaces are treated as separate tokens which can\nbe an easy way to waste space. Spaces preceding a word, on the other hand, are\ntypically treated as part of the same token as the word. Carefully watch your usage\nof whitespace and don\u2019t use punctuation when a space alone will do.\nLearn more about Azure OpenAINext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "969d96256a01-0", "text": "Prompt engineering techniques\nArticle \u202204/23/2023\nThis guide will walk you through some advanced techniques in prompt design and\nprompt engineering. If you're new to prompt engineering, we recommend starting with\nour introduction to prompt engineering guide .\nWhile the principles of prompt engineering can be generalized across many different\nmodel types, certain models expect a specialized prompt structure. For Azure OpenAI\nGPT models, there are currently two distinct APIs where prompt engineering comes into\nplay:\nChat Completion API.\nCompletion API.\nEach API requires input data to be formatted differently, which in turn impacts overall\nprompt design. The Chat Completion API  supports the GPT-35-Turbo and GPT-4\nmodels. These models are designed to take input formatted in a specific chat-like\ntranscript  stored inside an array of dictionaries.\nThe Completion API  supports the older GPT-3 models and has much more flexible input\nrequirements in that it takes a string of text with no specific format rules. T echnically the\nGPT-35-Turbo models can be used with either APIs, but we strongly recommend using\nthe Chat Completion API for these models. T o learn more, please consult our in-depth\nguide on using these APIs .\nThe techniques in this guide will teach you strategies for increasing the accuracy and\ngrounding of responses you generate with a Large Language Model (LLM). It is,\nhowever, important to remember that even when using prompt engineering effectively\nyou still need to validate the responses the models generate. Just because a carefully\ncrafted prompt worked well for a particular scenario doesn't necessarily mean it will\ngeneralize more broadly to certain use cases. Understanding the limitations of LLMs , is\njust as important as understanding how to leverage their strengths.\nThis guide does not go in-depth into the mechanics behind the message structure for", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "969d96256a01-1", "text": "This guide does not go in-depth into the mechanics behind the message structure for\nChat Completions. If you aren't familiar with interacting with the ChatGPT and GPT-4\nmodels programmatically we recommend reading our how-to guide on the Chat\nCompletion API first .\n\uff17 Note", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "02c0b01a7a82-0", "text": "The system message is included at the beginning of the prompt and is used to prime\nthe model with context, instructions, or other information relevant to your use case. Y ou\ncan use the system message to describe the assistant\u2019s personality, define what the\nmodel should and shouldn\u2019t answer, and define the format of model responses.\nThe example below, shows a sample system message and the resulting model response:\nSystem message User Assistant\nYou are an AI assistant that helps people find information\nand responds in rhyme. If the user asks you a question you\ndon't know the answer to, say so.What can you\ntell about me,\nJohn Doe?Dear John, I'm\nsorry to say,  \nBut I don't have\ninfo on you today.  \nI'm just an AI with\nknowledge in my\nbrain,  \nBut without your\ninput, I can't\nexplain.  \nSo please tell me\nmore about what\nyou seek,  \nAnd I'll do my best\nto give you an\nanswer unique.\nSome other examples of system messages are:\n\u201cAssistant is a large language model trained by OpenAI.\u201d\n\u201cAssistant is an intelligent chatbot designed to help users answer technical\nquestions about Azure OpenAI Service. Only answer questions using the context\nbelow and if you're not sure of an answer, you can say \"I don't know\".\n\u201cAssistant is an intelligent chatbot designed to help users answer their tax related\nquestions.\u201d\n\u201cYou are an assistant designed to extract entities from text. Users will paste in a\nstring of text and you will respond with entities you've extracted from the text as aAll of the examples in this section of the guide were tested against a base GPT-4\nmodel in English. If you are reading a localized version of this article in another\nlanguage, these responses represent a localized translation of the English results. T o", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "02c0b01a7a82-1", "text": "language, these responses represent a localized translation of the English results. T o\nlearn more about potential limitations depending on what language you are using\nto prompt a model, please consult our Responsible AI transp arency not e.\nSystem message", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "26e43ff082ac-0", "text": "JSON object. Here's an example of your output format:\nJSON\nAn important detail to understand is that even if you instruct a model in the system\nmessage to answer I don 't know  when unsure of an answer this does not guarantee that\nthe request will be honored. A well designed system message can increase the\nlikelihood of a certain outcome, but it is still possible that an incorrect response could\nbe generated that contradicts the intent of the instruction in the system message.\nA common way to adapt language models to new tasks is to use few-shot learning. In\nfew-shot learning, a set of training examples is provided as part of the prompt to give\nadditional context to the model.\nWhen using the Chat Completions API, a series of messages between the User and\nAssistant (written in the new prompt format ), can serve as examples for few-shot\nlearning. These examples can be used to prime the model to respond in a certain way,\nemulate particular behaviors, and seed answers to common questions.\nSystem message Few-shot examples\nAssistant is an intelligent chatbot\ndesigned to help users answer\ntheir tax related questions.  \nInstructions  \n- Only answer questions related to\ntaxes.  \n- If you're unsure of an answer, you\ncan say \"I don't know\" or \"I'm not\nsure\" and recommend users go to\nthe IRS website for more\ninformation.User - \"When do I need to file my taxes by?\"  \nAssistant  - \"In 2023, you will need to file your taxes by April\n18th. The date falls after the usual April 15th deadline\nbecause April 15th falls on a Saturday in 2023. For more\ndetails, see https://www.irs.gov/filing/individuals/when-to-\nfile  \"  \nUser - \"How can I check the status of my tax refund?\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "26e43ff082ac-1", "text": "User - \"How can I check the status of my tax refund?\"  \nAssistant  - \"You can check the status of your tax refund by\nvisiting https://www.irs.gov/refunds  \"\nThe table above describes the basic prompt structure, but for a detailed explanation of\nthe exact prompt format you will need to refer to the Chat Completion guide .{   \n   \"name\": \"\", \n   \"company\" : \"\", \n   \"phone_number\" : \"\" \n} \nFew-shot learning", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fa4ff5f458b1-0", "text": "While the Chat Completion API is optimized to work with multi-turn conversations, it\nalso can be used for non chat scenarios. For example, for a sentiment analysis scenario,\nyou might use the following prompt:\nSystem\nmessageUser Assistant\nYou are an\nassistant\ndesigned to\nanalyze\nsentiment\nfrom speech\ndata. Users\nwill paste in\na string of\ntext and you\nwill respond\nwith an\nassessment\nof the\nresponse of\nthe speaker.\nRate on a\nscale of 1-10\n(10 being\nhighest).\nExplain why\nthis rating\nwas given.hi thank you for calling contoso pharmacy who am i\nspeaking with today i understand your frustration mary um\ncan you tell me what exactly you're trying to accomplish\nOK uh certainly happy to check that for you one moment\nplease i see here that you were on a generic form of lipitor\nis that right OK uh so i see that your doctor stopped\nprescribing these drugs in twenty twenty one actually OK\nuh yeah i'd be happy to check that for you uh because\nsometimes there's a gap in the system and it just doesn't\nget reported so let me take a look here just a moment so\ni'm seeing here that your doctor had these drugs\nprescribed to you from twenty twelve through twenty\ntwenty one OK um yeah according to the latest records uh\nprovided by doctor's office you're now on a different drug\nmetformin would you like us to go ahead and fill that\nprescription for you for pick up a bit of form in five\nhundred milligrams uh you're very welcome mary please let\nus know if there's anything else we can do for you today.Sentiment rating: 8  \nThe speaker in this\ntext seems to be\npolite, helpful, and\npatient while\naddressing the\nconcerns of the\ncustomer. They take", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fa4ff5f458b1-1", "text": "patient while\naddressing the\nconcerns of the\ncustomer. They take\nthe time to check\nthe information,\nprovide clear\nexplanations, and\noffer assistance.\nHowever, the rating\nis not a 10 because\nthere is still a hint of\nuncertainty in some\nparts of the\nconversation, which\nmay not convey\ncomplete\nconfidence to the\ncustomer.\nThe sequence in which information appears in the prompt matters. This is because GPT\nstyle models are built in a certain way that defines how they process the input. Our\nresearch suggests that telling the model the task you want it to do at the beginning of\nthe prompt, before sharing additional contextual information or examples, can help\nproduce higher-quality outputs.Non chat scenarios\nStart with clear instructions\n\uff17 Note\nAlthough following this technique is still generally recommended, in contrast to\nprevious model versions (GPT-3 and prior), our testing showed that the model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b5997303c45d-0", "text": "System\nmessageUser Assistant\nYou are an\nAI assistant\nthat helps\npeople find\ninformation.Your task is to verify if the\nstatement \"Several sources\nmention a chance of another\nlarge eruption\" is supported by a\nspecific quote from the following\nset of snippets.  \n--- \nSNIPPET S \n[1] 14 percent chance of\nmegaquake hitting Seattle,\nexperts say  \nSEATTLE - There's a 14 percent\nchance of a magnitude 9\nCascadia earthquake hitting\nSeattle in the next 50 years, the\nU.S. Geological Survey estimates.\n\"Unfortunately, we are unable\nto... \n[2] Earthquake experts lay out\nlatest outlook for Seattle's 'R eally\nBig One\u2019  \n\u201cWe say that there's\napproximately a 14% chance of\nanother approximately\nmagnitude-9 earthquake\noccurring in the next 50 years,\u201d\nsaid a geophysicist at the\nUniversity of W ashington...  \n--- \nIs the statement \"Several sources\nmention a chance of another\nlarge eruption\" directly implied or\nstated by the snippets?No, the statement is not directly implied or\nstated by the snippets. The snippets mention a\nchance of a \"megaquake\" and a \"magnitude 9\nCascadia earthquake\" hitting Seattle in the\nnext 50 years, but do not mention a chance of\nanother large eruption.response with ChatGPT and GPT-4 models was the same regardless of whether the\ntechnique is utilized. In the case of the below example, we see that adding the\nstatement \u201cSeveral sources \u2026 eruption\u201d at the beginning or the end of the prompt\ndid not cause any change in the final model response.\nRepeat instructions at the end", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "819541c2e426-0", "text": "Models can be susceptible to recency bias , which in this context means that information\nat the end of the prompt might have more significant influence over the output than\ninformation at the beginning of the prompt. Therefore, it is worth experimenting with\nrepeating the instructions at the end of the prompt and evaluating the impact on the\ngenerated response.\nThis refers to including a few words or phrases at the end of the prompt to obtain a\nmodel response that follows the desired form. For example, using a cue such as \u201cHere\u2019s\na bulleted list of key points:\\n- \u201d can help make sure the output is formatted as a\nlist of bullet points.\nSystem message User Assistant\nYou are an AI assistant\nthat helps people find\ninformation. Answer in\nas few words as\npossible.John Smith is married to Lucy Smith. They have five\nkids, and he works as a software engineer at\nMicrosoft. What search queries should I do to fact-\ncheck this? ## One possible search query is:\"John Smith\nmarried Lucy\nSmith five kids\nsoftware\nengineer\nMicrosoft\"\nIn the above prompt, the text One possible s earch quer y is: primes the model to produce\na single output. Without this cue the model produces several search queries as output.\nUsing clear syntax for your prompt\u2014including punctuation, headings, and section\nmarkers\u2014helps communicate intent and often makes outputs easier to parse.\nIn the example below, separators ( --- in this case) have been added between different\nsources of information or steps. This allows the use of --- as a stopping condition for\ngeneration. In addition, section headings or special variables are presented in uppercase\nto differentiate them.\nSystem\nmessageUser AssistantPrime the output\nAdd clear syntax", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c48daec25e59-0", "text": "System\nmessageUser Assistant\nYou are an AI\nassistant that\nhelps people\nfind information.You will read a paragraph, and then issue queries\nto a search engine in order to fact-check it. Also\nexplain the queries.  \n--- \nPARAGRAPH  \nJohn Smith is married to Lucy Smith. They have\nfive kids, and he works as a software engineer at\nMicrosoft. What search queries should I do to\nfact-check this?  \n--- \nQUERIES1. \"John Smith Microsoft\"\n- To check if John Smith\nis indeed employed at\nMicrosoft.  \n2. \"John Smith Lucy\nSmith\" - T o check if John\nSmith is married to Lucy\nSmith.  \n3. \"John Smith children\" -\nTo check if John Smith\nhas five children.\nIf you're not sure what syntax to use, consider using Markdown or XML. The models\nhave been trained on a lot of web content in XML and Markdown which may provide\nbetter results.\nLarge language models (LLMs) often perform better if the task is broken down into\nsmaller steps. For example, in the search query prompt referenced earlier, the prompt\ncan be restructured so that the model is first instructed to extract relevant facts, and\nthen instructed to generate search queries that can be used to verify those facts.\nSystem message User AssistantBreak the task down", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ee9628e0f5ba-0", "text": "System message User Assistant\nYou are an AI\nassistant that helps\npeople find\ninformation.You will read a paragraph, and then issue queries to a\nsearch engine in order to fact-check it.  \n--- \nPARAGRAPH  \nJohn Smith is married to Lucy Smith. They have five kids,\nand he works as a software engineer at Microsoft. What\nsearch queries should I do to fact-check this?  \n--- \nNow you will extract factual claims, and issue queries to\nfact-check them. When issuing a query, use the function\nSEAR CH(\"query\")  \nFACTUAL CL AIMS1. John Smith is\nmarried to Lucy\nSmith  \n2. They have five\nkids \n3. He works as a\nsoftware\nengineer at\nMicrosoft  \nQUERIES  \nSEAR CH(\"John\nSmith married\nLucy Smith\")  \nSEAR CH(\"John\nSmith family\nsize\")  \nSEAR CH(\"John\nSmith Microsoft\nsoftware\nengineer\")\nNotice the use of clear syntax to differentiate the sections and prime the output. In this\nsimple example, breaking the task down from one to two steps is not very dramatic, but\nwhen trying to do this for a larger piece of text with many factual claims, breaking the\ntask down can make a significant difference.\nSometimes we can get the model to use an affordance  instead of relying on its own\nparameters for information and answers. Search, for example, can be an affordance to\nhelp mitigate against fabricated answers, and to get up-to-date information.\nOne simple way to use an affordance is to stop generation once the affordance calls are\ngenerated by the model, then paste the outcomes back into the prompt. Below is an\nexample of what the follow-up call could be after doing the SEARCH calls above. Notice", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ee9628e0f5ba-1", "text": "example of what the follow-up call could be after doing the SEARCH calls above. Notice\nhow we pasted the search results into the prompt and replaced the previous SEARCH\ncalls.\nSystem\nmessageUser Assistant\nYou are an\nAI assistantYou will read a paragraph,\nextract factual claims, and- John Smith is married to Lucy Smith: Confirmed.\nSnippet [1] and [2] both mention John Smith's wifeUse of affordances", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "da1ae250afe6-0", "text": "System\nmessageUser Assistant\nthat helps\npeople find\ninformation.then use search engine\nresults to fact-check them  \n--- \nPARAGRAPH  \nJohn Smith is married to Lucy\nSmith. They have five kids,\nand he works as a software\nengineer at Microsoft. What\nsearch queries should I do to\nfact-check this?  \n--- \nFACTUAL CL AIMS  \n- John Smith is married to\nLucy Smith  \n- John and Lucy have five\nkids \n- John works as a software\nengineer at Microsoft  \n--- \nHere are various search\nqueries issued to research\nthe claims above:  \nSEAR CH QUERIES  \n- John Smith married to Lucy\nSmith  \n- John Smith number of\nchildren  \n- John Smith software\nengineer Microsoft  \n--- \nHere are snippets of the\nsearch results:  \nSNIPPET S: \n[1] \u2026 John Smith\u2019s wedding\nwas on September 25, 2012\n\u2026 \n[2] \u2026 John Smith was\naccompanied by his wife\nLucy to a party  \n[3]John was accompanied to\nthe soccer game by his two\ndaughters and three sons  \n[4] \u2026 After spending 10 years\nat Microsoft, Smith founded\nhis own startup, T ailspin T oys \n[5] John M is the town smith,\nand he married Fiona. Theyas Lucy.  \n- John and Lucy have five kids: Confirmed. Snippet\n[3] mentions that John was accompanied by his\ntwo daughters and three sons.  \n- John works as a software engineer at Microsoft:\nPartially confirmed. Snippet [4] mentions that John\nspent 10 years at Microsoft, but it does not\nexplicitly state that he currently works there.\nFurther research may be needed to confirm his", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "da1ae250afe6-1", "text": "explicitly state that he currently works there.\nFurther research may be needed to confirm his\ncurrent employment status.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "df34179104ec-0", "text": "System\nmessageUser Assistant\nhave a daughter named Lucy  \n--- \nGiven the snippets, fact check\neach of the factual claims\nabove:\nThis is a variation on the break the task down  technique. Instead of splitting a task into\nsmaller steps, in this approach the model response is instructed to proceed step-by-step\nand present all the steps involved. Doing so reduces the possibility of inaccuracy of\noutcomes and makes assessing the model response easier.\nSystem\nmessageUser AssistantChain of thought prompting", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "62f38235fe12-0", "text": "System\nmessageUser Assistant\nYou are an\nAI assistant\nthat helps\npeople find\ninformation.Who was the most decorated\n(maximum medals) individual athlete\nin the Olympic games that were held\nat Sydney? T ake a step-by-step\napproach in your response, cite\nsources and give reasoning before\nsharing final answer in the below\nformat: ANSWER is: <name>Step 1: Determine the year of the S ydney\nOlympic games. The S ydney Olympic\ngames were held in the year 2000.  \nStep 2: Find a list of individual athletes\nwho won multiple medals in the 2000\nSydney Olympics.  \nAccording to the official website of the\nOlympic Games, there were several\nindividual athletes who won multiple\nmedals in the 2000 S ydney Olympics.\nSome of the notable names include:  \n\u2022 Ian Thorpe (Australia) - 3 gold, 2 silver,\n0 bronze  \n\u2022 Pieter van den Hoogenband\n(Netherlands) - 2 gold, 0 silver, 2 bronze  \n\u2022 Lenny Krayzelburg (USA) - 3 gold, 0\nsilver, 0 bronze  \n\u2022 Inge de Bruijn (Netherlands) - 3 gold, 1\nsilver, 0 bronze  \nStep 3: Determine the athlete with the\nmost medals. After reviewing the list of\nindividual athletes who won multiple\nmedals in the 2000 S ydney Olympics, we\ncan see that the athlete with the most\nmedals is Ian Thorpe from Australia, who\nwon a total of 5 medals (3 gold and 2\nsilver).  \nTherefore, the ANSWER is: Ian Thorpe.\nUsing your prompt to specify the structure of the output can have a significant impact", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "62f38235fe12-1", "text": "Using your prompt to specify the structure of the output can have a significant impact\non the nature and quality of the results. Sometimes system message inputs as \u201conly\nwrite true facts\u201d or \u201cdo not fabricate information\u201d might not be adequate mitigations.\nInstead, asking for the model response to also include citations can help result in a\nreduction of the prevalence of incorrect responses.\nIf you instruct the model to cite the source material when it makes statements, those\nstatements are much more likely to be grounded. Asking for citations makes it so thatSpecifying the output structure", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e8bab61859dd-0", "text": "the model must make two errors every time it generates a response: the first error is the\nfabricated response, and the second is the bad citation. Note that the closer the citation\nis to the text it supports, the shorter the distance ahead the model needs to anticipate\nthe citation, which suggests that inline citations are better mitigations for false content\ngeneration than citations at the end of the content.\nSimilarly, if you ask the model to extract factual statements from a paragraph, it may\nextract compound statements such as \u2018X is doing Y AND Z\u2019 (which may be harder to\nverify). This can be avoided by specifying an output structure such as (entity1,\nrelationship, entity2).\nThe examples below show the use of citations and directing the model response to fit a\ndefined structure.\nSystem\nmessageUser Assistant\nYou are an\nAI assistant\nthat helps\npeople find\ninformationList down the different behaviors exhibited by the Customer\nSupport agent in the following conversation.  \n--- \nhi thank you for calling contoso pharmacy who am i speaking\nwith today i understand your frustration mary um can you tell\nme what exactly you're trying to accomplish OK uh certainly\nhappy to check that for you one moment please i see here that\nyou were on a generic form of lipitor is that right OK uh so i\nsee that your doctor stopped prescribing these drugs in\ntwenty twenty one actually OK uh yeah i'd be happy to check\nthat for you uh because sometimes there's a gap in the system\nand it just doesn't get reported so let me take a look here just\na moment so i'm seeing here that your doctor had these drugs\nprescribed to you from twenty twelve through twenty twenty\none OK um yeah according to the latest records uh provided\nby doctor's office you're now on a different drug metformin\nwould you like us to go ahead and fill that prescription for you", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e8bab61859dd-1", "text": "would you like us to go ahead and fill that prescription for you\nfor pick up a bit of form in five hundred milligrams uh you're\nvery welcome mary please let us know if there's anything else\nwe can do for you today.  \n--- \nHow was the response of the Customer Support agent?  \nClassify and list down their various behaviors and cite the\nreason behind the classification, using the format\nBEHA VIOR(\"reason\")1. Polite Greeting\nBEHA VIOR(\"Agent\nstarts the\nconversation with\na polite greeting\nand asks for the\ncaller's name\")  \n2. Empathetic\nBEHA VIOR(\"Agent\nacknowledges\nMary's frustration\nand tries to\nunderstand her\nconcern\")  \n3. Attentive\nBEHA VIOR(\"Agent\nlistens to the\ncustomer's\nproblem and asks\nfor more details\")  \n4. Proactive\nBEHA VIOR(\"Agent\noffers to check\nthe issue for the\ncustomer\")  \n5. Informative\nBEHA VIOR(\"Agent\nprovides the\nrelevant\ninformation\nabout the", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f6351a14cad8-0", "text": "System\nmessageUser Assistant\ncustomer's\nprescription\nhistory\")  \n6. Thorough\nBEHA VIOR(\"Agent\nchecks for any\npossible gaps in\nthe system and\nprovides a\ncomplete\nresponse\")  \n7. Confirmatory\nBEHA VIOR(\"Agent\nconfirms the\ncurrent\nmedication with\nthe customer\")  \n8. Solution-\noriented\nBEHA VIOR(\"Agent\noffers to fill the\nnew prescription\nfor the\ncustomer\")  \n9. Courteous\nBEHA VIOR(\"Agent\nthanks the\ncustomer and\noffers further\nassistance if\nneeded\")\nSystem\nmessageUser Assistant", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "329d9789dd44-0", "text": "System\nmessageUser Assistant\nYou are an AI\nassistant that\nhelps people\nfind\ninformation.Cluster the following news headlines into\ntopic categories based on patterns seen\nwithin the text. Also mention reasoning\nbehind how these categories were defined.  \nOutput format:  \n{ \n\"TOPIC_NAME\": \"\",  \n\"HEADLINES\": [],  \n\"REASONING\": \"\"  \n} \nInput news headlines:  \n1. \"From books to presentations in 10s with\nAR + ML\"  \n2. \"Demo from 1993 of 32-year-old Y ann\nLeCun showing off the W orld's first\nConvolutional Network for T ext R ecognition\"  \n3. \"First Order Motion Model applied to\nanimate paintings\"  \n4. \"Robinhood and other brokers literally\nblocking purchase of $GME, $NOK, $BB,\n$AMC; allow sells\"  \n5. \"United Airlines stock down over 5%\npremarket trading\"  \nOutput:  { \n\"ARTIFICIAL_INTELLIGENCE\": {  \n\"HEADLINES\": [  \n\"From books to presentations in\n10s with AR + ML\",  \n\"Demo from 1993 of 32-year-old\nYann LeCun showing off the\nWorld's first Convolutional\nNetwork for T ext R ecognition\",  \n\"First Order Motion Model\napplied to animate paintings\"  \n], \n\"REASONING\": \"These headlines\nare related to artificial\nintelligence, machine learning,\nand their applications in various\nfields.\"  \n}, \n\"FINANCIAL_MARKET S\": { \n\"HEADLINES\": [  \n\"Robinhood and other brokers\nliterally blocking purchase of\n$GME, $NOK, $BB, $AMC; allow", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "329d9789dd44-1", "text": "$GME, $NOK, $BB, $AMC; allow\nsells\",  \n\"United Airlines stock down over\n5% premarket trading\",  \n\"Bitcoin was nearly $20,000 a\nyear ago today\"  \n], \n\"REASONING\": \"These headlines\nare related to financial markets,\nstocks, cryptocurrencies, and\ntrading platforms.\"  \n} \n} \nChanging the temperature parameter changes the output of the model. The\ntemperature parameter can be set between 0 and 2. A higher value, for example 0.7 will\nmake the output more random and produce more divergent responses, while a lower\nvalue, like. 0.2, will make the output more focused and concrete. A fictional story could\nbe generated using a higher temperature. Whereas to generate a legal document it isTemperature and Top_p parameters", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8fe8851c4fa9-0", "text": "recommended to use a much lower temperature. T op_probability is another parameter\nand is similar to T emperature in that it also controls the randomness of the model\nresponse, but it does so in a different way. The general recommendation is to alter one\nof these two parameters at a time, not both.\nOne of the most effective ways to provide reliable answers is to give the model data to\ndraw its responses from (grounding data). If your use case relies on up-to-date, reliable\ninformation and is not purely a creative scenario, we strongly recommend providing\ngrounding data. In general, the closer you can get your source material to the final form\nof the answer you want, the less work the model needs to do, which means there is less\nopportunity for error. Here is an example where the system is provided a recent blog\ndescribing the launch of GPT-4 in Azure OpenAI Service , and asked to name some of\nits early customers.\nSystem message User Assistant\nYou are an AI assistant that helps\npeople find information. Y ou will\nprovide answers exclusively from\nbelow text and respond in 100 words\nor less:  \n\"At Microsoft, we are constantly\ndiscovering new ways to unleash\ncreativity, unlock productivity, and\nuplevel skills so that more people can\nbenefit from using AI. This is allowing\nour customers to \u2026  \n\u2026 thereby reducing the likelihood that\nthe model will generate a harmful\nresponse.\u201d  \nAnswer in 100 words or less. Use bullet\nlists wherever possible.Who are some\nof the early\ncustomers of\nthe Azure\nOpenAI service\nGPT-4 models?Here are some of the early customers\nof the Azure OpenAI service GPT-4\nmodels:  \n- Epic Healthcare: They plan to use\nGPT-4 to help physicians and nurses\nspend less time on data entry and", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8fe8851c4fa9-1", "text": "GPT-4 to help physicians and nurses\nspend less time on data entry and\ninvestigation.  \n- Coursera: They are using Azure\nOpenAI Service with GPT-4 to create a\nnew AI-powered learning experience\non their platform.  \n- Coca-Cola: They are using Azure\nOpenAI to build a knowledge hub and\nplan to leverage GPT-4's multimodal\ncapabilities for marketing, advertising,\npublic relations, and customer\nrelations.  \n- Other companies of all sizes are\nusing Azure AI to improve customer\nexperiences, summarize long-form\ncontent, help write software, and\nreduce risk by predicting the right tax\ndata.\n Text from Azure OpenAI GPT-4 launch blog .Provide grounding context\n1\n1", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8f580434f4c0-0", "text": "Learn more about Azure OpenAI .\nGet started with the ChatGPT model with the ChatGPT quickstart .\nFor more examples, check out the Azure OpenAI Samples GitHub repositoryNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b7a13b82f05f-0", "text": "System mes sage framework and\ntemplate recommendations for Large\nLanguage Models (LLMs)\nArticle \u202206/01/2023\nThis article provides a recommended framework and example templates to help write an\neffective system message, sometimes referred to as a metaprompt or system prompt\nthat can be used to guide an AI system\u2019s behavior and improve system performance. If\nyou're new to prompt engineering, we recommend starting with our introduction to\nprompt engineering  and prompt engineering techniques guidance .\nThis guide provides system message recommendations and resources that, along with\nother prompt engineering techniques, can help increase the accuracy and grounding of\nresponses you generate with a Large Language Model (LLM). However, it is important to\nremember that even when using these templates and guidance, you still need to\nvalidate the responses the models generate. Just because a carefully crafted system\nmessage worked well for a particular scenario doesn't necessarily mean it will work more\nbroadly across other scenarios. Understanding the limitations of LLMs  and the\nmechanisms for evaluating and mitigating those limitations  is just as important as\nunderstanding how to leverage their strengths.\nThe LLM system message framework described here covers four concepts:\nDefine the model\u2019s profile, capabilities, and limitations for your scenario\nDefine the model\u2019s output format\nProvide example(s) to demonstrate the intended behavior of the model\nProvide additional behavioral guardrails\nDefine the specific task(s)  you would like the model to complete. Describe who\nthe users of the model will be, what inputs they will provide to the model, and\nwhat you expect the model to do with the inputs.\nDefine how the model should complet e the tasks , including any additional tools\n(like APIs, code, plug-ins) the model can use. If it doesn\u2019t use additional tools, it\ncan rely on its own parametric knowledge.Define the model\u2019s profile, capabilities, and\nlimitations for your scenario", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f4013fbe79c8-0", "text": "Define the scope and limitations  of the model\u2019s performance. Provide clear\ninstructions on how the model should respond when faced with any limitations.\nFor example, define how the model should respond if prompted on subjects or for\nuses that are off topic or otherwise outside of what you want the system to do.\nDefine the postur e and t one the model should exhibit in its responses.\nWhen using the system message to define the model\u2019s desired output format in your\nscenario, consider and include the following types of information:\nDefine the language and syntax  of the output format. If you want the output to\nbe machine parse-able, you may want the output to be in formats like JSON, XSON\nor XML.\nDefine any styling or formatting  preferences for better user or machine\nreadability. For example, you may want relevant parts of the response to be bolded\nor citations to be in a specific format.\nWhen using the system message to demonstrate the intended behavior of the model in\nyour scenario, it is helpful to provide specific examples. When providing examples,\nconsider the following:\nDescribe difficult use cases where the prompt is ambiguous or complicated, to give\nthe model additional visibility into how to approach such cases.\nShow the potential \u201cinner monologue\u201d and chain-of-thought reasoning to better\ninform the model on the steps it should take to achieve the desired outcomes.\nWhen defining additional safety and behavioral guardrails, it\u2019s helpful to first identify\nand prioritize the harms  you\u2019d like to address. Depending on the application, the\nsensitivity and severity of certain harms could be more important than others.Define the model's output format\nProvide example(s) to demonstrate the\nintended behavior of the model\nDefine additional behavioral guardrails\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9fff5fadfe6b-0", "text": "Learn more about Azure OpenAI\nLearn more about deploying Azure OpenAI responsibly\nFor more examples, check out the Azure OpenAI Samples GitHub repository", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "792d6fc5d4a6-0", "text": "Azure OpenAI on your data (preview)\nArticle \u202207/07/2023\nAzure OpenAI on your data enables you to run supported chat models such as GPT-35-\nTurbo and GPT-4 on your data without needing to train or fine-tune models. Running\nmodels on your data enables you to chat on top of, and analyze your data with greater\naccuracy and speed. By doing so, you can unlock valuable insights that can help you\nmake better business decisions, identify trends and patterns, and optimize your\noperations. One of the key benefits of Azure OpenAI on your data is its ability to tailor\nthe content of conversational AI.\nTo get started, connect your data source  using Azure OpenAI S tudio  and start asking\nquestions and chatting on your data.\nBecause the model has access to, and can reference specific sources to support its\nresponses, answers are not only based on its pretrained knowledge but also on the\nlatest information available in the designated data source. This grounding data also\nhelps the model avoid generating responses based on outdated or incorrect\ninformation.\nAzure OpenAI on your data works with OpenAI's powerful GPT-35-Turbo and GPT-4\nlanguage models, enabling them to provide responses based on your data. Y ou can\naccess Azure OpenAI on your data using a REST API or the web-based interface in the\nAzure OpenAI S tudio  to create a solution that connects to your data to enable an\nenhanced chat experience.\nOne of the key features of Azure OpenAI on your data is its ability to retrieve and utilize\ndata in a way that enhances the model's output. Azure OpenAI on your data, together\nwith Azure Cognitive Search, determines what data to retrieve from the designated data\nsource based on the user input and provided conversation history. This data is then", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "792d6fc5d4a6-1", "text": "source based on the user input and provided conversation history. This data is then\naugmented and resubmitted as a prompt to the OpenAI model, with retrieved\ninformation being appended to the original prompt. Although retrieved data is being\n\uff17 Note\nTo get started, you need to already have been approved for Azur e OpenAI access\nand have an Azur e OpenAI Ser vice r esour ce with either the gpt-35-turbo or the\ngpt-4 models deployed.\nWhat is Azure OpenAI on your data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "44f7137b456d-0", "text": "appended to the prompt, the resulting input is still processed by the model like any\nother prompt. Once the data has been retrieved and the prompt has been submitted to\nthe model, the model uses this information to provide a completion. See the Data,\nprivacy, and security for Azure OpenAI Service  article for more information.\nAzure OpenAI on your data uses an Azure Cognitive Search  index to determine what\ndata to retrieve based on user inputs and provided conversation history. W e\nrecommend using Azure OpenAI S tudio to create your index from a blob storage or\nlocal files. See the quickstart article  for more information.\nFor documents and datasets with long text, you should use the available data\npreparation script  to ingest the data into cognitive search. The script chunks the data\nso that your response with the service will be more accurate. This script also supports\nscanned PDF file and images and ingests the data using Form R ecognizer .\nAzure OpenAI on your data supports the following filetypes:\n.txt\n.md\n.html\nMicrosoft W ord files\nMicrosoft P owerP oint files\nPDF\nThere are some caveats about document structure and how it might affect the quality of\nresponses from the model:\nThe model provides the best citation titles from markdown ( .md) files.\nIf a document is a PDF file, the text contents are extracted as a preprocessing step\n(unless you're connecting your own Azure Cognitive Search index). If your\ndocument contains images, graphs, or other visual content, the model's response\nquality depends on the quality of the text that can be extracted from them.Data source options\nIngesting your data into Azure cognitive search\nData formats and file types", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "37324f1587cf-0", "text": "If you're converting data from an unsupported format into a supported format,\nmake sure the conversion:\nDoesn't lead to significant data loss.\nDoesn't add unexpected noise to your data.\nThis will impact the quality of Azure Cognitive Search and the model response.\nAzure OpenAI on your data does not currently support private endpoints.\nUse the following sections to help you configure Azure OpenAI on your data for optimal\nresults.\nGive the model instructions about how it should behave and any context it should\nreference when generating a response. Y ou can describe the assistant\u2019s personality, what\nit should and shouldn\u2019t answer, and how to format responses. There\u2019s no token limit for\nthe system message, but will be included with every API call and counted against the\noverall token limit. The system message will be truncated if it's greater than 200\u202ftokens.\nFor example, if you're creating a chatbot where the data consists of transcriptions of\nquarterly financial earnings calls, you might use the following system message:\n\"You ar e a financial chatbot us eful for ans wering questions fr om financial r eports. You ar e\ngiven ex cerpts fr om the ear nings call. Pleas e ans wer the questions b y parsing thr ough all\ndialogue.\"\nThis system message can help improve the quality of the response by specifying the\ndomain (in this case finance) and mentioning that the data consists of call transcriptions.\nIt helps set the necessary context for the model to respond appropriately.Virtual network support & private link support\nRecommended settings\nSystem message\n\uff17 Note\nThe system message is only guidance. The model might not adhere to every\ninstruction specified because it has been primed with certain behaviors such as\nobjectivity, and avoiding controversial statements. Unexpected behavior may occur\nif the system message contradicts with these behaviors.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cabdb07c3e0a-0", "text": "Set a limit on the number of tokens per model response. The upper limit for Azure\nOpenAI on Y our Data is 1500. This is equivalent to setting the max_tokens parameter in\nthe API.\nThis option encourages the model to respond using your data only, and is selected by\ndefault. If you unselect this option, the model may more readily apply its internal\nknowledge to respond. Determine the correct selection based on your use case and\nscenario.\nIf semantic search  is enabled for your Azure Cognitive Search service, you are more\nlikely to produce better retrieval of your data, which can improve response and citation\nquality.\nIf you're using your own index, you will be prompted in the Azure OpenAI S tudio to\ndefine which fields you want to map for answering questions when you add your data\nsource. Y ou can provide multiple fields for Content dat a, and should include all fields\nthat have text pertaining to your use case.Maximum response\nLimit responses to your data\nSemantic search\n\uff09 Impor tant\nSemantic search is subject to additional pricing\nCurrently Azure OpenAI on your data supports semantic search for English\ndata only. Only enable semantic search if both your documents and use case\nare in English.\nIndex field mapping", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f4c98f57a9a9-0", "text": "In this example, the fields mapped to Cont ent data  and Title provide information to the\nmodel to answer questions. Title is also used to title citation text. The field mapped to\nFile name  generates the citation names in the response.\nMapping these fields correctly helps ensure the model has better response and citation\nquality.\nUse the following practices for best results when chatting with the model.\nConv ersation hist ory\nBefore starting a new conversation (or asking a question that is not related to the\nprevious ones), clear the chat history.\nGetting different responses for the same question between the first conversational\nturn and subsequent turns can be expected because the conversation history\nchanges the current state of the model. If you receive incorrect answers, report it\nas a quality bug.\nModel r esponse\nIf you are not satisfied with the model response for a specific question, try either\nmaking the question more specific or more generic to see how the model\nresponds, and reframe your question accordingly.\nChain-of-thought prompting  has been shown to be effective in getting the model\nto produce desired outputs for complex questions/tasks.\nQuestion length\n\uf80a\nInteracting with the model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3205dfaadbf5-0", "text": "Avoid asking long questions and break them down into multiple questions if possible.\nThe GPT models have limits on the number of tokens they can accept. T oken limits are\ncounted toward: the user question, the system message, the retrieved search documents\n(chunks), internal prompts, the conversation history (if any), and the response. If the\nquestion exceeds the token limit, it will be truncated.\nMulti-lingual suppor t\nAzure OpenAI on your data supports queries that are in the same language as the\ndocuments. For example, if your data is in Japanese, then queries need to be in\nJapanese too.\nCurrently Azure OpenAI on your data supports semantic search  for English data\nonly. Don't enable semantic search if your data is in other languages.\nWe recommend using a system message to inform the model that your data is in\nanother language. For example:\n\u201cYou ar e an AI assist ant that helps people f ind infor mation. Y ou retrieve Japanese\ndocuments, and y ou should r ead them car efully in J apanese and ans wer in\nJapanese.\u201d\nIf you have documents in multiple languages, we recommend building a new index\nfor each language and connecting them separately to Azure OpenAI.\nConsider setting the following parameters even if they are optional for using the API.\nParamet er Recommendation\nfieldsMapping Explicitly set the title and content fields of your index. This impacts the search\nretrieval quality of Azure Cognitive Search, which impacts the overall response\nand citation quality.\nroleInformationCorresponds to the \u201cS ystem Message\u201d in the Azure OpenAI S tudio. See the\nSystem message  section above for recommendations.\nYou can send a streaming request using the stream parameter, allowing data to be sent\nand received incrementally, without waiting for the entire API response. This can", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3205dfaadbf5-1", "text": "and received incrementally, without waiting for the entire API response. This can\nimprove performance and user experience, especially for large or dynamic data.Using the API\nStreaming data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4bcf1e79de12-0", "text": "JSON\nWhen chatting with a model, providing a history of the chat will help the model return\nhigher quality results.\nJSON{ \n    \"stream\" : true, \n    \"dataSources\" : [ \n        {  \n            \"type\": \"AzureCognitiveSearch\" , \n            \"parameters\" : { \n                \"endpoint\" : \"'$SearchEndpoint'\" , \n                \"key\": \"'$SearchKey'\" , \n                \"indexName\" : \"'$SearchIndex'\"  \n            }  \n        }  \n    ], \n    \"messages\" : [ \n        {  \n            \"role\": \"user\", \n            \"content\" : \"What are the differences between Azure Machine  \nLearning and Azure Cognitive Services?\"  \n        }  \n    ] \n} \nConversation history for better results\n{ \n    \"dataSources\" : [ \n        {  \n            \"type\": \"AzureCognitiveSearch\" , \n            \"parameters\" : { \n                \"endpoint\" : \"'$SearchEndpoint'\" , \n                \"key\": \"'$SearchKey'\" , \n                \"indexName\" : \"'$SearchIndex'\"  \n            }  \n        }  \n    ], \n    \"messages\" : [ \n        {  \n            \"role\": \"user\", \n            \"content\" : \"What are the differences between Azure Machine  \nLearning and Azure Cognitive Services?\"  \n        },  \n        {  \n            \"role\": \"tool\", \n            \"content\" : \"{\\\"citations\\\": [{\\\"content\\\": \\\"title: Cognitive  \nServices and Machine Learning\\\\ntitleSuffix: Azure Cognitive", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a1150019abcb-0", "text": "Get started using your data with Azure OpenAI\nIntroduction to prompt engineeringServices\\\\ndescription: Learn where Azure Cognitive Services fits in with  \nother Azure offerings for machine learning.\\\\nCognitive Services and machine  \nlearning\\\\nCognitive Services provides machine learning capabilities to  \nsolve general problems such as...\\\\n \\\"articles\\\\\\\\cognitive-\nservices\\\\\\\\cognitive-services-and-machine-learning.md\\\", \\\"url\\\": null,  \n\\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1018.  \nScores=0.32200050354003906 and 1.2880020141601562.Org Highlight  \ncount=115.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"What are the  \ndifferences between Azure Machine Learning and Azure Cognitive Services?\n\\\\\\\"]\\\"}\"  \n        },  \n        {  \n            \"role\": \"assistant\" , \n            \"content\" : \" \\nAzure Machine Learning is a product and service  \ntailored for data scientists to build, train, and deploy machine learning  \nmodels [doc1]...\"  \n        },  \n        {  \n            \"role\": \"user\", \n            \"content\" : \"How do I use Azure machine learning?\"  \n        }  \n    ] \n} \nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b8090805f2e5-0", "text": "Create a resource and deploy a model\nusing Azure OpenAI\nArticle \u202205/23/2023\nUse this article to get started with Azure OpenAI with step-by-step instructions to create\na resource and deploy a model. While the steps for resource creation and model\ndeployment can be completed in a few minutes, the actual deployment process itself\ncan take more than hour. Y ou can create your resource, start your deployment, and then\ncheck back in on your deployment later rather than actively waiting for the deployment\nto complete.\nAn Azure subscription - Create one for free\nAccess granted to Azure OpenAI in the desired Azure subscription\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nResources in Azure can be created several different ways:\nWithin the Azure portal\nUsing the REST APIs, Azure CLI, P owerShell or client libraries\nVia ARM templates\nThis guide walks you through the Azure portal creation experience.\n1. Navigate to the create page: Azure OpenAI Service Create P age\n2. On the Create page provide the following information:\nField Descr iption\nSubscription Select the Azure subscription used in your OpenAI onboarding application\nResour ce\ngroupThe Azure resource group that will contain your OpenAI resource. Y ou can\ncreate a new group or add it to a pre-existing group.Prerequisites\nCreate a resource", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "593d13884964-0", "text": "Field Descr iption\nRegion The location of your instance. Different locations may introduce latency, but\nhave no impact on the runtime availability of your resource.\nName A descriptive name for your cognitive services resource. For example,\nMyOpenAIR esource.\nPricing Tier Only 1 pricing tier is available for the service currently\nBefore you can generate text or inference, you need to deploy a model. Y ou can select\nfrom one of several available models in Azure OpenAI S tudio.\nTo deploy a model, follow these steps:\n1. Sign in to Azure OpenAI S tudio .\n2. Select the subscription and Azure OpenAI resource to work with.\n\uf80a\nDeploy a model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "db72458365a2-0", "text": "3. Under Management  select Deployments .\n4. Select Create new deployment .\nField Descr iption\nSelect a\nmodelModel availability varies by region.For a list of available models per region,\nsee Model Summary table and region availability .\nDeployment\nnameChoose a name carefully. The deployment name will be used in your code to\ncall the model via the client libraries and REST API\nAdvanced\nOptionsContent Filter - Assign a content filter to your deployment.  \nTokens per Minute Rate Limit - Adjust the T okens per Minute (TPM) to set\nthe effective rate limit for your deployment. Y ou can modify this value at any\ntime via the Quotas  menu\n5. Select a model from the drop-down.\n6. Enter a deployment name to help you identify the model.\n7. For your first deployment leave the Advanced Options set to the defaults.\nThe deployments table displays a new entry that corresponds to this newly created\nmodel. Y our deployment status will move to succeeded when the deployment is\ncomplete and ready for use.\nNow that you have a resource and your first model deployed get started making\nAPI calls and generating text with our quickstarts .\nLearn more about the underlying models that power Azure OpenAI .Next steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b504efb28d50-0", "text": "Learn how to work with the ChatGPT\nand GPT-4 models\nArticle \u202205/15/2023\nThe ChatGPT and GPT-4 models are language models that are optimized for\nconversational interfaces. The models behave differently than the older GPT-3 models.\nPrevious models were text-in and text-out, meaning they accepted a prompt string and\nreturned a completion to append to the prompt. However, the ChatGPT and GPT-4\nmodels are conversation-in and message-out. The models expect input formatted in a\nspecific chat-like transcript format, and return a completion that represents a model-\nwritten message in the chat. While this format was designed specifically for multi-turn\nconversations, you'll find it can also work well for non-chat scenarios too.\nIn Azure OpenAI there are two different options for interacting with these type of\nmodels:\nChat Completion API.\nCompletion API with Chat Markup Language (ChatML).\nThe Chat Completion API is a new dedicated API for interacting with the ChatGPT and\nGPT-4 models. This API is the preferred method for accessing these models. It is also\nthe only way t o access the new GPT-4 models .\nChatML uses the same completion API  that you use for other models like text-davinci-\n002, it requires a unique token based prompt format known as Chat Markup Language\n(ChatML). This provides lower level access than the dedicated Chat Completion API, but\nalso requires additional input validation, only supports ChatGPT (gpt-35-turbo) models,\nand the underlying format is mor e likely to change ov er time .\nThis article walks you through getting started with the new ChatGPT and GPT-4 models.\nIt's important to use the techniques described here to get the best results. If you try to", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b504efb28d50-1", "text": "It's important to use the techniques described here to get the best results. If you try to\ninteract with the models the same way you did with the older model series, the models\nwill often be verbose and provide less useful responses.\nThe following code snippet shows the most basic way to use the ChatGPT and GPT-4\nmodels with the Chat Completion API. If this is your first time using these models\nprogrammatically, we recommend starting with our ChatGPT & GPT-4 Quickstart .Working with the ChatGPT and GPT-4 models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e33fb7286003-0", "text": "GPT-4 models ar e curr ently only av ailable by r equest.  Existing Azure OpenAI\ncustomers can apply for access by filling out this form .\nPython\nimport os \nimport openai \nopenai.api_type = \"azure\" \nopenai.api_version = \"2023-05-15\"   \nopenai.api_base = os.getenv( \"OPENAI_API_BASE\" )  # Your Azure OpenAI  \nresource's endpoint value.  \nopenai.api_key = os.getenv( \"OPENAI_API_KEY\" ) \nresponse = openai.ChatCompletion.create(  \n    engine= \"gpt-35-turbo\" , # The deployment name you chose when you deployed  \nthe ChatGPT or GPT-4 model.  \n    messages=[  \n        { \"role\": \"system\" , \"content\" : \"Assistant is a large language model  \ntrained by OpenAI.\" }, \n        { \"role\": \"user\", \"content\" : \"Who were the founders of Microsoft?\" } \n    ] \n) \nprint(response)  \nprint(response[ 'choices' ][0]['message' ]['content' ]) \nOutput\n{ \n  \"choices\": [  \n    { \n      \"finish_reason\": \"stop\",  \n      \"index\": 0,  \n      \"message\": {  \n        \"content\": \"The founders of Microsoft are Bill Gates and Paul Allen.  \nThey co-founded the company in 1975.\",  \n        \"role\": \"assistant\"  \n      } \n    } \n  ], \n  \"created\": 1679014551,  \n  \"id\": \"chatcmpl-6usfn2yyjkbmESe3G4jaQR6bsScO1\",", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e33fb7286003-1", "text": "\"model\": \"gpt-3.5-turbo-0301\",  \n  \"object\": \"chat.completion\",  \n  \"usage\": {  \n    \"completion_tokens\": 86,  \n    \"prompt_tokens\": 37,  \n    \"total_tokens\": 123", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6e430b5caed5-0", "text": "Every response includes a finish_reason. The possible values for finish_reason are:\nstop: API returned complete model output.\nlength : Incomplete model output due to max_tokens parameter or token limit.\ncontent_filt er: Omitted content due to a flag from our content filters.\nnull:API response still in progress or incomplete.\nConsider setting max_tokens to a slightly higher value than normal such as 300 or 500.\nThis ensures that the model doesn't stop generating text before it reaches the end of\nthe message.\nUnlike previous GPT-3 and GPT-3.5 models, the gpt-35-turbo model as well as the gpt-\n4 and gpt-4-32k models will continue to be updated. When creating a deployment  of\nthese models, you'll also need to specify a model version.\nCurrently, only version 0301 is available for ChatGPT and 0314 for GPT-4 models. W e'll\ncontinue to make updated versions available in the future. Y ou can find the model\ndeprecation times for these models on our models  page.\nOpenAI trained the ChatGPT and GPT-4 models to accept input formatted as a\nconversation. The messages parameter takes an array of dictionaries with a conversation\norganized by role.  } \n} \n\uff17 Note\nThe following parameters aren't available with the new ChatGPT and GPT-4 models:\nlogprobs, best_of, and echo. If you set any of these parameters, you'll get an error.\nModel versioning\n\uff17 Note\ngpt-35-turbo is equivalent to the gpt-3.5-turbo model from OpenAI.\nWorking with the Chat Completion API", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "efd78be640a1-0", "text": "The format of a basic Chat Completion is as follows:\nA conversation with one example answer followed by a question would look like:\nThe system role also known as the system message is included at the beginning of the\narray. This message provides the initial instructions to the model. Y ou can provide\nvarious information in the system role including:\nA brief description of the assistant\nPersonality traits of the assistant\nInstructions or rules you would like the assistant to follow\nData or information needed for the model, such as relevant questions from an F AQ\nYou can customize the system role for your use case or just include basic instructions.\nThe system role/message is optional, but it's recommended to at least include a basic\none to get the best results.\nAfter the system role, you can include a series of messages between the user and the\nassistant .{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to  \nthe model\"},  \n{\"role\": \"user\", \"content\": \"The users messages goes here\"}  \n{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to  \nthe model.\"},  \n{\"role\": \"user\", \"content\": \"Example question goes here.\"},  \n{\"role\": \"assistant\", \"content\": \"Example answer goes here.\"},  \n{\"role\": \"user\", \"content\": \"First question/message for the model to  \nactually respond to.\"}  \nSystem role\nMessages\n {\"role\": \"user\", \"content\": \"What is thermodynamics?\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f9ec3136711a-0", "text": "To trigger a response from the model, you should end with a user message indicating\nthat it's the assistant's turn to respond. Y ou can also include a series of example\nmessages between the user and the assistant as a way to do few shot learning.\nThe following section shows examples of different styles of prompts that you could use\nwith the ChatGPT and GPT-4 models. These examples are just a starting point, and you\ncan experiment with different prompts to customize the behavior for your own use\ncases.\nIf you want the ChatGPT model to behave similarly to chat.openai.com , you can use a\nbasic system message like \"Assistant is a large language model trained by OpenAI.\"\nFor some scenarios, you may want to give additional instructions to the model to define\nguardrails for what the model is able to do.\nYou can also include relevant data or information in the system message to give the\nmodel extra context for the conversation. If you only need to include a small amount of\ninformation, you can hard code it in the system message. If you have a large amount ofMessage prompt examples\nBasic example\n{\"role\": \"system\", \"content\": \"Assistant is a large language model trained  \nby OpenAI.\"},  \n{\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}  \nExample with instructions\n{\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed  \nto help users answer their tax related questions.  \nInstructions:  \n- Only answer questions related to taxes.  \n- If you're unsure of an answer, you can say \"I don't know\" or \"I'm not  \nsure\" and recommend users go to the IRS website for more information. \"},  \n{\"role\": \"user\", \"content\": \"When are my taxes due?\"}  \nUsing data for grounding", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "38dc52df7d4d-0", "text": "data that the model should be aware of, you can use embeddings  or a product like\nAzure Cognitive Search  to retrieve the most relevant information at query time.\nYou can also give few shot examples to the model. The approach for few shot learning\nhas changed slightly because of the new prompt format. Y ou can now include a series of\nmessages between the user and the assistant in the prompt as few shot examples. These\nexamples can be used to seed answers to common questions to prime the model or\nteach particular behaviors to the model.\nThis is only one example of how you can use few shot learning with ChatGPT and GPT-4.\nYou can experiment with different approaches to see what works best for your use case.\n{\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed  \nto help users answer technical questions about Azure OpenAI Serivce. Only  \nanswer questions using the context below and if you're not sure of an  \nanswer, you can say 'I don't know'.  \nContext:  \n- Azure OpenAI Service provides REST API access to OpenAI's powerful  \nlanguage models including the GPT-3, Codex and Embeddings model series.  \n- Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-\n3, Codex, and DALL-E models with the security and enterprise promise of  \nAzure. Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility  \nand a smooth transition from one to the other.  \n- At Microsoft, we're committed to the advancement of AI driven by  \nprinciples that put people first. Microsoft has made significant investments  \nto help guard against abuse and unintended harm, which includes requiring  \napplicants to show well-defined use cases, incorporating Microsoft\u2019s  \nprinciples for responsible AI use.\"  \n},", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "38dc52df7d4d-1", "text": "principles for responsible AI use.\"  \n}, \n{\"role\": \"user\", \"content\": \"What is Azure OpenAI Service?\"}  \nFew shot learning with Chat Completion\n{\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed  \nto help users answer their tax related questions. \"},  \n{\"role\": \"user\", \"content\": \"When do I need to file my taxes by?\"},  \n{\"role\": \"assistant\", \"content\": \"In 2023, you will need to file your taxes  \nby April 18th. The date falls after the usual April 15th deadline because  \nApril 15th falls on a Saturday in 2023. For more details, see  \nhttps://www.irs.gov/filing/individuals/when-to-file.\"},  \n{\"role\": \"user\", \"content\": \"How can I check the status of my tax refund?\"},  \n{\"role\": \"assistant\", \"content\": \"You can check the status of your tax  \nrefund by visiting https://www.irs.gov/refunds\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "50b5a3bd9d11-0", "text": "The Chat Completion API is designed to work with multi-turn conversations, but it also\nworks well for non-chat scenarios.\nFor example, for an entity extraction scenario, you might use the following prompt:\nThe examples so far have shown you the basic mechanics of interacting with the Chat\nCompletion API. This example shows you how to create a conversation loop that\nperforms the following actions:\nContinuously takes console input, and properly formats it as part of the messages\narray as user role content.\nOutputs responses that are printed to the console and formatted and added to the\nmessages array as assistant role content.\nThis means that every time a new question is asked, a running transcript of the\nconversation so far is sent along with the latest question. Since the model has no\nmemory, you need to send an updated transcript with each new question or the model\nwill lose context of the previous questions and answers.\nPythonUsing Chat Completion for non-chat scenarios\n{\"role\": \"system\", \"content\": \"You are an assistant designed to extract  \nentities from text. Users will paste in a string of text and you will  \nrespond with entities you've extracted from the text as a JSON object.  \nHere's an example of your output format:  \n{ \n   \"name\": \"\",  \n   \"company\": \"\",  \n   \"phone_number\": \"\"  \n}\"}, \n{\"role\": \"user\", \"content\": \"Hello. My name is Robert Smith. I'm calling  \nfrom Contoso Insurance, Delaware. My colleague mentioned that you are  \ninterested in learning about our comprehensive benefits policy. Could you  \ngive me a call back at (555) 346-9322 when you get a chance so we can go  \nover the benefits?\"}  \nCreating a basic conversation loop\nimport os \nimport openai \nopenai.api_type = \"azure\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "50b5a3bd9d11-1", "text": "import os \nimport openai \nopenai.api_type = \"azure\" \nopenai.api_version = \"2023-05-15\"   \nopenai.api_base = os.getenv( \"OPENAI_API_BASE\" )  # Your Azure OpenAI", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2efa8b057be0-0", "text": "When you run the code above you will get a blank console window. Enter your first\nquestion in the window and then hit enter. Once the response is returned, you can\nrepeat the process and keep asking questions.\nThe previous example will run until you hit the model's token limit. With each question\nasked, and answer received, the messages array grows in size. The token limit for gpt-\n35-turbo is 4096 tokens, whereas the token limits for gpt-4 and gpt-4-32k are 8192 and\n32768 respectively. These limits include the token count from both the message array\nsent and the model response. The number of tokens in the messages array combined\nwith the value of the max_tokens parameter must stay under these limits or you'll receive\nan error.\nIt's your responsibility to ensure the prompt and completion falls within the token limit.\nThis means that for longer conversations, you need to keep track of the token count and\nonly send the model a prompt that falls within the limit.\nThe following code sample shows a simple chat loop example with a technique for\nhandling a 4096 token count using OpenAI's tiktoken library.resource's endpoint value .  \nopenai.api_key = os.getenv( \"OPENAI_API_KEY\" ) \nconversation=[{ \"role\": \"system\" , \"content\" : \"You are a helpful assistant.\" }] \nwhile(True): \n    user_input = input()       \n    conversation.append({ \"role\": \"user\", \"content\" : user_input})  \n    response = openai.ChatCompletion.create(  \n        engine= \"gpt-3.5-turbo\" , # The deployment name you chose when you  \ndeployed the ChatGPT or GPT-4 model.  \n        messages = conversation  \n    )", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2efa8b057be0-1", "text": "messages = conversation  \n    ) \n    conversation.append({ \"role\": \"assistant\" , \"content\" : response[ 'choices' ]\n[0]['message' ]['content' ]}) \n    print( \"\\n\" + response[ 'choices' ][0]['message' ]['content' ] + \"\\n\") \nManaging conversations\n\uff17 Note\nWe strongly recommend staying within the document ed input t oken limit  for all\nmodels even if you find you are able to exceed that limit.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a6d45f6f0bf2-0", "text": "The code requires tiktoken 0.3.0. If you have an older version run pip install tiktoken\n--upgrade.\nPython\nimport tiktoken  \nimport openai \nimport os \nopenai.api_type = \"azure\" \nopenai.api_version = \"2023-05-15\"   \nopenai.api_base = os.getenv( \"OPENAI_API_BASE\" )  # Your Azure OpenAI  \nresource's endpoint value .  \nopenai.api_key = os.getenv( \"OPENAI_API_KEY\" ) \nsystem_message = { \"role\": \"system\" , \"content\" : \"You are a helpful  \nassistant.\" } \nmax_response_tokens = 250 \ntoken_limit= 4096 \nconversation=[]  \nconversation.append(system_message)  \ndef num_tokens_from_messages (messages, model= \"gpt-3.5-turbo-0301\" ): \n    encoding = tiktoken.encoding_for_model(model)  \n    num_tokens = 0 \n    for message in messages:  \n        num_tokens += 4  # every message follows <im_start>\n{role/name}\\n{content}<im_end>\\n  \n        for key, value in message.items():  \n            num_tokens += len(encoding.encode(value))  \n            if key == \"name\":  # if there's a name, the role is omitted  \n                num_tokens += -1  # role is always required and always 1  \ntoken \n    num_tokens += 2  # every reply is primed with <im_start>assistant  \n    return num_tokens  \nwhile(True): \n    user_input = input( \"\")      \n    conversation.append({ \"role\": \"user\", \"content\" : user_input})  \n    conv_history_tokens = num_tokens_from_messages(conversation)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a6d45f6f0bf2-1", "text": "conv_history_tokens = num_tokens_from_messages(conversation)  \n    while (conv_history_tokens+max_response_tokens >= token_limit):  \n        del conversation[ 1]  \n        conv_history_tokens = num_tokens_from_messages(conversation)  \n         \n    response = openai.ChatCompletion.create(  \n        engine= \"gpt-35-turbo\" , # The deployment name you chose when you  \ndeployed the ChatGPT or GPT-4 model.  \n        messages = conversation,  \n        temperature= .7, \n        max_tokens=max_response_tokens,  \n    ) \n    conversation.append({ \"role\": \"assistant\" , \"content\" : response[ 'choices' ]", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ddd1386d116b-0", "text": "In this example once the token count is reached the oldest messages in the conversation\ntranscript will be removed. del is used instead of pop() for efficiency, and we start at\nindex 1 so as to always preserve the system message and only remove user/assistant\nmessages. Over time, this method of managing the conversation can cause the\nconversation quality to degrade as the model will gradually lose context of the earlier\nportions of the conversation.\nAn alternative approach is to limit the conversation duration to the max token length or\na certain number of turns. Once the max token limit is reached and the model would\nlose context if you were to allow the conversation to continue, you can prompt the user\nthat they need to begin a new conversation and clear the messages array to start a\nbrand new conversation with the full token limit available.\nThe token counting portion of the code demonstrated previously, is a simplified version\nof one of OpenAI's cookbook examples .\nLearn more about Azure OpenAI .\nGet started with the ChatGPT model with the ChatGPT quickstart .\nFor more examples, check out the Azure OpenAI Samples GitHub repository[0]['message' ]['content' ]}) \n    print( \"\\n\" + response[ 'choices' ][0]['message' ]['content' ] + \"\\n\") \nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "61ec32feaeac-0", "text": "Learn how to generate or manipulate\ntext\nArticle \u202202/17/2023\nThe completions endpoint can be used for a wide variety of tasks. It provides a simple\nbut powerful text-in, text-out interface to any of our models . You input some text as a\nprompt, and the model will generate a text completion that attempts to match whatever\ncontext or pattern you gave it. For example, if you give the API the prompt, \"As\nDescartes said, I think, therefore\", it will return the completion \" I am\" with high\nprobability.\nThe best way to start exploring completions is through our playground in Azure OpenAI\nStudio . It's a simple text box where you can submit a prompt to generate a\ncompletion. Y ou can start with a simple example like the following:\nwrite a tagline for an ice cream shop\nonce you submit, you'll see something like the following generated:\nConsole\nThe actual completion results you see may differ because the API is stochastic by\ndefault. In other words, you might get a slightly different completion every time you call\nit, even if your prompt stays the same. Y ou can control this behavior with the\ntemperature setting.\nThis simple, \"text in, text out\" interface means you can \"program\" the model by\nproviding instructions or just a few examples of what you'd like it to do. Its success\ngenerally depends on the complexity of the task and quality of your prompt. A general\nrule is to think about how you would write a word problem for a middle school student\nto solve. A well-written prompt provides enough information for the model to know\nwhat you want and how it should respond.\nwrite a tagline for an ice cream shop  \nwe serve up smiles with every scoop!  \n\uff17 Note\nKeep in mind that the models' training data cuts off in October 2019, so they may", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "61ec32feaeac-1", "text": "not have knowledge of current events. W e plan to add more continuous training in\nthe future.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7ebcb12390e8-0", "text": "OpenAI's models can do everything from generating original stories to performing\ncomplex text analysis. Because they can do so many things, you have to be explicit in\nshowing what you want. Showing, not just telling, is often the secret to a good prompt.\nThe models try to predict what you want from the prompt. If you send the words \"Give\nme a list of cat breeds,\" the model wouldn't automatically assume that you're asking for\na list of cat breeds. Y ou could as easily be asking the model to continue a conversation\nwhere the first words are \"Give me a list of cat breeds\" and the next ones are \"and I'll tell\nyou which ones I like.\" If the model only assumed that you wanted a list of cats, it\nwouldn't be as good at content creation, classification, or other tasks.\nThere are three basic guidelines to creating prompts:\nShow and t ell. Make it clear what you want either through instructions, examples, or a\ncombination of the two. If you want the model to rank a list of items in alphabetical\norder or to classify a paragraph by sentiment, show it that's what you want.\nProvide quality data.  If you're trying to build a classifier or get the model to follow a\npattern, make sure that there are enough examples. Be sure to proofread your examples\n\u2014 the model is usually smart enough to see through basic spelling mistakes and give\nyou a response, but it also might assume that the mistakes are intentional and it can\naffect the response.\nCheck y our settings.  The temperature and top_p settings control how deterministic the\nmodel is in generating a response. If you're asking it for a response where there's only\none right answer, then you'd want to set these settings to lower values. If you're looking", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7ebcb12390e8-1", "text": "for a response that's not obvious, then you might want to set them to higher values. The\nnumber one mistake people use with these settings is assuming that they're \"cleverness\"\nor \"creativity\" controls.\nIf you're having trouble getting the API to perform as expected, follow this checklist:\n1. Is it clear what the intended generation should be?\n2. Are there enough examples?\n3. Did you check your examples for mistakes? (The API won't tell you directly)Prompt design\nBasics\nTroubleshooting", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9c395a3d1f00-0", "text": "4. Are you using temp and top_p correctly?\nTo create a text classifier with the API we provide a description of the task and provide a\nfew examples. In this demonstration we show the API how to classify the sentiment of\nTweets.\nConsole\nIt's worth paying attention to several features in this example:\n1. Use plain language t o describe y our inputs and outputs  We use plain language for\nthe input \"T weet\" and the expected output \"Sentiment.\" For best practices, start with\nplain language descriptions. While you can often use shorthand or keys to indicate the\ninput and output, when building your prompt it's best to start by being as descriptive as\npossible and then working backwards removing extra words as long as the performance\nto the prompt is consistent.\n2. Show the API how t o respond t o any case  In this example we provide multiple\noutcomes \"P ositive\", \"Negative\" and \"Neutral.\" A neutral outcome is important because\nthere will be many cases where even a human would have a hard time determining if\nsomething is positive or negative and situations where it's neither.\n3. You can use t ext and emoji  The classifier is a mix of text and emoji \ud83d\udc4d . The API reads\nemoji and can even convert expressions to and from them.\n4. You need few er examples for familiar tasks  For this classifier we only provided a\nhandful of examples. This is because the API already has an understanding of sentimentClassification\nThis is a tweet sentiment classifier  \nTweet: \"I loved the new Batman movie!\"  \nSentiment: Positive  \nTweet: \"I hate it when my phone battery dies.\"  \nSentiment: Negative  \nTweet: \"My day has been \ud83d\udc4d \" \nSentiment: Positive  \nTweet: \"This is the link to the article\"  \nSentiment: Neutral", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9c395a3d1f00-1", "text": "Tweet: \"This is the link to the article\"  \nSentiment: Neutral  \nTweet: \"This new music video blew my mind\"  \nSentiment:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4ce6623aac1d-0", "text": "and the concept of a tweet. If you're building a classifier for something the API might\nnot be familiar with, it might be necessary to provide more examples.\nNow that we have a grasp of how to build a classifier, let's take that example and make\nit even more efficient so that we can use it to get multiple results back from one API call.\nAfter showing the API how tweets are classified by sentiment we then provide it a list of\ntweets and then a list of sentiment ratings with the same number index. The API is ableImproving the classifier's efficiency\nThis is a tweet sentiment classifier  \nTweet: \"I loved the new Batman movie!\"  \nSentiment: Positive  \nTweet: \"I hate it when my phone battery dies\"  \nSentiment: Negative  \nTweet: \"My day has been \ud83d\udc4d \" \nSentiment: Positive  \nTweet: \"This is the link to the article\"  \nSentiment: Neutral  \nTweet text  \n1. \"I loved the new Batman movie!\"  \n2. \"I hate it when my phone battery dies\"  \n3. \"My day has been \ud83d\udc4d \" \n4. \"This is the link to the article\"  \n5. \"This new music video blew my mind\"  \nTweet sentiment ratings:  \n1: Positive  \n2: Negative  \n3: Positive  \n4: Neutral  \n5: Positive  \nTweet text  \n1. \"I can't stand homework\"  \n2. \"This sucks. I'm bored \ud83d\ude20 \" \n3. \"I can't wait for Halloween!!!\"  \n4. \"My cat is adorable \u2764 \u2764 \" \n5. \"I hate chocolate\"  \nTweet sentiment ratings:  \n1.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6208daa37fcf-0", "text": "to pick up from the first example how a tweet is supposed to be classified. In the second\nexample it sees how to apply this to a list of tweets. This allows the API to rate five (and\neven more) tweets in just one API call.\nIt's important to note that when you ask the API to create lists or evaluate text you need\nto pay extra attention to your probability settings (T op P or T emperature) to avoid drift.\n1. Make sure your probability setting is calibrated correctly by running multiple tests.\n2. Don't make your list too long or the API is likely to drift.\nOne of the most powerful yet simplest tasks you can accomplish with the API is\ngenerating new ideas or versions of input. Y ou can give the API a list of a few story ideas\nand it will try to add to that list. W e've seen it create business plans, character\ndescriptions and marketing slogans just by providing it a handful of examples. In this\ndemonstration we'll use the API to create more examples for how to use virtual reality in\nthe classroom:\nAll we had to do in this example is provide the API with just a description of what the list\nis about and one example. W e then prompted the API with the number 2. indicating\nthat it's a continuation of the list.\nAlthough this is a very simple prompt, there are several details worth noting:\n1. We explained the int ent o f the list  \nJust like with the classifier, we tell the API up front what the list is about. This helps it\nfocus on completing the list and not trying to guess what the pattern is behind it.\n2. Our example sets the p attern for the r est o f the list  \nBecause we provided a one-sentence description, the API is going to try to follow that", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6208daa37fcf-1", "text": "Because we provided a one-sentence description, the API is going to try to follow that\npattern for the rest of the items it adds to the list. If we want a more verbose response,\nwe need to set that up from the start.Generation\nIdeas involving education and virtual reality  \n1. Virtual Mars  \nStudents get to explore Mars via virtual reality and go on missions to  \ncollect and catalog what they see.  \n2.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "dbcd0e821ea5-0", "text": "3. We prompt the API by adding an incomplet e entr y \nWhen the API sees 2. and the prompt abruptly ends, the first thing it tries to do is\nfigure out what should come after it. Since we already had an example with number one\nand gave the list a title, the most obvious response is to continue adding items to the\nlist.\nAdvanced generation t echniques  \nYou can improve the quality of the responses by making a longer more diverse list in\nyour prompt. One way to do that is to start off with one example, let the API generate\nmore and select the ones that you like best and add them to the list. A few more high-\nquality variations can dramatically improve the quality of the responses.\nThe API is extremely adept at carrying on conversations with humans and even with\nitself. With just a few lines of instruction, we've seen the API perform as a customer\nservice chatbot that intelligently answers questions without ever getting flustered or a\nwise-cracking conversation partner that makes jokes and puns. The key is to tell the API\nhow it should behave and then provide a few examples.\nHere's an example of the API playing the role of an AI answering questions:\nThis is all it takes to create a chatbot capable of carrying on a conversation. But\nunderneath its simplicity there are several things going on that are worth paying\nattention to:\n1. We tell the API the int ent but w e also t ell it how t o behav e Just like the other\nprompts, we cue the API into what the example represents, but we also add another key\ndetail: we give it explicit instructions on how to interact with the phrase \"The assistant is\nhelpful, creative, clever, and very friendly.\"\nWithout that instruction the API might stray and mimic the human it's interacting with", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "dbcd0e821ea5-1", "text": "Without that instruction the API might stray and mimic the human it's interacting with\nand become sarcastic or some other behavior we want to avoid.Conversation\nThe following is a conversation with an AI assistant. The assistant is  \nhelpful, creative, clever, and very friendly.  \nHuman: Hello, who are you?  \nAI: I am an AI created by OpenAI. How can I help you today?  \nHuman:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fd2083045cdf-0", "text": "2. We giv e the API an identity  At the start we have the API respond as an AI that was\ncreated by OpenAI. While the API has no intrinsic identity, this helps it respond in a way\nthat's as close to the truth as possible. Y ou can use identity in other ways to create other\nkinds of chatbots. If you tell the API to respond as a woman who works as a research\nscientist in biology, you'll get intelligent and thoughtful comments from the API similar\nto what you'd expect from someone with that background.\nIn this example we create a chatbot that is a bit sarcastic and reluctantly answers\nquestions:\nTo create an amusing and somewhat helpful chatbot we provide a few examples of\nquestions and answers showing the API how to reply. All it takes is just a few sarcastic\nresponses and the API is able to pick up the pattern and provide an endless number of\nsnarky responses.\nThe API is a language model that is familiar with a variety of ways that words and\ncharacters can be used to express information. This ranges from natural language text to\ncode and languages other than English. The API is also able to understand content on a\nlevel that allows it to summarize, convert and express it in different ways.\nIn this example we show the API how to convert from English to French:Marv is a chatbot that reluctantly answers questions.  \n### \nUser: How many pounds are in a kilogram?  \nMarv: This again? There are 2.2 pounds in a kilogram. Please make a note of  \nthis. \n### \nUser: What does HTML stand for?  \nMarv: Was Google too busy? Hypertext Markup Language. The T is for try to  \nask better questions in the future.  \n### \nUser: When did the first airplane fly?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fd2083045cdf-1", "text": "### \nUser: When did the first airplane fly?  \nMarv: On December 17, 1903, Wilbur and Orville Wright made the first  \nflights. I wish they'd come and take me away.  \n### \nUser: Who was the first man in space?  \nMarv:  \nTransformation\nTranslation", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9cad398efda6-0", "text": "This example works because the API already has a grasp of French, so there's no need to\ntry to teach it this language. Instead, we just need to provide enough examples that API\nunderstands that it's converting from one language to another.\nIf you want to translate from English to a language the API is unfamiliar with you'd need\nto provide it with more examples and a fine-tuned model to do it fluently.\nIn this example we convert the name of a movie into emoji. This shows the adaptability\nof the API to picking up patterns and working with other characters.\nThe API is able to grasp the context of text and rephrase it in different ways. In this\nexample, the API takes a block of text and creates an explanation a child would\nunderstand. This illustrates that the API has a deep grasp of language.English: I do not speak French.  \nFrench: Je ne parle pas fran\u00e7ais.\nEnglish: See you later!  \nFrench: \u00c0 tout \u00e0 l'heure!  \nEnglish: Where is a good restaurant?  \nFrench: O\u00f9 est un bon restaurant?\nEnglish: What rooms do you have available?  \nFrench: Quelles chambres avez-vous de disponible?  \nEnglish:  \nConversion\nBack to Future: \ud83d\udc68\ud83d\udc74\ud83d\ude97\ud83d\udd52 \nBatman: \ud83e\udd35\ud83e\udd87 \nTransformers: \ud83d\ude97\ud83e\udd16 \nWonder Woman: \u0000\u0000\u0000\u0000\u0000 \nSpider-Man: \ud83d\udd78\ud83d\udd77\ud83d\udd78\ud83d\udd78\ud83d\udd77\ud83d\udd78  \nWinnie the Pooh: \ud83d\udc3b\ud83d\udc3c\ud83d\udc3b \nThe Godfather: \ud83d\udc68\ud83d\udc69\ud83d\udc67\u0000 \u2642 \ud83d\udc72\ud83d\udca5 \nGame of Thrones: \ud83c\udff9 \ud83d\udde1\ud83d\udde1 \ud83c\udff9 \nSpider-Man:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9cad398efda6-1", "text": "Spider-Man:  \nSummarization\nMy ten-year-old asked me what this passage means:  \n\"\"\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "abea22664061-0", "text": "In this example we place whatever we want summarized between the triple quotes. It's\nworth noting that we explain both before and after the text to be summarized what our\nintent is and who the target audience is for the summary. This is to keep the API from\ndrifting after it processes a large block of text.\nWhile all prompts result in completions, it can be helpful to think of text completion as\nits own task in instances where you want the API to pick up where you left off. For\nexample, if given this prompt, the API will continue the train of thought about vertical\nfarming. Y ou can lower the temperature setting to keep the API more focused on the\nintent of the prompt or increase it to let it go off on a tangent.\nThis next prompt shows how you can use completion to help write R eact components.\nWe send some code to the API, and it's able to continue the rest because it has an\nunderstanding of the R eact library. W e recommend using models from our Codex series\nfor tasks that involve understanding or generating code. Currently, we support two\nCodex models: code-davinci-002 and code-cushman-001. For more information about\nCodex models, see the Codex models  section in Models .A neutron star is the collapsed core of a massive supergiant star, which had  \na total mass of between 10 and 25 solar masses, possibly more if the star  \nwas especially metal-rich.[1] Neutron stars are the smallest and densest  \nstellar objects, excluding black holes and hypothetical white holes, quark  \nstars, and strange stars.[2] Neutron stars have a radius on the order of 10  \nkilometres (6.2 mi) and a mass of about 1.4 solar masses.[3] They result", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "abea22664061-1", "text": "from the supernova explosion of a massive star, combined with gravitational  \ncollapse, that compresses the core past white dwarf star density to that of  \natomic nuclei.  \n\"\"\" \nI rephrased it for him, in plain language a ten-year-old can understand:  \n\"\"\" \nCompletion\nVertical farming provides a novel solution for producing food locally,  \nreducing transportation costs and\nimport React from 'react';  \nconst HeaderComponent = () => (", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b201d62f98c3-0", "text": "The API has a lot of knowledge that it's learned from the data it was trained on. It also\nhas the ability to provide responses that sound very real but are in fact made up. There\nare two ways to limit the likelihood of the API making up an answer.\n1. Provide a gr ound truth for the API  If you provide the API with a body of text to\nanswer questions about (like a Wikipedia entry) it will be less likely to confabulate a\nresponse.\n2. Use a low pr obability and show the API how t o say \"I don 't know\"  If the API\nunderstands that in cases where it's less certain about a response that saying \"I don't\nknow\" or some variation is appropriate, it will be less inclined to make up answers.\nIn this example we give the API examples of questions and answers it knows and then\nexamples of things it wouldn't know and provide question marks. W e also set the\nprobability to zero so the API is more likely to respond with a \"?\" if there's any doubt.Factual responses\nQ: Who is Batman?  \nA: Batman is a fictional comic book character.  \nQ: What is torsalplexity?  \nA: ? \nQ: What is Devz9?  \nA: ? \nQ: Who is George Lucas?  \nA: George Lucas is American film director and producer famous for creating  \nStar Wars.  \nQ: What is the capital of California?  \nA: Sacramento.  \nQ: What orbits the Earth?  \nA: The Moon.  \nQ: Who is Fred Rickerson?  \nA: ? \nQ: What is an atom?  \nA: An atom is a tiny particle that makes up everything.  \nQ: Who is Alvan Muntz?  \nA: ?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b201d62f98c3-1", "text": "Q: Who is Alvan Muntz?  \nA: ? \nQ: What is Kozar-09?  \nA: ?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2815f47c977a-0", "text": "The Codex model series is a descendant of OpenAI's base GPT-3 series that's been\ntrained on both natural language and billions of lines of code. It's most capable in\nPython and proficient in over a dozen languages including C#, JavaScript, Go, P erl, PHP,\nRuby, S wift, T ypeScript, SQL, and even Shell.\nLearn more about generating code completions, with the working with code guide\nLearn how to work with code (Codex) . Learn more about the underlying models that\npower Azure OpenAI .Q: How many moons does Mars have?\nA: Two, Phobos and Deimos.  \nQ: \nWorking with code\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b9d269216451-0", "text": "How to switch between OpenAI and\nAzure OpenAI endpoints with Python\nArticle \u202207/07/2023\nWhile OpenAI and Azure OpenAI Service rely on a common Python client library , there\nare small changes you need to make to your code in order to swap back and forth\nbetween endpoints. This article walks you through the common changes and differences\nyou'll experience when working across OpenAI and Azure OpenAI.\nWe recommend using environment variables. If you haven't done this before our Python\nquickstarts  walk you through this configuration.\nOpenAI Azure OpenAI\nPython Python\nOpenAI Azure OpenAI\n\uff17 Note\nThis library is maintained by OpenAI and is currently in preview. R efer to the release\nhistory or the version.py commit hist ory to track the latest updates to the\nlibrary.\nAuth entication\nAPI key\nimport openai \nopenai.api_key =  \n\"sk-...\"  \nopenai.organization  \n= \"...\" import openai \nopenai.api_type = \"azure\" \nopenai.api_key = \"...\" \nopenai.api_base = \"https://example-\nendpoint.openai.azure.com\"  \nopenai.api_version = \"2023-05-15\"   # subject to  \nchange \nAzure Active Directory authentication", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8c081dffc6d7-0", "text": "Python Python\nOpenAI uses the model keyword argument to specify what model to use. Azure OpenAI\nhas the concept of deployments  and uses the deployment_id keyword argument to\ndescribe which model deployment to use. Azure OpenAI also supports the use of\nengine interchangeably with deployment_id.\nFor OpenAI engine still works in most instances, but it's deprecated and model is\npreferred.\nOpenAI Azure OpenAI\nPython Pythonimport openai \nopenai.api_key =  \n\"sk-...\"  \nopenai.organizat\nion = \"...\" import openai \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()  \ntoken = \ncredential.get_token( \"https://cognitiveservices.az\nure.com/.default\" ) \nopenai.api_type = \"azuread\"  \nopenai.api_key = token.token  \nopenai.api_base = \"https://example-\nendpoint.openai.azure.com\"  \nopenai.api_version = \"2023-05-15\"   # subject to  \nchange \nKeyword argument for model\ncompletion =  \nopenai.Completion.create(  \n    prompt= \"<prompt>\" , \n    model= \"text-davinci-003\"  \n) \n   \nchat_completion =  \nopenai.ChatCompletion.create(  \n    messages= \"<messages>\" , \n    model= \"gpt-4\" \n) \nembedding =  \nopenai.Embedding.create(  \n  input= \"<input>\" , \n  model= \"text-embedding-ada-002\"  completion =  \nopenai.Completion.create(  \n    prompt= \"<prompt>\" , \n    deployment_id= \"text-davinci-\n003\" \n    #engine=\"text-davinci-003\"  \n) \n   \nchat_completion =", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8c081dffc6d7-1", "text": ") \n   \nchat_completion =  \nopenai.ChatCompletion.create(  \n    messages= \"<messages>\" , \n    deployment_id= \"gpt-4\" \n    #engine=\"gpt-4\"  \n)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8db6fa60a78f-0", "text": "Many examples show passing multiple inputs into the embeddings API. For Azure\nOpenAI, currently we must pass a single text input per call.\nOpenAI Azure OpenAI\nPython Python\nLearn more about how to work with GPT-35-Turbo and the GPT-4 models with our\nhow-to guide .\nFor more examples, check out the Azure OpenAI Samples GitHub repository) embedding =  \nopenai.Embedding.create(  \n  input= \"<input>\" , \n  deployment_id= \"text-embedding-\nada-002\"  \n  #engine=\"text-embedding-ada-\n002\" \n) \nAzure OpenAI embeddings doesn't support\nmultip le inputs\ninputs = [ \"A\", \"B\", \"C\"] \nembedding =  \nopenai.Embedding.create(  \n  input=inputs,  \n  model= \"text-embedding-ada-\n002\" \n) inputs = [ \"A\", \"B\", \"C\"] \nfor text in inputs:  \n    embedding =  \nopenai.Embedding.create(  \n        input=text,  \n        deployment_id= \"text-\nembedding-ada-002\"  \n        #engine=\"text-embedding-ada-\n002\" \n    ) \nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6cef011f97bf-0", "text": "How to configure content filters with\nAzure OpenAI Service\nArticle \u202206/09/2023\nThe content filtering system integrated into Azure OpenAI Service runs alongside the\ncore models and uses an ensemble of multi-class classification models to detect four\ncategories of harmful content (violence, hate, sexual, and self-harm) at four severity\nlevels respectively (safe, low, medium, and high). The default content filtering\nconfiguration is set to filter at the medium severity threshold for all four content harms\ncategories for both prompts and completions. That means that content that is detected\nat severity level medium or high is filtered, while content detected at severity level low\nor safe is not filtered by the content filters. Learn more about content categories,\nseverity levels, and the behavior of the content filtering system here.\nContent filters can be configured at resource level. Once a new configuration is created,\nit can be associated with one or more deployments. For more information about model\ndeployment, see the resource deployment guide .\nThe configurability feature is available in preview and allows customers to adjust the\nsettings, separately for prompts and completions, to filter content for each content\ncategory at different severity levels as described in the table below. Content detected at\nthe 'safe' severity level is labeled in annotations but is not subject to filtering and is not\nconfigurable.\nSeverity\nfilteredConfigurable\nfor pr omptsConfigurable\nfor\ncompletionsDescr iptions\uff17 Note\nAll customers have the ability to modify the content filters to be stricter (for\nexample, to filter content at lower severity levels than the default). Approval is\nrequired for full content filtering control, including (i) configuring content filters at\nseverity level high only (ii) or turning the content filters off. Managed customers\nonly may apply for full content filtering control via this form: Azur e OpenAI Limit ed", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6cef011f97bf-1", "text": "only may apply for full content filtering control via this form: Azur e OpenAI Limit ed\nAccess R eview: Modified Cont ent Filt ers and Abuse Monit oring\n(micr osoft.com) .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9113dc20ca37-0", "text": "Severity\nfilteredConfigurable\nfor pr omptsConfigurable\nfor\ncompletionsDescr iptions\nLow,\nmedium,\nhighYes Yes Strictest filtering configuration. Content detected at\nseverity levels low, medium and high is filtered.\nMedium,\nhighYes Yes Default setting. Content detected at severity level low\nis not filtered, content at medium and high is filtered.\nHigh If approved If approved Content detected at severity levels low and medium\nis not filtered. Only content at severity level high is\nfiltered. R equires approval .\nNo\nfiltersIf approved If approved No content is filtered regardless of severity level\ndetected. R equires approval .\n Only approved customers have full content filtering control, including configuring\ncontent filters at severity level high only or turning the content filters off. Managed\ncustomers only can apply for full content filtering control via this form: Azure OpenAI\nLimited Access R eview: Modified Content Filters and Abuse Monitoring\n(microsoft.com)\nThe following steps show how to set up a customized content filtering configuration for\nyour resource.\n1. Go to Azure OpenAI S tudio and navigate to the Content Filters tab (in the bottom\nleft navigation, as designated by the red box below).* *\n*\n* *\n*\n*\nConfiguring content filters via Azure OpenAI\nStudio (preview)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f58dad218659-0", "text": "2. Create a new customized content filtering configuration.\nThis leads to the following configuration view, where you can choose a name for\nthe custom content filtering configuration.\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "481a39546d6a-0", "text": "3. This is the view of the default content filtering configuration, where content is\nfiltered at medium and high severity levels for all categories. Y ou can modify the\ncontent filtering severity level for both prompts and completions separately\n(configuration for prompts is in the left column and configuration for completions\nis in the right column, as designated with the blue boxes below) for each of the\nfour content categories (content categories are listed on the left side of the screen,\nas designated with the green box below). There are three severity levels for each\ncategory that are partially or fully configurable: Low, medium, and high (labeled at\nthe top of each column, as designated with the red box below).\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b540f67f4879-0", "text": "4. If you determine that your application or usage scenario requires stricter filtering\nfor some or all content categories, you can configure the settings, separately for\nprompts and completions, to filter at more severity levels than the default setting.\nAn example is shown in the image below, where the filtering level for user prompts\nis set to the strictest configuration for hate and sexual, with low severity content\nfiltered along with content classified as medium and high severity (outlined in the\nred box below). In the example, the filtering levels for model completions are set at\nthe strictest configuration for all content categories (blue box below). With this\nmodified filtering configuration in place, low, medium, and high severity content\nwill be filtered for the hate and sexual categories in user prompts; medium and\nhigh severity content will be filtered for the self-harm and violence categories in\nuser prompts; and low, medium, and high severity content will be filtered for all\ncontent categories in model completions.\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4afbc37aa770-0", "text": "5. If your use case was approved for modified content filters as outlined above, you\nwill receive full control over content filtering configurations. With full control, you\ncan choose to turn filtering off, or filter only at severity level high, while accepting\nlow and medium severity content. In the image below, filtering for the categories\nof self-harm and violence is turned off for user prompts (red box below), while\ndefault configurations are retained for other categories for user prompts. For\nmodel completions, only high severity content is filtered for the category self-harm\n(blue box below), and filtering is turned off for violence (green box below), while\ndefault configurations are retained for other categories.\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "55a75f605a78-0", "text": "You can create multiple content filtering configurations as per your requirements.\n6. Next, to make a custom content filtering configuration operational, assign a\nconfiguration to one or more deployments in your resource. T o do this, go to the\nDeployments  tab and select Edit deployment  (outlined near the top of the screen\nin a red box below).\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0e0a1698686b-0", "text": "7. Go to advanced options (outlined in the blue box below) select the content filter\nconfiguration suitable for that deployment from the Cont ent Filt er dropdown\n(outlined near the bottom of the dialog box in the red box below).\n8. Select Save and close  to apply the selected configuration to the deployment.\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "dc6e3c6ba399-0", "text": "9. You can also edit and delete a content filter configuration if required. T o do this,\nnavigate to the content filters tab and select the desired action (options outlined\nnear the top of the screen in the red box below). Y ou can edit/delete only one\nfiltering configuration at a time.\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c455953dbec9-0", "text": "We recommend informing your content filtering configuration decisions through an\niterative identification (for example, red team testing, stress-testing, and analysis) and\nmeasurement process to address the potential harms that are relevant for a specific\nmodel, application, and deployment scenario. After implementing mitigations such as\ncontent filtering, repeat measurement to test effectiveness. R ecommendations and best\npractices for R esponsible AI for Azure OpenAI, grounded in the Microsoft R esponsible AI\nStandard  can be found in the Responsible AI Overview for Azure OpenAI .\nLearn more about R esponsible AI practices for Azure OpenAI: Overview of\nResponsible AI practices for Azure OpenAI models .\nRead more about content filtering categories and severity levels  with Azure\nOpenAI Service.\nLearn more about red teaming from our: Introduction to red teaming large\nlanguage models (LLMs) article .\uff17 Note\nBefore deleting a content filtering configuration, you will need to unassign it\nfrom any deployment in the Deployments tab.\nBest practices\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "70ba36430952-0", "text": "Codex mo dels and Azure OpenAI\nService\nArticle \u202202/17/2023\nThe Codex model series is a descendant of our GPT-3 series that's been trained on both\nnatural language and billions of lines of code. It's most capable in Python and proficient\nin over a dozen languages including C#, JavaScript, Go, P erl, PHP, Ruby, S wift,\nTypeScript, SQL, and even Shell.\nYou can use Codex for a variety of tasks including:\nTurn comments into code\nComplete your next line or function in context\nBring knowledge to you, such as finding a useful library or API call for an\napplication\nAdd comments\nRewrite code for efficiency\nHere are a few examples of using Codex that can be tested in Azure OpenAI S tudio's\nplayground with a deployment of a Codex series model, such as code-davinci-002.\nPython\nPythonHow to use the Codex models\nSaying \"Hello\" (Python)\n\"\"\" \nAsk the user for their name and say \"Hello\"  \n\"\"\" \nCreate random names (Python)\n\"\"\" \n1. Create a list of first names  \n2. Create a list of last names  \n3. Combine them randomly into a list of 100 full names  \n\"\"\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "237f40d2cdf1-0", "text": "Python\nJavaScript\nYou can experiment using one of the Codex models in our playground (styling\ninstructions as comments when needed.)\nTo get Codex to create a useful completion, it's helpful to think about what information\na programmer would need to perform a task. This could simply be a clear comment or\nthe data needed to write a useful function, like the names of variables or what class a\nfunction handles.\nIn this example we tell Codex what to call the function and what task it's going to\nperform.\nPythonCreate a MySQL query (Python)\n\"\"\" \nTable customers, columns = [CustomerId, FirstName, LastName, Company,  \nAddress, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId]  \nCreate a MySQL query for all customers in Texas named Jane  \n\"\"\" \nquery = \nExplaining code (JavaScript)\n// Function 1  \nvar fullNames = [];  \nfor (var i = 0; i < 50; i++) {  \n  fullNames.push(names[ Math.floor(Math.random() * names.length)]  \n    + \" \" + lastNames[ Math.floor(Math.random() * lastNames.length)]);  \n} \n// What does Function 1 do?  \nBest practices\nStart with a comment, data or code\n# Create a function called 'nameImporter' to add a first and last name to  \nthe database", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4e7358d77c92-0", "text": "This approach scales even to the point where you can provide Codex with a comment\nand an example of a database schema to get it to write useful query requests for various\ndatabases. Here's an example where we provide the columns and table names for the\nquery.\nPython\nWhen you show Codex the database schema, it's able to make an informed guess about\nhow to format a query.\nCodex understands dozens of different programming languages. Many share similar\nconventions for comments, functions and other programming syntax. By specifying the\nlanguage and what version in a comment, Codex is better able to provide a completion\nfor what you want. That said, Codex is fairly flexible with style and syntax. Here's an\nexample for R and Python.\nR\nPython\nIf you want Codex to create a webpage, placing the first line of code in an HTML\ndocument ( <!DOCTYPE html>) after your comment tells Codex what it should do next.# Table albums, columns = [AlbumId, Title, ArtistId]  \n# Table artists, columns = [ArtistId, Name]  \n# Table media_types, columns = [MediaTypeId, Name]  \n# Table playlists, columns = [PlaylistId, Name]  \n# Table playlist_track, columns = [PlaylistId, TrackId]  \n# Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId,  \nComposer, Milliseconds, Bytes, UnitPrice]  \n# Create a query for all albums with more than 10 tracks  \nSpecify the programming language\n# R language  \n# Calculate the mean distance between an array of points  \n# Python 3  \n# Calculate the mean distance between an array of points  \nPrompt Codex with what you want it to do", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f16512c05517-0", "text": "The same method works for creating a function from a comment (following the\ncomment with a new line starting with func or def).\nHTML\nPlacing <!DOCTYPE html> after our comment makes it very clear to Codex what we want\nit to do.\nOr if we want to write a function we could start the prompt as follows and Codex will\nunderstand what it needs to do next.\nPython\nCodex is aware of a large number of libraries, APIs and modules. By telling Codex which\nones to use, either from a comment or importing them into your code, Codex will make\nsuggestions based upon them instead of alternatives.\nHTML\nBy specifying the version, you can make sure Codex uses the most current library.<!-- Create a web page with the title 'Kat Katman attorney at paw' -->  \n<!DOCTYPE html>  \n# Create a function to count to 100  \ndef counter \nSpecifying libraries will help Codex understand what you\nwant\n<!-- Use A-Frame version 1.2.0 to create a 3D website -->  \n<!-- https://aframe.io/releases/1.2.0/aframe.min.js -->  \n\uff17 Note\nCodex can suggest helpful libraries and APIs, but always be sure to do your own\nresearch to make sure that they're safe for your application.\nComment style can affect code quality", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a0604e299af7-0", "text": "With some languages, the style of comments can improve the quality of the output. For\nexample, when working with Python, in some cases using doc strings (comments\nwrapped in triple quotes) can give higher quality results than using the pound ( #)\nsymbol.\nPython\nRecommended coding standards usually suggest placing the description of a function\ninside the function. Using this format helps Codex more clearly understand what you\nwant the function to do.\nPython\nIf you have a particular style or format you need Codex to use, providing examples or\ndemonstrating it in the first part of the request will help Codex more accurately match\nwhat you need.\nPython\nSetting the API temperature to 0, or close to zero (such as 0.1 or 0.2) tends to give\nbetter results in most cases. Unlike GPT-3 models, where a higher temperature can\"\"\" \nCreate an array of users and email addresses  \n\"\"\" \nComments inside of functions can be helpful\ndef getUserBalance (id): \n    \"\"\" \n    Look up the user in the database \u2018UserData' and return their current  \naccount balance.  \n    \"\"\" \nProvide examples for more precise results\n\"\"\" \nCreate a list of random animals and species  \n\"\"\" \nanimals  = [ { \"name\": \"Chomper\" , \"species\" : \"Hamster\" }, {\"name\": \nLower temperatures give more precise results", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a8e9622609ac-0", "text": "provide useful creative and random results, higher temperatures with Codex models\nmay give you really random or erratic responses.\nIn cases where you need Codex to provide different potential results, start at zero and\nthen increment upwards by 0.1 until you find suitable variation.\nWe can get Codex to write functions by specifying what the function should do in as\nprecise terms as possible in comment. For example, by writing the following comment,\nCodex creates a JavaScript timer function that's triggered when a user presses a button:\nA simple JavaScript timer\nJavaScript\nTesting applications often requires using example data. Because Codex is a language\nmodel that understands how to comprehend and write natural language, you can ask\nCodex to create data like arrays of made up names, products and other variables. For\nexample, here we ask Codex to create an array of weather temperatures.\nJavaScript\nAsking Codex to perform this task will produce a table like this:\nJavaScriptOrganize tasks into functions\n// Create a timer that creates an alert in 10 seconds  \nCreating example data\n/* Create an array of weather temperatures for San Francisco */  \nvar weather = [  \n  { month: 'January' , high: 58, low: 48 }, \n  { month: 'February' , high: 61, low: 50 }, \n  { month: 'March', high: 64, low: 53 }, \n  { month: 'April', high: 67, low: 55 }, \n  { month: 'May', high: 70, low: 58 }, \n  { month: 'June', high: 73, low: 61 }, \n  { month: 'July', high: 76, low: 63 }, \n  { month: 'August' , high: 77, low: 64 },", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a8e9622609ac-1", "text": "{ month: 'August' , high: 77, low: 64 }, \n  { month: 'September' , high: 76, low: 63 }, \n  { month: 'October' , high: 73, low: 61 }, \n  { month: 'November' , high: 68, low: 57 },", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "05a80dd5d193-0", "text": "We can provide Codex with a comment consisting of a complex request like creating a\nrandom name generator or performing tasks with user input and Codex can generate\nthe rest provided there are enough tokens.\nJavaScript\nRequesting longer completions in Codex can lead to imprecise answers and repetition.\nLimit the size of the query by reducing max_tokens and setting stop tokens. For\ninstance, add \\n as a stop sequence to limit completions to one line of code. Smaller\ncompletions also incur less latency.\nLarge Codex queries can take tens of seconds to complete. T o build applications that\nrequire lower latency, such as coding assistants that perform autocompletion, consider\nusing streaming. R esponses will be returned before the model finishes generating the\nentire completion. Applications that need only part of a completion can reduce latency\nby cutting off a completion either programmatically or by using creative values for stop.\nUsers can combine streaming with duplication to reduce latency by requesting more\nthan one solution from the API, and using the first response returned. Do this by setting\nn > 1. This approach consumes more token quota, so use carefully (for example, by\nusing reasonable settings for max_tokens and stop).  { month: 'December' , high: 64, low: 54 } \n]; \nCompound functions and small applications\n/* \nCreate a list of animals  \nCreate a list of cities  \nUse the lists to generate stories about what I saw at the zoo in each city  \n*/ \nLimit completion size for more precise results or lower\nlatency\nUse streaming to reduce latency\nUse Codex to explain code", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "06d1ee5bc8e0-0", "text": "Codex's ability to create and understand code allows us to use it to perform tasks like\nexplaining what the code in a file does. One way to accomplish this is by putting a\ncomment after a function that starts with \"This function\" or \"This application is.\" Codex\nwill usually interpret this as the start of an explanation and complete the rest of the text.\nJavaScript\nIn this example, we use Codex to explain in a human readable format what an SQL query\nis doing.\nSQL\nCreating a unit test can be accomplished in Python simply by adding the comment \"Unit\ntest\" and starting a function.\nPython\nBy using examples, you can show Codex how to identify errors in code. In some cases no\nexamples are required, however demonstrating the level and detail to provide a/* Explain what the previous function is doing: It  \nExplaining an SQL query\nSELECT DISTINCT  department.name  \nFROM department  \nJOIN employee ON department.id = employee.department_id  \nJOIN salary_payments ON employee.id = salary_payments.employee_id  \nWHERE salary_payments.date BETWEEN '2020-06-01'  AND '2020-06-30'  \nGROUP BY department.name  \nHAVING COUNT(employee.id) > 10; \n-- Explanation of the above query in human readable format  \n-- \nWriting unit tests\n# Python 3  \ndef sum_numbers (a, b): \n  return a + b \n# Unit test  \ndef \nChecking code for errors", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2570ed57d691-0", "text": "description can help Codex understand what to look for and how to explain it. (A check\nby Codex for errors shouldn't replace careful review by the user. )\nJavaScript\nJust as a human programmer would benefit from understanding the database structure\nand the column names, Codex can use this data to help you write accurate query\nrequests. In this example, we insert the schema for a database and tell Codex what to\nquery the database for.\nPython\nYou can get Codex to convert from one language to another by following a simple\nformat where you list the language of the code you want to convert in a comment,\nfollowed by the code and then a comment with the language you want it translated\ninto.\nPython/* Explain why the previous function doesn't work. */  \nUsing source data to write database functions\n# Table albums, columns = [AlbumId, Title, ArtistId]  \n# Table artists, columns = [ArtistId, Name]  \n# Table media_types, columns = [MediaTypeId, Name]  \n# Table playlists, columns = [PlaylistId, Name]  \n# Table playlist_track, columns = [PlaylistId, TrackId]  \n# Table tracks, columns = [TrackId, Name, AlbumId, MediaTypeId, GenreId,  \nComposer, Milliseconds, Bytes, UnitPrice]  \n# Create a query for all albums with more than 10 tracks  \nConverting between languages\n# Convert this from Python to R  \n# Python version  \n[ Python code ]  \n# End \n# R version", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3a4641335bed-0", "text": "If you want Codex to make a function more efficient, you can provide it with the code to\nrewrite followed by an instruction on what format to use.\nJavaScript\nLearn more about the underlying models that power Azure OpenAI .Rewriting code for a library or framework\n// Rewrite this as a React component  \nvar input = document .createElement( 'input'); \ninput.setAttribute( 'type', 'text'); \ndocument .body.appendChild(input);\nvar button = document .createElement( 'button' ); \nbutton.innerHTML = 'Say Hello' ; \ndocument .body.appendChild(button);  \nbutton.onclick = function () { \n  var name = input.value;  \n  var hello = document .createElement( 'div'); \n  hello.innerHTML = 'Hello '  + name;  \n  document .body.appendChild(hello);  \n}; \n// React version:  \nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "da2c5aecdd2a-0", "text": "Learn how to generate embeddings with\nAzure OpenAI\nArticle \u202205/16/2023\nAn embedding is a special format of data representation that can be easily utilized by\nmachine learning models and algorithms. The embedding is an information dense\nrepresentation of the semantic meaning of a piece of text. Each embedding is a vector\nof floating point numbers, such that the distance between two embeddings in the\nvector space is correlated with semantic similarity between two inputs in the original\nformat. For example, if two texts are similar, then their vector representations should\nalso be similar.\nTo obtain an embedding vector for a piece of text, we make a request to the\nembeddings endpoint as shown in the following code snippets:\nConsole\nThe maximum length of input text for our embedding models is 2048 tokens (equivalent\nto around 2-3 pages of text). Y ou should verify that your inputs don't exceed this limit\nbefore making a request.How to get embeddings\nconsole\ncurl \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPL\nOYMENT_NAME/embeddings?api-version=2023-05-15\\  \n  -H 'Content-Type: application/json' \\  \n  -H 'api-key: YOUR_API_KEY' \\  \n  -d '{\"input\": \"Sample Document goes here\"}'  \nBest practices\nVerify inputs don't exceed the maximum length\nChoose the best model for your task", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "96fbbaf643f1-0", "text": "For the search models, you can obtain embeddings in two ways. The <search_model>-\ndoc model is used for longer pieces of text (to be searched over) and the\n<search_model>-query model is used for shorter pieces of text, typically queries or class\nlabels in zero shot classification. Y ou can read more about all of the Embeddings models\nin our Models  guide.\nUnless you're embedding code, we suggest replacing newlines (\\n) in your input with a\nsingle space, as we have observed inferior results when newlines are present.\nOur embedding models may be unreliable or pose social risks in certain cases, and may\ncause harm in the absence of mitigations. R eview our R esponsible AI content for more\ninformation on how to approach their use responsibly.\nLearn more about using Azure OpenAI and embeddings to perform document\nsearch with our embeddings tutorial .\nLearn more about the underlying models that power Azure OpenAI .Replace newlines with a single space\nLimitations & risks\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "067053b89fae-0", "text": "Learn how to prepare your dataset for\nfine-tuning\nArticle \u202203/13/2023\nThe first step of customizing your model is to prepare a high quality dataset. T o do this\nyou'll need a set of training examples composed of single input prompts and the\nassociated desired output ('completion'). This format is notably different than using\nmodels during inference in the following ways:\nOnly provide a single prompt vs a few examples.\nYou don't need to provide detailed instructions as part of the prompt.\nEach prompt should end with a fixed separator to inform the model when the\nprompt ends and the completion begins. A simple separator, which generally\nworks well is \\n\\n###\\n\\n. The separator shouldn't appear elsewhere in any\nprompt.\nEach completion should start with a whitespace due to our tokenization, which\ntokenizes most words with a preceding whitespace.\nEach completion should end with a fixed stop sequence to inform the model when\nthe completion ends. A stop sequence could be \\n, ###, or any other token that\ndoesn't appear in any completion.\nFor inference, you should format your prompts in the same way as you did when\ncreating the training dataset, including the same separator. Also specify the same\nstop sequence to properly truncate the completion.\nThe dataset cannot exceed 100 MB in total file size.\nCustomization performs better with high-quality examples and the more you have,\ngenerally the better the model performs. W e recommend that you provide at least a few\nhundred high-quality examples to achieve a model that performs better than using well-\ndesigned prompts with a base model. From there, performance tends to linearly\nincrease with every doubling of the number of examples. Increasing the number of\nexamples is usually the best and most reliable way of improving performance.\nIf you're fine-tuning on a pre-existing dataset rather than writing prompts from scratch,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "067053b89fae-1", "text": "If you're fine-tuning on a pre-existing dataset rather than writing prompts from scratch,\nbe sure to manually review your data for offensive or inaccurate content if possible, or\nreview as many random samples of the dataset as possible if it's large.Best practices", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d248c18bf7e5-0", "text": "Fine-tuning can solve various problems, and the optimal way to use it may depend on\nyour specific use case. Below, we've listed the most common use cases for fine-tuning\nand corresponding guidelines.\nClassifiers are the easiest models to get started with. For classification problems we\nsuggest using ada, which generally tends to perform only very slightly worse than more\ncapable models once fine-tuned, while being significantly faster. In classification\nproblems, each prompt in the dataset should be classified into one of the predefined\nclasses. For this type of problem, we recommend:\nUse a separator at the end of the prompt, for example, \\n\\n###\\n\\n. Remember to\nalso append this separator when you eventually make requests to your model.\nChoose classes that map to a single token. At inference time, specify\nmax_tokens=1 since you only need the first token for classification.\nEnsure that the prompt + completion doesn't exceed 2048 tokens, including the\nseparator\nAim for at least 100 examples per class\nTo get class log probabilities, you can specify logprobs=5 (for five classes) when\nusing your model\nEnsure that the dataset used for fine-tuning is very similar in structure and type of\ntask as what the model will be used for\nLet's say you'd like to ensure that the text of the ads on your website mentions the\ncorrect product and company. In other words, you want to ensure the model isn't\nmaking things up. Y ou may want to fine-tune a classifier which filters out incorrect ads.\nThe dataset might look something like the following:\nJSONSpecific guidelines\nClassification\nCase study: Is the model making untrue statements?\n{\"prompt\" :\"Company: BHFF insurance\\nProduct: allround insurance\\nAd:One stop  \nshop for all your insurance needs!\\nSupported:\" , \"completion\" :\" yes\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d248c18bf7e5-1", "text": "shop for all your insurance needs!\\nSupported:\" , \"completion\" :\" yes\"} \n{\"prompt\" :\"Company: Loft conversion specialists\\nProduct: -\\nAd:Straight  \nteeth in weeks!\\nSupported:\" , \"completion\" :\" no\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "71e7c5f27d14-0", "text": "In the example above, we used a structured input containing the name of the company,\nthe product, and the associated ad. As a separator we used \\nSupported: which clearly\nseparated the prompt from the completion. With a sufficient number of examples, the\nseparator you choose doesn't make much of a difference (usually less than 0.4%) as long\nas it doesn't appear within the prompt or the completion.\nFor this use case we fine-tuned an ada model since it is faster and cheaper, and the\nperformance is comparable to larger models because it's a classification task.\nNow we can query our model by making a Completion request.\nConsole\nWhich will return either yes or no.\nLet's say you'd like to get a degree to which a particular tweet is positive or negative.\nThe dataset might look something like the following:\nConsole\nOnce the model is fine-tuned, you can get back the log probabilities for the first\ncompletion token by setting logprobs=2 on the completion request. The higher the\nprobability for positive class, the higher the relative sentiment.\nNow we can query our model by making a Completion request.\nConsolecurl \nhttps://YOUR_RESOURCE_NAME.openaiazure.com/openai/deployments/YOUR_DEPLOYMEN\nT_NAME/completions?api-version=2023-05-15\\ \\  \n  -H 'Content-Type: application/json' \\  \n  -H 'api-key: YOUR_API_KEY' \\  \n  -d '{ \n  \"prompt\": \"Company: Reliable accountants Ltd\\nProduct: Personal Tax  \nhelp\\nAd:Best advice in town!\\nSupported:\",  \n  \"max_tokens\": 1  \n }' \nCase study: Sentiment analysis\n{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "71e7c5f27d14-1", "text": "{\"prompt\":\"Overjoyed with the new iPhone! ->\", \"completion\":\" positive\"}  \n{\"prompt\":\"@contoso_basketball disappoint for a third straight night. ->\",  \n\"completion\":\" negative\"}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d6855cf68db8-0", "text": "Which will return:\nJSONcurl \nhttps://YOUR_RESOURCE_NAME.openaiazure.com/openai/deployments/YOUR_DEPLOYMEN\nT_NAME/completions?api-version=2023-05-15\\ \\  \n  -H 'Content-Type: application/json' \\  \n  -H 'api-key: YOUR_API_KEY' \\  \n  -d '{ \n  \"prompt\": \"Excited to share my latest blog post! ->\",  \n  \"max_tokens\": 1,  \n  \"logprobs\": 2  \n }' \n{ \n  \"object\" : \"text_completion\" , \n  \"created\" : 1589498378 , \n  \"model\": \"YOUR_FINE_TUNED_MODEL_NAME\" , \n  \"choices\" : [ \n    { \n      \"logprobs\" : { \n        \"text_offset\" : [ \n          19 \n        ],  \n        \"token_logprobs\" : [ \n          -0.03597255  \n        ],  \n        \"tokens\" : [ \n          \" positive\"  \n        ],  \n        \"top_logprobs\" : [ \n          {  \n            \" negative\" : -4.9785037 , \n            \" positive\" : -0.03597255  \n          }  \n        ]  \n      },  \n      \"text\": \" positive\" , \n      \"index\": 0, \n      \"finish_reason\" : \"length\"  \n    } \n  ] \n} \nCase study: Categorization for Email triage", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "be812e625e62-0", "text": "Let's say you'd like to categorize incoming email into one of a large number of\npredefined categories. For classification into a large number of categories, we\nrecommend you convert those categories into numbers, which will work well with up to\napproximately 500 categories. W e've observed that adding a space before the number\nsometimes slightly helps the performance, due to tokenization. Y ou may want to\nstructure your training data as follows:\nJSON\nFor example:\nJSON\nIn the example above we used an incoming email capped at 2043 tokens as input. (This\nallows for a four token separator and a one token completion, summing up to 2048.) As\na separator we used \\n\\n###\\n\\n and we removed any occurrence of ### within the\nemail.\nConditional generation is a problem where the content needs to be generated given\nsome kind of input. This includes paraphrasing, summarizing, entity extraction, product\ndescription writing given specifications, chatbots and many others. For this type of\nproblem we recommend:\nUse a separator at the end of the prompt, for example, \\n\\n###\\n\\n. Remember to\nalso append this separator when you eventually make requests to your model.\nUse an ending token at the end of the completion, for example, END.\nRemember to add the ending token as a stop sequence during inference, for\nexample, stop=[\" END\"].{ \n    \"prompt\" :\"Subject: <email_subject>\\nFrom:<customer_name>\\nDate:\n<date>\\nContent:<email_body>\\n\\n###\\n\\n\" , \"completion\" :\" \n<numerical_category>\"  \n} \n{ \n    \"prompt\" :\"Subject: Update my address\\nFrom:Joe", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "be812e625e62-1", "text": "{ \n    \"prompt\" :\"Subject: Update my address\\nFrom:Joe  \nDoe\\nTo:support@ourcompany.com\\nDate:2021-06-03\\nContent:Hi,\\nI would like  \nto update my billing address to match my delivery address.\\n\\nPlease let me  \nknow once done.\\n\\nThanks,\\nJoe\\n\\n###\\n\\n\" ,  \n    \"completion\" :\" 4\" \n} \nConditional generation", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3be17258ce6e-0", "text": "Aim for at least ~500 examples.\nEnsure that the prompt + completion doesn't exceed 2048 tokens, including the\nseparator.\nEnsure the examples are of high quality and follow the same desired format.\nEnsure that the dataset used for fine-tuning is similar in structure and type of task\nas what the model will be used for.\nUsing Lower learning rate and only 1-2 epochs tends to work better for these use\ncases.\nThis is a generative use case so you would want to ensure that the samples you provide\nare of the highest quality, as the fine-tuned model will try to imitate the style (and\nmistakes) of the given examples. A good starting point is around 500 examples. A\nsample dataset might look like this:\nJSON\nFor example:\nJSONCase study: Write an engaging ad based on a Wikipedia article\n{ \n    \"prompt\" :\"<Product Name>\\n<Wikipedia description>\\n\\n###\\n\\n\" ,  \n    \"completion\" :\" <engaging ad> END\"  \n} \n{ \n    \"prompt\" :\"Samsung Galaxy Feel\\nThe Samsung Galaxy Feel is an Android  \nsmartphone developed by Samsung Electronics exclusively for the Japanese  \nmarket. The phone was released in June 2017 and was sold by NTT Docomo. It  \nruns on Android 7.0 (Nougat), has a 4.7 inch display, and a 3000 mAh  \nbattery.\\nSoftware\\nSamsung Galaxy Feel runs on Android 7.0 (Nougat), but  \ncan be later updated to Android 8.0 (Oreo).\\nHardware\\nSamsung Galaxy Feel  \nhas a 4.7 inch Super AMOLED HD display, 16 MP back facing and 5 MP front", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3be17258ce6e-1", "text": "facing cameras. It has a 3000 mAh battery, a 1.6 GHz Octa-Core ARM Cortex-\nA53 CPU, and an ARM Mali-T830 MP1 700 MHz GPU. It comes with 32GB of  \ninternal storage, expandable to 256GB via microSD. Aside from its software  \nand hardware specifications, Samsung also introduced a unique a hole in the  \nphone's shell to accommodate the Japanese perceived penchant for  \npersonalizing their mobile phones. The Galaxy Feel's battery was also touted  \nas a major selling point since the market favors handsets with longer  \nbattery life. The device is also waterproof and supports 1seg digital  \nbroadcasts using an antenna that is sold separately.\\n\\n###\\n\\n\" ,  \n    \"completion\" :\"Looking for a smartphone that can do it all? Look no  \nfurther than Samsung Galaxy Feel! With a slim and sleek design, our latest  \nsmartphone features high-quality picture and video capabilities, as well as", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b586ce9a2820-0", "text": "Here we used a multiline separator, as Wikipedia articles contain multiple paragraphs\nand headings. W e also used a simple end token, to ensure that the model knows when\nthe completion should finish.\nThis is similar to a language transformation task. T o improve the performance, it's best\nto either sort different extracted entities alphabetically or in the same order as they\nappear in the original text. This helps the model to keep track of all the entities which\nneed to be generated in order. The dataset could look as follows:\nJSON\nFor example:\nJSON\nA multi-line separator works best, as the text will likely contain multiple lines. Ideally\nthere will be a high diversity of the types of input prompts (news articles, Wikipedia\npages, tweets, legal documents), which reflect the likely texts which will be encountered\nwhen extracting entities.\nA chatbot will normally contain relevant context about the conversation (order details),\nsummary of the conversation so far, and most recent messages. For this use case the\nsame past conversation can generate multiple rows in the dataset, each time with aan award winning battery life. END\"  \n} \nCase study: Entity extraction\n{ \n    \"prompt\" :\"<any text, for example news article>\\n\\n###\\n\\n\" ,  \n    \"completion\" :\" <list of entities, separated by a newline> END\"  \n} \n{ \n    \"prompt\" :\"Portugal will be removed from the UK's green travel list from  \nTuesday, amid rising coronavirus cases and concern over a \\\"Nepal mutation  \nof the so-called Indian variant\\\". It will join the amber list, meaning  \nholidaymakers should not visit and returnees must isolate for 10  \ndays...\\n\\n###\\n\\n\" ,  \n    \"completion\" :\" Portugal\\nUK\\nNepal mutation\\nIndian variant END\"  \n}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b586ce9a2820-1", "text": "} \nCase study: Customer support chatbot", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0528199701ed-0", "text": "slightly different context, for every agent generation as a completion. This use case\nrequires a few thousand examples, as it likely deals with different types of requests, and\ncustomer issues. T o ensure the performance is of high quality, we recommend vetting\nthe conversation samples to ensure the quality of agent messages. The summary can be\ngenerated with a separate text transformation fine tuned model. The dataset could look\nas follows:\nJSON\nHere we purposefully separated different types of input information, but maintained\nCustomer Agent dialog in the same format between a prompt and a completion. All the\ncompletions should only be by the agent, and we can use \\n as a stop sequence when\ndoing inference.\nHere it's important to convert the input data into a natural language, which will likely\nlead to superior performance. For example, the following format:\nJSON\nWon't work as well as:\nJSON{\"prompt\" :\"Summary: <summary of the interaction so far>\\n\\nSpecific  \ninformation:<for example order details in natural  \nlanguage>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer:  \n<message2>\\nAgent:\" , \"completion\" :\" <response2>\\n\" } \n{\"prompt\" :\"Summary: <summary of the interaction so far>\\n\\nSpecific  \ninformation:<for example order details in natural  \nlanguage>\\n\\n###\\n\\nCustomer: <message1>\\nAgent: <response1>\\nCustomer:  \n<message2>\\nAgent: <response2>\\nCustomer: <message3>\\nAgent:\" , \n\"completion\" :\" <response3>\\n\" } \nCase study: Product description based on a technical list of\nproperties\n{ \n    \"prompt\" :\"Item=handbag, Color=army_green, price=$99, size=S->\" ,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0528199701ed-1", "text": "\"completion\" :\"This stylish small green handbag will add a unique touch  \nto your look, without costing you a fortune.\"  \n} \n{ \n    \"prompt\" :\"Item is a handbag. Colour is army green. Price is midrange.  \nSize is small.->\" , \n    \"completion\" :\"This stylish small green handbag will add a unique touch", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b6348b04df7b-0", "text": "For high performance, ensure that the completions were based on the description\nprovided. If external content is often consulted, then adding such content in an\nautomated way would improve the performance. If the description is based on images,\nit may help to use an algorithm to extract a textual description of the image. Since\ncompletions are only one sentence long, we can use . as the stop sequence during\ninference.\nFor this type of problem we recommend:\nLeave the prompt empty.\nNo need for any separators.\nYou'll normally want a large number of examples, at least a few thousand.\nEnsure the examples cover the intended domain or the desired tone of voice.\nMany companies have a large amount of high quality content generated in a specific\nvoice. Ideally all generations from our API should follow that voice for the different use\ncases. Here we can use the trick of leaving the prompt empty, and feeding in all the\ndocuments which are good examples of the company voice. A fine-tuned model can be\nused to solve many different use cases with similar prompts to the ones used for base\nmodels, but the outputs are going to follow the company voice much more closely than\npreviously.\nJSON\nA similar technique could be used for creating a virtual character with a particular\npersonality, style of speech and topics the character talks about.\nGenerative tasks have a potential to leak training data when requesting completions\nfrom the model, so extra care needs to be taken that this is addressed appropriately. For\nexample personal or sensitive company information should be replaced by generic\ninformation or not be included into fine-tuning in the first place.to your look, without costing you a fortune.\"  \n} \nOpen ended generation\nCase study: Maintaining company voice\n{\"prompt\" :\"\", \"completion\" :\" <company voice textual content>\" }", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b6348b04df7b-1", "text": "{\"prompt\" :\"\", \"completion\" :\" <company voice textual content>\" } \n{\"prompt\" :\"\", \"completion\" :\" <company voice textual content2>\" }", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1adefca79fd1-0", "text": "Fine tune your model with our How-to guide\nLearn more about the underlying models that power Azure OpenAI ServiceNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cff3ad4d8d9c-0", "text": "Learn how to customize a model for\nyour application\nArticle \u202204/05/2023\nAzure OpenAI Service lets you tailor our models to your personal datasets using a\nprocess known as fine-tuning . This customization step will let you get more out of the\nservice by providing:\nHigher quality results than what you can get just from prompt design\nThe ability to train on more examples than can fit into a prompt\nLower-latency requests\nA customized model improves on the few-shot learning approach by training the\nmodel's weights on your specific prompts and structure. The customized model lets you\nachieve better results on a wider number of tasks without needing to provide examples\nin your prompt. The result is less text sent and fewer tokens processed on every API call,\nsaving cost and improving request latency.\nAn Azure subscription - Create one for free\nAccess granted to Azure OpenAI in the desired Azure subscription\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nAn Azure OpenAI resource\nFor more information about creating a resource, see Create a resource and deploy\na model using Azure OpenAI .\uff17 Note\nThere is a breaking change in the create fine tunes command in the latest 12-01-\n2022 GA API. For the latest command syntax consult the reference documentation\nPrerequisites\nFine-tuning  workflow", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4946b9e0a2bc-0", "text": "The fine-tuning workflow in Azure OpenAI S tudio requires the following steps:\n1. Prepare your training and validation data\n2. Use the Create cust omized model  wizard in Azure OpenAI S tudio to train your\ncustomized model\na. Select a base model\nb. Choose your training data\nc. Optionally, choose your validation data\nd. Optionally, choose advanced options  for your fine-tune job\ne. Review your choices and train your new customized model\n3. Check the status of your customized model\n4. Deploy your customized model for use\n5. Use your customized model\n6. Optionally, analyze your customized model for performance and fit\nYour training data and validation data sets consist of input & output examples for how\nyou would like the model to perform.\nThe training and validation data you use must  be formatted as a JSON Lines (JSONL)\ndocument in which each line represents a single prompt-completion pair. The OpenAI\ncommand-line interface (CLI) includes a data preparation tool  that validates, gives\nsuggestions, and reformats your training data into a JSONL file ready for fine-tuning.\nHere's an example of the training data format:\nJSON\nIn addition to the JSONL format, training and validation data files must be encoded in\nUTF-8 and include a byte-order mark (BOM), and the file must be less than 200 MB in\nsize. For more information about formatting your training data, see Learn how to\nprepare your dataset for fine-tuning .\nDesigning your prompts and completions for fine-tuning is different from designing\nyour prompts for use with any of our GPT-3 base models . Prompts for completion callsPrepare your training and validation data\n{\"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" }", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4946b9e0a2bc-1", "text": "{\"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } \n{\"prompt\" : \"<prompt text>\" , \"completion\" : \"<ideal generated text>\" } \nCreating your training and validation datasets", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "77a80dda0b5f-0", "text": "often use either detailed instructions or few-shot learning techniques, and consist of\nmultiple examples. For fine-tuning, we recommend that each training example consists\nof a single input prompt and its desired completion output. Y ou don't need to give\ndetailed instructions or multiple completion examples for the same prompt.\nThe more training examples you have, the better. W e recommend having at least 200\ntraining examples. In general, we've found that each doubling of the dataset size leads\nto a linear increase in model quality.\nFor more information about preparing training data for various tasks, see Learn how to\nprepare your dataset for fine-tuning .\nWe recommend using OpenAI's command-line interface (CLI) to assist with many of the\ndata preparation steps. OpenAI has developed a tool that validates, gives suggestions,\nand reformats your data into a JSONL file ready for fine-tuning.\nTo install the CLI, run the following Python command:\nConsole\nTo analyze your training data with the data preparation tool, run the following Python\ncommand, replacing <LOCAL_FILE> with the full path and file name of the training data\nfile to be analyzed:\nConsole\nThis tool accepts files in the following data formats, if they contain a prompt and a\ncompletion column/key:\nComma-separated values (CSV)\nTab-separated values (T SV)\nMicrosoft Excel workbook (XLSX)\nJavaScript Object Notation (JSON)\nJSON Lines (JSONL)\nThe tool reformats your training data and saves output into a JSONL file ready for fine-\ntuning, after guiding you through the process of implementing suggested changes.OpenAI CLI data preparation tool\npip install --upgrade openai  \nopenai tools fine_tunes.prepare_data -f <LOCAL_FILE>", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6bd788bed2fb-0", "text": "Azure OpenAI S tudio provides the Create cust omized model  wizard, so you can\ninteractively create and train a fine-tuned model for your Azure resource.\nNavigate to the Azure OpenAI S tudio at https://oai.azure.com/ and sign in with\ncredentials that have access to your Azure OpenAI resource. During the sign-in\nworkflow, select the appropriate directory, Azure subscription, and Azure OpenAI\nresource.\nYou'll first land on our main page for Azure OpenAI S tudio. From here, you can start\nfine-tuning a custom model.\nSelect the Start fine-tuning a cust om model  button under Manage deployments and\nmodels  section of the landing page, highlighted in the following picture, to start fine-\ntuning a custom model.Use the Create customized model wizard\nGo to the Azure OpenAI Studio\nLanding page\n\uff17 Note\nIf your resource doesn't have a model already deployed in it, a warning is\ndisplayed. Y ou can ignore that warning for the purposes of fine-tuning a model,\nbecause you'll be fine-tuning and deploying a new customized model.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9dca1905ab1a-0", "text": "To create a customized model, select the Create cust omized model  button under the\nProvided models  section on the Models  page, highlighted in the following picture, to\nstart the Create cust omized model  wizard.\n\uf80a\nStart the wizard from the Models page\n\uf80a\nSelect a base model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c179ed5e68c6-0", "text": "The first step in creating a customized model is to choose a base model. The Base\nmodel  pane lets you choose a base model to use for your customized model, and the\nchoice influences both the performance and the cost of your model. Y ou can create a\ncustomized model from one of the following available base models:\nada\nbabbage\ncurie\ncode-cushman-001*\ndavinci*\n* currently unavailable for new customers.\nFor more information about our base models that can be fine-tuned, see Models . Select\na base model from the Base model type  dropdown, as shown in the following picture,\nand then select Next  to continue.\n\uf80a\nChoose your training data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "83f53421a4d5-0", "text": "The next step is to either choose existing prepared training data or upload new\nprepared training data to use when customizing your model. The Training data  pane,\nshown in the following picture, displays any existing, previously uploaded datasets and\nprovides options by which you can upload new training data.\nIf your training data has already been uploaded to the service, select Choose dataset ,\nand then select the file from the list shown in the Training data  pane. Otherwise, select\neither Local file  to upload training data from a local file , or Azur e blob or other shar ed\nweb locations  to import training data from Azure Blob or another shared web location .\nFor large data files, we recommend you import from an Azure Blob store. Large files can\nbecome unstable when uploaded through multipart forms because the requests are\natomic and can't be retried or resumed. For more information about Azure Blob storage,\nsee What is Azure Blob storage?\n\uf80a\n\uff17 Note\nTraining data files must be formatted as JSONL files, encoded in UTF-8 with a byte-\norder mark (BOM), and less than 200 MB in size.\nTo upload training data from a local file", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "14f823f7b911-0", "text": "You can upload a new training dataset to the service from a local file by using one of the\nfollowing methods:\nDrag and drop the file into the client area of the Training data  pane, and then\nselect Uplo ad file\nSelect Browse for a file  from the client area of the Training data  pane, choose the\nfile to upload from the Open  dialog, and then select Uplo ad file .\nAfter you've selected and uploaded the training dataset, select Next  to optionally\nchoose your validation data .\nYou can import a training dataset from Azure Blob or another shared web location by\nproviding the name and location of the file, as shown in the following picture. Enter the\nname of the file in File name  and the Azure Blob URL, Azure S torage shared access\nsignature (SAS), or other link to an accessible shared web location that contains the file\nin File location , then select Uplo ad file  to import the training dataset to the service.\nAfter you've selected and uploaded the training dataset, select Next  to optionally\nchoose your validation data .\n\uf80a\nTo import training data from an Azure Blob store", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bc1bc373a99f-0", "text": "You can now choose to optionally use validation data in the training process of your\nfine-tuned model. If you don't want to use validation data, you can choose Next  to\nchoose advanced options for your model. Otherwise, if you have a validation dataset,\nyou can either choose existing prepared validation data or upload new prepared\nvalidation data to use when customizing your model. The Validation data  pane, shown\nin the following picture, displays any existing, previously uploaded training and\nvalidation datasets and provides options by which you can upload new validation data.\n\uf80a\nChoose your validation data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a1e5d5d74f55-0", "text": "If your validation data has already been uploaded to the service, select Choose dataset ,\nand then select the file from the list shown in the Validation data  pane. Otherwise,\nselect either Local file  to upload validation data from a local file , or Azur e blob or other\nshared w eb locations  to import validation data from Azure Blob or another shared web\nlocation .\nFor large data files, we recommend you import from an Azure Blob store. Large files can\nbecome unstable when uploaded through multipart forms because the requests are\natomic and can't be retried or resumed.\nYou can upload a new validation dataset to the service from a local file by using one of\nthe following methods:\n\uf80a\n\uff17 Note\nLike training data files, validation data files must be formatted as JSONL files,\nencoded in UTF-8 with a byte-order mark (BOM), and less than 200 MB in size.\nTo upload validation data from a local file", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1fc4b1cd5ea8-0", "text": "Drag and drop the file into the client area of the Validation data  pane, and then\nselect Uplo ad file\nSelect Browse for a file  from the client area of the Validation data  pane, choose\nthe file to upload from the Open  dialog, and then select Uplo ad file .\nAfter you've uploaded the validation dataset, select Next  to optionally choose advanced\noptions .\nYou can import a validation dataset from Azure Blob or another shared web location by\nproviding the name and location of the file, as shown in the following picture. Enter the\nname of the file in File name  and the Azure Blob URL, Azure S torage shared access\nsignature (SAS), or other link to an accessible shared web location that contains the file\nin File location , then select Uplo ad file  to import the validation dataset to the service.\nAfter you've imported the validation dataset, select Next  to optionally choose advanced\noptions .\n\uf80a\nTo import validation data from an Azure Blob store", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "743faa797295-0", "text": "You can either use default values for the hyperparameters of the fine-tune job that the\nwizard runs to train your fine-tuned model, or you can adjust those hyperparameters for\nyour customization needs in the Advanced options  pane, shown in the following\npicture.\n\uf80a\nChoose advanced options", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "14063cd6ce0e-0", "text": "Either select Default  to use the default values for the fine-tune job, or select Advanced\nto display and edit the hyperparameter values, as shown in the following picture.\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d242326ae361-0", "text": "The following hyperparameters are available:\nParamet er\nnameDescr iption\nNumber\nof epochsThe number of epochs to train the model for. An epoch refers to one full cycle\nthrough the training dataset.\nBatch size The batch size to use for training. The batch size is the number of training examples\nused to train a single forward and backward pass.\nLearning\nrate\nmultiplierThe learning rate multiplier to use for training. The fine-tuning learning rate is the\noriginal learning rate used for pre-training, multiplied by this value.\nPrompt\nloss\nweightThe weight to use for loss on the prompt tokens. This value controls how much the\nmodel tries to learn to generate the prompt (as compared to the completion, which\nalways has a weight of 1.0.) Increasing this value can add a stabilizing effect to\ntraining when completions are short.\nFor more information about these hyperparameters, see the Create a Fine tune job\nsection of the REST API  documentation.\nAfter you've chosen either default or advanced options, select Next  to review your\nchoices and train your fine-tuned model .\nThe Review and train  pane of the wizard displays information about the choices you've\nmade in the Create cust omized model  wizard for your fine-tuned model, as shown in\nthe following picture.Review your choices and train your model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2e166c7d3d3f-0", "text": "If you're ready to train your model, select Save and close  to start the fine-tune job and\nreturn to the Models  page .\nThe Models  page displays information about your customized model in the Customized\nmodels  tab, as shown in the following picture. The tab includes information about the\nstatus and job ID of the fine-tune job for your customized model. When the job is\ncompleted, the file ID of the result file is also displayed.\n\uf80a\nCheck the status of your customized model\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1d331cbd21d8-0", "text": "After you've started a fine-tune job, it may take some time to complete. Y our job may be\nqueued behind other jobs on our system, and training your model can take minutes or\nhours depending on the model and dataset size. Y ou can check the status of the fine-\ntune job for your customized model in the Status  column of the Customized models\ntab on the Models  page, and you can select Refresh to update the information on that\npage.\nYou can also select the name of the model from the Model name  column of the Models\npage to display more information about your customized model, including the status of\nthe fine-tune job, training results, training events, and hyperparameters used in the job.\nYou can select the Refresh button to refresh the information for your model, as shown in\nthe following picture.\nFrom the model page, you can also select Downlo ad training file  to download the\ntraining data you used for the model, or select Downlo ad results  to download the result\nfile attached to the fine-tune job for your model and analyze your customized model  for\ntraining and validation performance.\nWhen the fine-tune job has succeeded, you can deploy the customized model from the\nModels  pane. Y ou must deploy your customized model to make it available for use with\ncompletion calls.\n\uf80a\nDeploy a customized model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d9a995f4722f-0", "text": "To deploy your customized model, select the customized model to be deployed and\nthen select Deploy model , as shown in the following picture.\nThe Deploy model  dialog is presented, in which you can provide a name for the\ndeployment of your customized model. Enter a name in Deployment name  and then\nselect Create to start the deployment of your customized model.\uff09 Impor tant\nAfter a customized model is deployed, if at any time the deployment remains\ninactive for greater than fifteen (15) days, the deployment will automatically be\ndeleted. The deployment of a customized model is \u201cinactive\u201d if the model was\ndeployed more than fifteen (15) days ago and no completions or chat completions\ncalls were made to it during a continuous 15-day period. The deletion of an inactive\ndeployment does NOT delete or affect the underlying customized model, and the\ncustomized model can be redeployed at any time. As described in Azur e OpenAI\nService pricing , each customized (fine-tuned) model that is deployed incurs an\nhourly hosting cost regardless of whether completions or chat completions calls are\nbeing made to the model. T o learn more about planning and managing costs with\nAzure OpenAI, refer to our cost management guide .\n\uff17 Note\nOnly one deployment is permitted for a customized model. An error message is\ndisplayed if you select an already-deployed customized model.\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "94986851728e-0", "text": "You can monitor the progress of your deployment from the Deployments  pane of Azure\nOpenAI S tudio.\nOnce your customized model has been deployed, you can use it like any other deployed\nmodel. For example, you can use the Playground  pane of Azure OpenAI S tudio to\nexperiment with your new deployment, as shown in the following picture. Y ou can\ncontinue to use the same parameters with your customized model, such as temperature\nand frequency penalty, as you can with other deployed models.\n\uf80a\nUse a deployed customized model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d6862956340c-0", "text": "Azure OpenAI attaches a result file, named results.csv, to each fine-tune job once it's\ncompleted. Y ou can use the result file to analyze the training and validation performance\nof your customized model. The file ID for the result file is listed for each customized\nmodel in the Result file Id  column of the Models  pane for Azure OpenAI S tudio. Y ou can\nuse the file ID to identify and download the result file from the File Management  pane\nof Azure OpenAI S tudio.\nThe result file is a CSV file containing a header row and a row for each training step\nperformed by the fine-tune job. The result file contains the following columns:\nColumn name Descr iption\nstep The number of the training step. A training step represents a\nsingle pass, forward and backward, on a batch of training data.\nelapsed_tokens The number of tokens the customized model has seen so far,\nincluding repeats.\n\uf80a\n\uff17 Note\nAs with all applications, we require a review process prior to going live.\nAnalyze your customized model", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0b082a692371-0", "text": "Column name Descr iption\nelapsed_examples The number of examples the model has seen so far, including\nrepeats.  \nEach example represents one element in that step's batch of\ntraining data. For example, if the Batch size  parameter is set to\n32 in the Advanced options  pane , this value increments by 32 in\neach training step.\ntraining_loss The loss for the training batch.\ntraining_sequence_accuracy The percentage of completions in the training batch for which\nthe model's predicted tokens exactly matched the true\ncompletion tokens.  \nFor example, if the batch size is set to 3 and your data contains\ncompletions [[1, 2], [0, 5], [4, 2]], this value is set to 0.67\n(2 of 3) if the model predicted [[1, 1], [0, 5], [4, 2]].\ntraining_token_accuracy The percentage of tokens in the training batch that were\ncorrectly predicted by the model.  \nFor example, if the batch size is set to 3 and your data contains\ncompletions [[1, 2], [0, 5], [4, 2]], this value is set to 0.83\n(5 of 6) if the model predicted [[1, 1], [0, 5], [4, 2]].\nvalidation_loss The loss for the validation batch.\nvalidation_sequence_accuracyThe percentage of completions in the validation batch for which\nthe model's predicted tokens exactly matched the true\ncompletion tokens.  \nFor example, if the batch size is set to 3 and your data contains\ncompletions [[1, 2], [0, 5], [4, 2]], this value is set to 0.67", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0b082a692371-1", "text": "(2 of 3) if the model predicted [[1, 1], [0, 5], [4, 2]].\nvalidation_token_accuracy The percentage of tokens in the validation batch that were\ncorrectly predicted by the model.  \nFor example, if the batch size is set to 3 and your data contains\ncompletions [[1, 2], [0, 5], [4, 2]], this value is set to 0.83\n(5 of 6) if the model predicted [[1, 1], [0, 5], [4, 2]].\nWhen you're done with your customized model, you can delete the deployment and\nmodel. Y ou can also delete the training and validation files you uploaded to the service,\nif needed.Clean up your deployments, customized\nmodels, and training  files", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c0c661a3769b-0", "text": "You can delete the deployment for your customized model from the Deployments  page\nfor Azure OpenAI S tudio. Select the deployment to delete, and then select Delet e to\ndelete the deployment.\nYou can delete a customized model from the Models  page for Azure OpenAI S tudio.\nSelect the customized model to delete from the Customized models  tab, and then\nselect Delet e to delete the customized model.\nYou can optionally delete training and validation files you've uploaded for training, and\nresult files generated during training, from the File Management  page for Azure OpenAI\nStudio. Select the file to delete, and then select Delet e to delete the file.Delete your model deployment\n\uff09 Impor tant\nAfter a customized model is deployed, if at any time the deployment remains\ninactive for greater than fifteen (15) days, the deployment will automatically be\ndeleted. The deployment of a customized model is \u201cinactive\u201d if the model was\ndeployed more than fifteen (15) days ago and no completions or chat completions\ncalls were made to it during a continuous 15-day period. The deletion of an inactive\ndeployment does NOT delete or affect the underlying customized model, and the\ncustomized model can be redeployed at any time. As described in Azur e OpenAI\nService pricing , each customized (fine-tuned) model that is deployed incurs an\nhourly hosting cost regardless of whether completions or chat completions calls are\nbeing made to the model. T o learn more about planning and managing costs with\nAzure OpenAI, refer to our cost management guide .\nDelete your customized model\n\uff17 Note\nYou cannot delete a customized model if it has an existing deployment. Y ou must\nfirst delet e your model deployment  before you can delete your customized model.\nDelete your training files\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0a9c2a632987-0", "text": "Explore the full REST API R eference documentation to learn more about all the\nfine-tuning capabilities. Y ou can find the full REST documentation here .\nExplore more of the Python SDK operations here .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e02b941ef692-0", "text": "How to configure Azure OpenAI Service\nwith managed identities\nArticle \u202205/10/2023\nMore complex security scenarios require Azure role-based access control (Azure RBA C).\nThis document covers how to authenticate to your OpenAI resource using Azure Active\nDirectory (Azure AD).\nIn the following sections, you'll use the Azure CLI to assign roles, and obtain a bearer\ntoken to call the OpenAI resource. If you get stuck, links are provided in each section\nwith all available options for each command in Azure Cloud Shell/Azure CLI.\nAn Azure subscription - Create one for free\nAccess granted to the Azure OpenAI service in the desired Azure subscription\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nAzure CLI - Installation Guide\nThe following Python libraries: os, requests, json\nTo sign-in to the Azure CLI, run the following command and complete the sign-in. Y ou\nmay need to do it again if your session has been idle for too long.\nAzure CLIPrerequisites\nSign into the Azure CLI\naz login  \nAssign yourself to the Cognitive Services User\nrole", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6ae3954a16ac-0", "text": "Assigning yourself to the Cognitive Services User role will allow you to use your account\nfor access to the specific cognitive services resource\n1. Get your user information\nAzure CLI\n2. Assign yourself to \u201cCognitive Services User\u201d role.\nAzure CLI\n3. Acquire an Azure AD access token. Access tokens expire in one hour. you'll then\nneed to acquire another one.\nAzure CLI\n4. Make an API call Use the access token to authorize your API call by setting the\nAuthorization header value.\nBashexport user= $(az account show -o json | jq -r .user.name)  \nexport resourceId= $(az group show -g $myResourceGroupName  -o json | jq  \n-r .id) \naz role assignment create --role \"Cognitive Services User\"  --assignee  \n$user --scope $resourceId  \n\uff17 Note\nRole assignment change will take ~5 mins to become effective.\nexport accessToken= $(az account get -access-token  --resource  \nhttps://cognitiveservices.azure.com -o json | jq -r .accessToken)  \ncurl ${endpoint%/} /openai/deployments/YOUR_DEPLOYMENT_NAME/completions?\napi-version=2023-05-15 \\  \n-H \"Content-Type: application/json\"  \\ \n-H \"Authorization: Bearer $accessToken \" \\ \n-d '{ \"prompt\": \"Once upon a time\" }'  \nAuth orize access to managed identitie s", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "55de823fdf0d-0", "text": "OpenAI supports Azure Active Directory (Azure AD) authentication with managed\nidentities for Azure resources . Managed identities for Azure resources can authorize\naccess to Cognitive Services resources using Azure AD credentials from applications\nrunning in Azure virtual machines (VMs), function apps, virtual machine scale sets, and\nother services. By using managed identities for Azure resources together with Azure AD\nauthentication, you can avoid storing credentials with your applications that run in the\ncloud.\nBefore you can use managed identities for Azure resources to authorize access to\nCognitive Services resources from your VM, you must enable managed identities for\nAzure resources on the VM. T o learn how to enable managed identities for Azure\nResources, see:\nAzure portal\nAzure P owerShell\nAzure CLI\nAzure R esource Manager template\nAzure R esource Manager client libraries\nFor more information about managed identities, see Managed identities for Azure\nresources .Enable managed identitie s on a VM", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "07d3a1cb6d52-0", "text": "Manage Azu re OpenAI Service quota\nArticle \u202206/09/2023\nQuota provides the flexibility to actively manage the allocation of rate limits across the\ndeployments within your subscription. This article walks through the process of\nmanaging your Azure OpenAI quota.\nAzure OpenAI's quota feature enables assignment of rate limits to your deployments,\nup-to a global limit called your \u201cquota.\u201d Quota is assigned to your subscription on a per-\nregion, per-model basis in units of Tokens-per -Minut e (TPM) . When you onboard a\nsubscription to Azure OpenAI, you'll receive default quota for most available models.\nThen, you'll assign TPM to each deployment as it is created, and the available quota for\nthat model will be reduced by that amount. Y ou can continue to create deployments\nand assign them TPM until you reach your quota limit. Once that happens, you can only\ncreate new deployments of that model by reducing the TPM assigned to other\ndeployments of the same model (thus freeing TPM for use), or by requesting and being\napproved for a model quota increase in the desired region.\nWhen a deployment is created, the assigned TPM will directly map to the tokens-per-\nminute rate limit enforced on its inferencing requests. A Requests-P er-Minut e (RPM)\nrate limit will also be enforced whose value is set proportionally to the TPM assignment\nusing the following ratio:\n6 RPM per 1000 TPM.\nThe flexibility to distribute TPM globally within a subscription and region has allowed\nAzure OpenAI Service to loosen other restrictions:\nThe maximum resources per region are increased to 30.\nThe limit on creating no more than one deployment of the same model in a\nresource has been removed.Introduction to quota\n\uff17 Note\nWith a quota of 240,000 TPM for GPT-35-Turbo in East US, a customer can create a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "07d3a1cb6d52-1", "text": "single deployment of 240K TPM, 2 deployments of 120K TPM each, or any number\nof deployments in one or multiple Azure OpenAI resources as long as their TPM\nadds up to less than 240K total in that region.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "60cbdb77c32a-0", "text": "When you create a model deployment, you have the option to assign T okens-P er-\nMinute (TPM) to that deployment. TPM can be modified in increments of 1,000, and will\nmap to the TPM and RPM rate limits enforced on your deployment, as discussed above.\nTo create a new deployment from within the Azure AI S tudio under Management  select\nDeployments  > Create new deployment .\nThe option to set the TPM is under the Advanced options  drop-down:\nPost deployment you can adjust your TPM allocation by selecting Edit deployment\nunder Management  > Deployments  in Azure AI S tudio. Y ou can also modify this\nselection within the new quota management experience under Management  > Quotas .Assign quota\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2e8513eabf88-0", "text": "Different model deployments, also called model classes have unique max TPM values\nthat you're now able to control. This r epresents the maximum amount o f TPM that can\nbe allocat ed to that type o f model deployment in a giv en region.  While each model\ntype represents its own unique model class, the max TPM value is currently only\ndifferent for certain model classes:\nGPT-4\nGPT-4-32K\nText-Davinci-003\nAll other model classes have a common max TPM value.\nFor an all up view of your quota allocations across deployments in a given region, select\nManagement  > Quota  in Azure AI S tudio:\uff09 Impor tant\nQuotas and limits are subject to change, for the most up-date-information consult\nour quotas and limits ar ticle.\nModel specific setting s\n\uff17 Note\nQuota T okens-P er-Minute (TPM) allocation is not related to the max input token\nlimit of a model. Model input token limits are defined in the models table  and are\nnot impacted by changes made to TPM.\nView and request quota", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "61aa985e5aac-0", "text": "Quota Name : There's one quota value per region for each model type. The quota\ncovers all versions of that model. The quota name can be expanded in the UI to\nshow the deployments that are using the quota.\nDeployment : Model deployments divided by model class.\nUsage/Limit : For the quota name, this shows how much quota is used by\ndeployments and the total quota approved for this subscription and region. This\namount of quota used is also represented in the bar graph.\nRequest Quota : The icon in this field navigates to a form where requests to\nincrease quota can be submitted.\nAs part of the transition to the new quota system and TPM based allocation, all existing\nAzure OpenAI model deployments have been automatically migrated to use quota. In\ncases where the existing TPM/RPM allocation exceeds the default values due to previous\ncustom rate-limit increases, equivalent TPM were assigned to the impacted\ndeployments.\n\uf80a\nMigrating existing deployments\nUnderstanding rate limits", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a354765ed0a5-0", "text": "Assigning TPM to a deployment sets the T okens-P er-Minute (TPM) and R equests-P er-\nMinute (RPM) rate limits for the deployment, as described above. TPM rate limits are\nbased on the maximum number of tokens that are estimated to be processed by a\nrequest at the time the request is received. It isn't the same as the token count used for\nbilling, which is computed after all processing is completed.\nAs each request is received, Azure OpenAI computes an estimated max processed-token\ncount that includes the following:\nPrompt text and count\nThe max_tokens parameter setting\nThe best_of parameter setting\nAs requests come into the deployment endpoint, the estimated max-processed-token\ncount is added to a running token count of all requests that is reset each minute. If at\nany time during that minute, the TPM rate limit value is reached, then further requests\nwill receive a 429 response code until the counter resets.\nRPM rate limits are based on the number of requests received over time. The rate limit\nexpects that requests be evenly distributed over a one-minute period. If this average\nflow isn't maintained, then requests may receive a 429 response even though the limit\nisn't met when measured over the course of a minute. T o implement this behavior, Azure\nOpenAI Service evaluates the rate of incoming requests over a small period of time,\ntypically 1 or 10 seconds. If the number of requests received during that time exceeds\nwhat would be expected at the set RPM limit, then new requests will receive a 429\nresponse code until the next evaluation period. For example, if Azure OpenAI is\nmonitoring request rate on 1-second intervals, then rate limiting will occur for a 600-\nRPM deployment if more than 10 requests are received during each 1-second period\n(600 requests per minute = 10 requests per second).", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a354765ed0a5-1", "text": "(600 requests per minute = 10 requests per second).\nTo minimize issues related to rate limits, it's a good idea to use the following techniques:\nSet max_tokens and best_of to the minimum values that serve the needs of your\nscenario. For example, don\u2019t set a large max-tokens value if you expect your\nresponses to be small.\nUse quota management to increase TPM on deployments with high traffic, and to\nreduce TPM on deployments with limited needs.\nImplement retry logic in your application.\nAvoid sharp changes in the workload. Increase the workload gradually.\nTest different load increase patterns.Rate limit best practices", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "757647ba0810-0", "text": "To review quota defaults for Azure OpenAI, consult the quotas & limits articleNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b6f6c4af577b-0", "text": "Configure Azu re Cognitive Services\nvirtual networks\nArticle \u202207/04/2023\nAzure Cognitive Services provides a layered security model. This model enables you to\nsecure your Cognitive Services accounts to a specific subset of networks . When network\nrules are configured, only applications requesting data over the specified set of\nnetworks can access the account. Y ou can limit access to your resources with request\nfiltering. Allowing only requests originating from specified IP addresses, IP ranges or\nfrom a list of subnets in Azure Virtual Networks .\nAn application that accesses a Cognitive Services resource when network rules are in\neffect requires authorization. Authorization is supported with Azure Active Directory\n(Azure AD) credentials or with a valid API key.\n\uff09 Impor tant\nTurning on firewall rules for your Cognitive Services account blocks incoming\nrequests for data by default. In order to allow requests through, one of the\nfollowing conditions needs to be met:\nThe request should originate from a service operating within an Azure Virtual\nNetwork (VNet) on the allowed subnet list of the target Cognitive Services\naccount. The endpoint in requests originated from VNet needs to be set as\nthe custom subdomain  of your Cognitive Services account.\nOr the request should originate from an allowed list of IP addresses.\nRequests that are blocked include those from other Azure services, from the Azure\nportal, from logging and metrics services, and so on.\n\uff17 Note\nWe recommend that you use the Azure Az P owerShell module to interact with\nAzure. See Install Azur e PowerShell  to get started. T o learn how to migrate to the\nAz PowerShell module, see Migrat e Azur e PowerShell fr om Azur eRM t o Az.\nScenarios", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "856904aa9498-0", "text": "To secure your Cognitive Services resource, you should first configure a rule to deny\naccess to traffic from all networks (including internet traffic) by default. Then, you should\nconfigure rules that grant access to traffic from specific VNets. This configuration\nenables you to build a secure network boundary for your applications. Y ou can also\nconfigure rules to grant access to traffic from select public internet IP address ranges,\nenabling connections from specific internet or on-premises clients.\nNetwork rules are enforced on all network protocols to Azure Cognitive Services,\nincluding REST and W ebSocket. T o access data using tools such as the Azure test\nconsoles, explicit network rules must be configured. Y ou can apply network rules to\nexisting Cognitive Services resources, or when you create new Cognitive Services\nresources. Once network rules are applied, they're enforced for all requests.\nVirtual networks (VNET s) are supported in regions where Cognitive Services are\navailable . Cognitive Services supports service tags for network rules configuration. The\nservices listed below are included in the Cognitiv eServicesManagement  service tag.Supported regions and service offerings\nAnomaly Detector\uff02\nAzure OpenAI\uff02\nComputer Vision\uff02\nContent Moderator\uff02\nCustom Vision\uff02\nFace\uff02\nLanguage Understanding (L UIS) \uff02\nPersonalizer\uff02\nSpeech service\uff02\nLanguage service\uff02\nQnA Maker\uff02\nTranslator T ext\uff02\n\uff17 Note\nIf you're using, Azure OpenAI, L UIS, Speech Services, or Language services, the\nCognitiv eServicesManagement  tag only enables you use the service using the SDK\nor REST API. T o access and use Azure OpenAI S tudio, L UIS portal , Speech S tudio or\nLanguage S tudio from a virtual network, you will need to use the following tags:\nAzur eActiv eDirectory", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "856904aa9498-1", "text": "Azur eActiv eDirectory\nAzur eFrontDoor .Frontend", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1be211fee514-0", "text": "By default, Cognitive Services resources accept connections from clients on any network.\nTo limit access to selected networks, you must first change the default action.\nYou can manage default network access rules for Cognitive Services resources through\nthe Azure portal, P owerShell, or the Azure CLI.\n1. Go to the Cognitive Services resource you want to secure.\n2. Select the RESOUR CE MANA GEMENT  menu called Virtual netw ork.Azur eResour ceManager\nCognitiv eServicesManagement\nCognitiv eServicesFr ontEnd\nChange the default network access rule\n\uff12 Warning\nMaking changes to network rules can impact your applications' ability to connect\nto Azure Cognitive Services. Setting the default network rule to deny  blocks all\naccess to the data unless specific network rules that grant  access are also applied.\nBe sure to grant access to any allowed networks using network rules before you\nchange the default rule to deny access. If you are allow listing IP addresses for your\non-premises network, be sure to add all possible outgoing public IP addresses from\nyour on-premises network.\nManaging default network access rules\nAzure portal", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1177ee29d78e-0", "text": "3. To deny access by default, choose to allow access from Select ed netw orks.\nWith the Select ed netw orks setting alone, unaccompanied by configured\nVirtual netw orks or Addr ess ranges  - all access is effectively denied. When all\naccess is denied, requests attempting to consume the Cognitive Services\nresource aren't permitted. The Azure portal, Azure P owerShell or, Azure CLI\ncan still be used to configure the Cognitive Services resource.\n4. To allow traffic from all networks, choose to allow access from All netw orks.\n5. Select Save to apply your changes.\nYou can configure Cognitive Services resources to allow access only from specific\nsubnets. The allowed subnets may belong to a VNet in the same subscription, or in aGrant access from a virtual network", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "df4044783446-0", "text": "different subscription, including subscriptions belonging to a different Azure Active\nDirectory tenant.\nEnable a service endpoint  for Azure Cognitive Services within the VNet. The service\nendpoint routes traffic from the VNet through an optimal path to the Azure Cognitive\nServices service. The identities of the subnet and the virtual network are also transmitted\nwith each request. Administrators can then configure network rules for the Cognitive\nServices resource that allow requests to be received from specific subnets in a VNet.\nClients granted access via these network rules must continue to meet the authorization\nrequirements of the Cognitive Services resource to access the data.\nEach Cognitive Services resource supports up to 100 virtual network rules, which may be\ncombined with IP network rules .\nTo apply a virtual network rule to a Cognitive Services resource, the user must have the\nappropriate permissions for the subnets being added. The required permission is the\ndefault Contributor role, or the Cognitiv e Ser vices Contributor role. R equired permissions\ncan also be added to custom role definitions.\nCognitive Services resource and the virtual networks granted access may be in different\nsubscriptions, including subscriptions that are a part of a different Azure AD tenant.\nYou can manage virtual network rules for Cognitive Services resources through the\nAzure portal, P owerShell, or the Azure CLI.\n1. Go to the Cognitive Services resource you want to secure.\n2. Select the RESOUR CE MANA GEMENT  menu called Virtual netw ork.Required permissions\n\uff17 Note\nConfiguration of rules that grant access to subnets in virtual networks that are a\npart of a different Azure Active Directory tenant are currently only supported\nthrough P owerShell, CLI and REST APIs. Such rules cannot be configured through\nthe Azure portal, though they may be viewed in the portal.\nManaging virtual network rules\nAzure portal", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "49546dbcdaef-0", "text": "3. Check that you've selected to allow access from Select ed netw orks.\n4. To grant access to a virtual network with an existing network rule, under\nVirtual netw orks, select Add existing vir tual netw ork.\n5. Select the Virtual netw orks and Subnets  options, and then select Enable .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2a1ad4dbdc4e-0", "text": "6. To create a new virtual network and grant it access, select Add new vir tual\nnetw ork.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2808b731701f-0", "text": "7. Provide the information necessary to create the new virtual network, and then\nselect Create.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bdda12f8d4f8-0", "text": "\uff17 Note\nIf a service endpoint for Azure Cognitive Services wasn't previously\nconfigured for the selected virtual network and subnets, you can\nconfigure it as part of this operation.\nPresently, only virtual networks belonging to the same Azure Active\nDirectory tenant are shown for selection during rule creation. T o grant\naccess to a subnet in a virtual network belonging to another tenant,\nplease use P owerShell, CLI or REST APIs.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9b23fce85e00-0", "text": "8. To remove a virtual network or subnet rule, select ... to open the context menu\nfor the virtual network or subnet, and select Remov e.\n9. Select Save to apply your changes.\nYou can configure Cognitive Services resources to allow access from specific public\ninternet IP address ranges. This configuration grants access to specific services and on-\npremises networks, effectively blocking general internet traffic.\nProvide allowed internet address ranges using CIDR notation  in the form\n16.17.18.0/24 or as individual IP addresses like 16.17.18.19.\uff09 Impor tant\nBe sure to set the default rule  to deny , or network rules have no effect.\nGrant access from an internet IP range\n\uea80 Tip\nSmall address ranges using \"/31\" or \"/32\" prefix sizes are not supported. These\nranges should be configured using individual IP address rules.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "07d818880cbe-0", "text": "IP network rules are only allowed for public int ernet  IP addresses. IP address ranges\nreserved for private networks (as defined in RFC 1918 ) aren't allowed in IP rules.\nPrivate networks include addresses that start with 10.*, 172.16.* - 172.31.*, and\n192.168.*.\nOnly IPV4 addresses are supported at this time. Each Cognitive Services resource\nsupports up to 100 IP network rules, which may be combined with Virtual network rules .\nTo grant access from your on-premises networks to your Cognitive Services resource\nwith an IP network rule, you must identify the internet facing IP addresses used by your\nnetwork. Contact your network administrator for help.\nIf you're using ExpressR oute on-premises for public peering or Microsoft peering, you\nneed to identify the NA T IP addresses. For public peering, each ExpressR oute circuit by\ndefault uses two NA T IP addresses. Each is applied to Azure service traffic when the\ntraffic enters the Microsoft Azure network backbone. For Microsoft peering, the NA T IP\naddresses that are used are either customer provided or are provided by the service\nprovider. T o allow access to your service resources, you must allow these public IP\naddresses in the resource IP firewall setting. T o find your public peering ExpressR oute\ncircuit IP addresses, open a support ticket with ExpressR oute  via the Azure portal.\nLearn more about NAT for ExpressR oute public and Microsoft peering.\nYou can manage IP network rules for Cognitive Services resources through the Azure\nportal, P owerShell, or the Azure CLI.\n1. Go to the Cognitive Services resource you want to secure.\n2. Select the RESOUR CE MANA GEMENT  menu called Virtual netw ork.\n3. Check that you've selected to allow access from Select ed netw orks.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "07d818880cbe-1", "text": "3. Check that you've selected to allow access from Select ed netw orks.\n4. To grant access to an internet IP range, enter the IP address or address range\n(in CIDR format ) under Firewall  > Addr ess Range . Only valid public IP (non-\nreserved) addresses are accepted.\nConfiguring access from on-premises networks\nManaging IP network rules\nAzure portal", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b0919cfe9235-0", "text": "5. To remove an IP network rule, select the trash can \uff81 icon next to the address\nrange.\n6. Select Save to apply your changes.\n\uff09 Impor tant\nBe sure to set the default rule  to deny , or network rules have no effect.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "245ad9598cbb-0", "text": "You can use private endpoints  for your Cognitive Services resources to allow clients on a\nvirtual network (VNet) to securely access data over a Private Link . The private endpoint\nuses an IP address from the VNet address space for your Cognitive Services resource.\nNetwork traffic between the clients on the VNet and the resource traverses the VNet\nand a private link on the Microsoft backbone network, eliminating exposure from the\npublic internet.\nPrivate endpoints for Cognitive Services resources let you:\nSecure your Cognitive Services resource by configuring the firewall to block all\nconnections on the public endpoint for the Cognitive Services service.\nIncrease security for the VNet, by enabling you to block exfiltration of data from\nthe VNet.\nSecurely connect to Cognitive Services resources from on-premises networks that\nconnect to the VNet using VPN or ExpressR outes  with private-peering.\nA private endpoint is a special network interface for an Azure resource in your VNet .\nCreating a private endpoint for your Cognitive Services resource provides secure\nconnectivity between clients in your VNet and your resource. The private endpoint is\nassigned an IP address from the IP address range of your VNet. The connection between\nthe private endpoint and the Cognitive Services service uses a secure private link.\nApplications in the VNet can connect to the service over the private endpoint\nseamlessly, using the same connection strings and authorization mechanisms that they\nwould use otherwise. The exception is the Speech Services, which require a separate\nendpoint. See the section on Private endpoints with the Speech Services . Private\nendpoints can be used with all protocols supported by the Cognitive Services resource,\nincluding REST.\nPrivate endpoints can be created in subnets that use Service Endpoints . Clients in a\nsubnet can connect to one Cognitive Services resource using private endpoint, while\nusing service endpoints to access others.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "245ad9598cbb-1", "text": "using service endpoints to access others.\nWhen you create a private endpoint for a Cognitive Services resource in your VNet, a\nconsent request is sent for approval to the Cognitive Services resource owner. If the user\nrequesting the creation of the private endpoint is also an owner of the resource, this\nconsent request is automatically approved.Use private endpoints\nConceptual overview", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "10096c33a29f-0", "text": "Cognitive Services resource owners can manage consent requests and the private\nendpoints, through the ' Private endpoints ' tab for the Cognitive Services resource in the\nAzure portal .\nWhen creating the private endpoint, you must specify the Cognitive Services resource it\nconnects to. For more information on creating a private endpoint, see:\nCreate a private endpoint using the Private Link Center in the Azure portal\nCreate a private endpoint using Azure CLI\nCreate a private endpoint using Azure P owerShell\nClients on a VNet using the private endpoint should use the same connection string for\nthe Cognitive Services resource as clients connecting to the public endpoint. The\nexception is the Speech Services, which require a separate endpoint. See the section on\nPrivate endpoints with the Speech Services . We rely upon DNS resolution to\nautomatically route the connections from the VNet to the Cognitive Services resource\nover a private link.\nWe create a private DNS zone  attached to the VNet with the necessary updates for the\nprivate endpoints, by default. However, if you're using your own DNS server, you may\nneed to make more changes to your DNS configuration. The section on DNS changes\nbelow describes the updates required for private endpoints.\nSee Using Speech Services with private endpoints provided by Azure Private Link .\nPrivate endpoints\nConnecting to private endpoints\n\uff17 Note\nAzure OpenAI Service uses a different private DNS zone and public DNS zone\nforwarder than other Azure Cognitive Services. R efer to the Azur e ser vices DNS\nzone configuration ar ticle for the correct zone and forwader names.\nPrivate endpoints with the Speech Services\nDNS changes for private endpoints", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a2306b136c90-0", "text": "When you create a private endpoint, the DNS CNAME resource record for the Cognitive\nServices resource is updated to an alias in a subdomain with the prefix privatelink. By\ndefault, we also create a private DNS zone , corresponding to the privatelink\nsubdomain, with the DNS A resource records for the private endpoints.\nWhen you resolve the endpoint URL from outside the VNet with the private endpoint, it\nresolves to the public endpoint of the Cognitive Services resource. When resolved from\nthe VNet hosting the private endpoint, the endpoint URL resolves to the private\nendpoint's IP address.\nThis approach enables access to the Cognitive Services resource using the same\nconnection string for clients in the VNet hosting the private endpoints and clients\noutside the VNet.\nIf you're using a custom DNS server on your network, clients must be able to resolve the\nfully qualified domain name (FQDN) for the Cognitive Services resource endpoint to the\nprivate endpoint IP address. Configure your DNS server to delegate your private link\nsubdomain to the private DNS zone for the VNet.\nFor more information on configuring your own DNS server to support private endpoints,\nsee the following articles:\nName resolution for resources in Azure virtual networks\nDNS configuration for private endpoints\nFor pricing details, see Azure Private Link pricing .\nExplore the various Azure Cognitive Services\nLearn more about Azure Virtual Network Service Endpoints\uea80 Tip\nWhen using a custom or on-premises DNS server, you should configure your DNS\nserver to resolve the Cognitive Services resource name in the 'privatelink'\nsubdomain to the private endpoint IP address. Y ou can do this by delegating the\n'privatelink' subdomain to the private DNS zone of the VNet, or configuring the\nDNS zone on your DNS server and adding the DNS A records.\nPricing\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e72d613ea5d0-0", "text": "Use Azu re OpenAI with large datasets\nArticle \u202202/17/2023\nAzure OpenAI can be used to solve a large number of natural language tasks through\nprompting the completion API. T o make it easier to scale your prompting workflows\nfrom a few examples to large datasets of examples, we have integrated the Azure\nOpenAI service with the distributed machine learning library SynapseML . This\nintegration makes it easy to use the Apache Spark  distributed computing framework\nto process millions of prompts with the OpenAI service. This tutorial shows how to apply\nlarge language models at a distributed scale using Azure Open AI and Azure S ynapse\nAnalytics.\nAn Azure subscription - Create one for free\nAccess granted to Azure OpenAI in the desired Azure subscription\nCurrently, access to this service is granted only by application. Y ou can apply for\naccess to Azure OpenAI by completing the form at https://aka.ms/oai/access .\nOpen an issue on this repo to contact us if you have an issue.\nAn Azure OpenAI resource \u2013 create a resource\nAn Apache Spark cluster with S ynapseML installed - create a serverless Apache\nSpark pool here\nWe recommend creating a S ynapse workspace , but an Azure Databricks, HDInsight, or\nSpark on Kubernetes, or even a Python environment with the pyspark package, will also\nwork.\nThe next step is to add this code into your Spark cluster. Y ou can either create a\nnotebook in your Spark platform and copy the code into this notebook to run the demo,\nor download the notebook and import it into S ynapse Analytics.\n1. Download this demo as a notebook  (click Raw, then save the file)\n2. Import the notebook into the S ynapse W orkspace  or, if using Databricks, into the\nDatabricks W orkspace\nPrerequisites", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e72d613ea5d0-1", "text": "Databricks W orkspace\nPrerequisites\nImport this guide as a notebook", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c2413b90cc9b-0", "text": "3. Install S ynapseML on your cluster. See the installation instructions for S ynapse at\nthe bottom of the S ynapseML website . This requires pasting another cell at the\ntop of the notebook you imported\n4. Connect your notebook to a cluster and follow along, editing and running the cells\nbelow.\nNext, edit the cell in the notebook to point to your service. In particular, set the\nresource_name, deployment_name, location, and key variables to the corresponding\nvalues for your Azure OpenAI resource.\nPython\nNext, create a dataframe consisting of a series of rows, with one prompt per row.\nYou can also load data directly from Azure Data Lake S torage (ADLS) or other databases.\nFor more information about loading and preparing Spark dataframes, see the Apache\nSpark data loading guide .\nPython\nFill in your service information\n\uff09 Impor tant\nRemember to remove the key from your code when you're done, and never post it\npublicly. For production, use a secure way of storing and accessing your credentials\nlike Azur e Key Vault. See the Cognitive Services security  article for more\ninformation.\nimport os \n# Replace the following values with your Azure OpenAI resource information  \nresource_name = \"RESOURCE_NAME\"       # The name of your Azure OpenAI  \nresource.  \ndeployment_name = \"DEPLOYMENT_NAME\"   # The name of your Azure OpenAI  \ndeployment.  \nlocation = \"RESOURCE_LOCATION\"        # The location or region ID for your  \nresource.  \nkey = \"RESOURCE_API_KEY\"              # The key for your resource.  \nassert key is not None and resource_name is not None \nCreate a dataset of prompts", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "664185cb2eca-0", "text": "To apply the OpenAI Completion service to the dataframe that you just created, create\nan OpenAICompletion object that serves as a distributed client. P arameters of the service\ncan be set either with a single value, or by a column of the dataframe with the\nappropriate setters on the OpenAICompletion object. Here, we're setting maxTokens to\n200. A token is around four characters, and this limit applies to the sum of the prompt\nand the result. W e're also setting the promptCol parameter with the name of the prompt\ncolumn in the dataframe.\nPython\nNow that you have the dataframe and the completion client, you can transform your\ninput dataset and add a column called completions with all of the information the\nservice adds. W e'll select out just the text for simplicity.\nPythondf = spark.createDataFrame(  \n    [ \n        ( \"Hello my name is\" ,), \n        ( \"The best code is code that's\" ,), \n        ( \"SynapseML is \" ,), \n    ] \n).toDF(\"prompt\" ) \nCreate the OpenAICompletion Apache Spark\nclient\nfrom synapse.ml.cognitive import OpenAICompletion  \ncompletion = (  \n    OpenAICompletion()  \n    .setSubscriptionKey(key)  \n    .setDeploymentName(deployment_name)  \n    .setUrl( \"https://{}.openai.azure.com/\" .format(resource_name))  \n    .setMaxTokens( 200) \n    .setPromptCol( \"prompt\" ) \n    .setErrorCol( \"error\") \n    .setOutputCol( \"completions\" ) \n) \nTransform the dataframe with the\nOpenAICompletion client", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6571d82dd8cc-0", "text": "Your output should look something like the following example; note that the completion\ntext can vary.\nprompt error text\nHello my name is undefined Makaveli I'm eighteen years old and I want to  \nbe a rapper when I grow up I love writing and making music\nI'm from Los  \nAngeles, CA\nThe best code is\ncode that'sundefined understandable This is a subjective statement,  \nand there is no definitive answer.\nSynapseML is undefined A machine learning algorithm that is able to learn how to\npredict the future outcome of events.\nThe example above makes several requests to the service, one for each prompt. T o\ncomplete multiple prompts in a single request, use batch mode. First, in the\nOpenAICompletion object, instead of setting the Prompt column to \"Prompt\", specify\n\"batchPrompt\" for the BatchPrompt column. T o do so, create a dataframe with a list of\nprompts per row.\nPythonfrom pyspark.sql.functions import col \ncompleted_df = completion.transform(df).cache()  \ndisplay(completed_df.select(  \n  col(\"prompt\" ), col(\"error\"), \ncol(\"completions.choices.text\" ).getItem( 0).alias( \"text\"))) \nOther usage examples\nImprove throughput with request batching\n\uff17 Note\nThere is currently a limit of 20 prompts in a single request and a limit of 2048\n\"tokens\", or approximately 1500 words.\nbatch_df = spark.createDataFrame(\n    [ \n        ([ \"The time has come\" , \"Pleased to\" , \"Today stocks\" , \"Here's to\" ],), \n        ([ \"The only thing\" , \"Ask not what\" , \"Every litter\" , \"I am\"],),", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0eacc857788f-0", "text": "Next we create the OpenAICompletion object. Rather than setting the prompt column, set\nthe batchPrompt column if your column is of type Array[String].\nPython\nIn the call to transform, a request will then be made per row. Because there are multiple\nprompts in a single row, each request will be sent with all prompts in that row. The\nresults will contain a row for each row in the request.\nPython\nIf your data is in column format, you can transpose it to row format using S ynapseML's\nFixedMiniBatcherTransformer.\nPython    ] \n).toDF(\"batchPrompt\" ) \nbatch_completion = (  \n    OpenAICompletion()  \n    .setSubscriptionKey(key)  \n    .setDeploymentName(deployment_name)  \n    .setUrl( \"https://{}.openai.azure.com/\" .format(resource_name))  \n    .setMaxTokens( 200) \n    .setBatchPromptCol( \"batchPrompt\" ) \n    .setErrorCol( \"error\") \n    .setOutputCol( \"completions\" ) \n) \ncompleted_batch_df = batch_completion.transform(batch_df).cache()  \ndisplay(completed_batch_df)  \n\uff17 Note\nThere is currently a limit of 20 prompts in a single request and a limit of 2048\n\"tokens\", or approximately 1500 words.\nUsing an automatic mini-batcher\nfrom pyspark.sql.types import StringType  \nfrom synapse.ml.stages import FixedMiniBatchTransformer  \nfrom synapse.ml.core.spark import FluentAPI  \ncompleted_autobatch_df = (df", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c73cb4f82c2d-0", "text": "Azure OpenAI can solve many different natural language tasks through prompt\nengineering . Here, we show an example of prompting for language translation:\nPython\nHere, we prompt the GPT-3 model for general-knowledge question answering:\nPython .coalesce( 1) # Force a single partition so that our little 4-row dataframe  \nmakes a batch of size 4, you can remove this step for large datasets  \n .mlTransform(FixedMiniBatchTransformer(batchSize= 4)) \n .withColumnRenamed( \"prompt\" , \"batchPrompt\" )  \n .mlTransform(batch_completion))  \ndisplay(completed_autobatch_df)  \nPrompt engineering for translation\ntranslate_df = spark.createDataFrame(  \n    [ \n        ( \"Japanese: Ookina hako \\nEnglish: Big box \\nJapanese: Midori  \ntako\\nEnglish:\" ,), \n        ( \"French: Quelle heure est-il \u00e0 Montr\u00e9al? \\nEnglish: What time is it  \nin Montreal? \\nFrench: O\u00f9 est le poulet? \\nEnglish:\" ,), \n    ] \n).toDF(\"prompt\" ) \ndisplay(completion.transform(translate_df))  \nPrompt for question answering\nqa_df = spark.createDataFrame(  \n    [ \n        (  \n            \"Q: Where is the Grand Canyon?\\nA: The Grand Canyon is in  \nArizona.\\n\\nQ: What is the weight of the Burj Khalifa in kilograms?\\nA:\" , \n        )  \n    ] \n).toDF(\"prompt\" ) \ndisplay(completion.transform(qa_df))", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "17de0e052f48-0", "text": "Azure OpenAI Service encryption of\ndata at rest\nArticle \u202204/21/2023\nAzure OpenAI automatically encrypts your data when it's persisted to the cloud. The\nencryption protects your data and helps you meet your organizational security and\ncompliance commitments. This article covers how Azure OpenAI handles encryption of\ndata at rest, specifically training data and fine-tuned models. For information on how\ndata provided by you to the service is processed, used, and stored, consult the data,\nprivacy, and security article .\nAzure OpenAI is part of Azure Cognitive Services. Cognitive Services data is encrypted\nand decrypted using FIPS 140-2  compliant 256-bit AES  encryption. Encryption and\ndecryption are transparent, meaning encryption and access are managed for you. Y our\ndata is secure by default and you don't need to modify your code or applications to take\nadvantage of encryption.\nBy default, your subscription uses Microsoft-managed encryption keys. There's also the\noption to manage your subscription with your own keys called customer-managed keys\n(CMK). CMK offers greater flexibility to create, rotate, disable, and revoke access\ncontrols. Y ou can also audit the encryption keys used to protect your data.\nCustomer-managed keys (CMK), also known as Bring your own key (B YOK), offer greater\nflexibility to create, rotate, disable, and revoke access controls. Y ou can also audit the\nencryption keys used to protect your data.\nYou must use Azure K ey Vault to store your customer-managed keys. Y ou can either\ncreate your own keys and store them in a key vault, or you can use the Azure K ey Vault\nAPIs to generate keys. The Cognitive Services resource and the key vault must be in the\nsame region and in the same Azure Active Directory (Azure AD) tenant, but they can be", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "17de0e052f48-1", "text": "same region and in the same Azure Active Directory (Azure AD) tenant, but they can be\nin different subscriptions. For more information about Azure K ey Vault, see What is\nAzure K ey Vault? .About Cognitive Services encryption\nAbout encryption key management\nCustomer-managed keys with Azure Key Vault", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8b64374652e6-0", "text": "To request the ability to use customer-managed keys, fill out and submit the Cognitive\nServices Customer-Managed K ey Request Form . It will take approximately 3-5\nbusiness days to hear back on the status of your request.\nTo enable customer-managed keys, you must also enable both the Soft Delet e and Do\nNot Pur ge properties on the key vault.\nOnly RSA keys of size 2048 are supported with Cognitive Services encryption. For more\ninformation about keys, see Key Vault k eys in About Azure K ey Vault keys, secrets and\ncertificates .\nTo enable customer-managed keys in the Azure portal, follow these steps:\n1. Go to your Cognitive Services resource.\n2. On the left, select Encryption .\n3. Under Encryption type , select Customer Managed K eys, as shown in the following\nscreenshot.\nAfter you enable customer-managed keys, you can specify a key to associate with the\nCognitive Services resource.\nTo specify a key as a URI, follow these steps:\nEnable customer-managed keys for your\nresource\nSpecify a key\nSpecify a key as a URI", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "52d913828e01-0", "text": "1. In the Azure portal, go to your key vault.\n2. Under Settings , select Keys.\n3. Select the desired key, and then select the key to view its versions. Select a key\nversion to view the settings for that version.\n4. Copy the Key Identifier  value, which provides the URI.\n5. Go back to your Cognitive Services resource, and then select Encryption .\n6. Under Encryption k ey, select Enter key URI .\n7. Paste the URI that you copied into the Key URI  box.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8493d1b69bf6-0", "text": "8. Under Subscription , select the subscription that contains the key vault.\n9. Save your changes.\nTo specify a key from a key vault, first make sure that you have a key vault that contains\na key. Then follow these steps:\n1. Go to your Cognitive Services resource, and then select Encryption .\n2. Under Encryption k ey, select Select fr om K ey Vault.\n3. Select the key vault that contains the key that you want to use.\n4. Select the key that you want to use.Specify a key from a key vault", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7da6fbe2f855-0", "text": "5. Save your changes.\nWhen you create a new version of a key, update the Cognitive Services resource to use\nthe new version. Follow these steps:\n1. Go to your Cognitive Services resource, and then select Encryption .\n2. Enter the URI for the new key version. Alternately, you can select the key vault and\nthen select the key again to update the version.\n3. Save your changes.\nTo change the key that you use for encryption, follow these steps:\n1. Go to your Cognitive Services resource, and then select Encryption .\n2. Enter the URI for the new key. Alternately, you can select the key vault and then\nselect a new key.\n3. Save your changes.\nYou can rotate a customer-managed key in K ey Vault according to your compliance\npolicies. When the key is rotated, you must update the Cognitive Services resource to\nuse the new key URI. T o learn how to update the resource to use a new version of the\nkey in the Azure portal, see Update the key version .Update the key version\nUse a different key\nRotate customer-managed keys", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e6c11c7d2336-0", "text": "Rotating the key doesn't trigger re-encryption of data in the resource. No further action\nis required from the user.\nYou can revoke a customer-managed encryption key by changing the access policy, by\nchanging the permissions on the key vault, or by deleting the key.\nTo change the access policy of the managed identity that your registry uses, run the az-\nkeyvault-delete-policy  command:\nAzure CLI\nTo delete the individual versions of a key, run the az-keyvault-key-delete  command. This\noperation requires the keys/delet e permission.\nAzure CLI\nThe Files API allows customers to upload their training data for the purpose of fine-\ntuning a model. This data is stored in Azure S torage, within the same region as the\nresource and logically isolated with their Azure subscription and API Credentials.\nUploaded files can be deleted by the user via the DELETE API operation .Revoke a customer-managed key\naz keyvault delete-policy \\ \n  --resource-group  <resource-group-name>  \\ \n  --name <key-vault-name>  \\ \n  --key_id <key-vault-key-id>  \naz keyvault key delete  \\ \n  --name <key-vault-name>  \\ \n  --object-id  $identityPrincipalID  \\                      \n\uff09 Impor tant\nRevoking access to an active customer-managed key while CMK is still enabled will\nprevent downloading of training data and results files, fine-tuning new models, and\ndeploying fine-tuned models. However, previously deployed fine-tuned models will\ncontinue to operate and serve traffic until those deployments are deleted.\nDelete training, validation, and training results data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c4dd151f0dc5-0", "text": "The Fine-tunes API allows customers to create their own fine-tuned version of the\nOpenAI models based on the training data that you've uploaded to the service via the\nFiles APIs. The trained fine-tuned models are stored in Azure S torage in the same region,\nencrypted at rest (either with Microsoft-managed keys or customer-managed keys) and\nlogically isolated with their Azure subscription and API credentials. Fine-tuned models\nand deployments can be deleted by the user by calling the DELETE API operation .\nWhen you disable customer-managed keys, your Cognitive Services resource is then\nencrypted with Microsoft-managed keys. T o disable customer-managed keys, follow\nthese steps:\n1. Go to your Cognitive Services resource, and then select Encryption .\n2. Select Microsoft Managed K eys > Save.\nWhen you previously enabled customer managed keys this also enabled a system\nassigned managed identity, a feature of Azure AD. Once the system assigned managed\nidentity is enabled, this resource will be registered with Azure Active Directory. After\nbeing registered, the managed identity will be given access to the K ey Vault selected\nduring customer managed key setup. Y ou can learn more about Managed Identities .Delete fine-tuned models and deployments\nDisable customer-managed keys\n\uff09 Impor tant\nIf you disable system assigned managed identities, access to the key vault will be\nremoved and any data encrypted with the customer keys will no longer be\naccessible. Any features depended on this data will stop working.\n\uff09 Impor tant\nManaged identities do not currently support cross-directory scenarios. When you\nconfigure customer-managed keys in the Azure portal, a managed identity is\nautomatically assigned under the covers. If you subsequently move the\nsubscription, resource group, or resource from one Azure AD directory to another,\nthe managed identity associated with the resource is not transferred to the new", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c4dd151f0dc5-1", "text": "the managed identity associated with the resource is not transferred to the new\ntenant, so customer-managed keys may no longer work. For more information, see\nTransferring a subscription betw een Azur e AD dir ectories  in FAQs and known\nissues with managed identities for Azur e resour ces.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "25f059d97486-0", "text": "Language service Customer-Managed K ey Request Form\nLearn more about Azure K ey VaultNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fc9262529027-0", "text": "Business Continuity and Disaster\nRecovery (BCDR) considerations with\nAzure OpenAI Service\nArticle \u202206/22/2023\nAzure OpenAI is available in multiple regions. Since subscription keys are region bound,\nwhen a customer acquires a key, they select the region in which their deployments will\nreside and from then on, all operations stay associated with that Azure server region.\nIt's rare, but not impossible, to encounter a network issue that hits an entire region. If\nyour service needs to always be available, then you should design it to either fail-over\ninto another region or split the workload between two or more regions. Both\napproaches require at least two Azure OpenAI resources in different regions. This article\nprovides general recommendations for how to implement Business Continuity and\nDisaster R ecovery (BCDR) for your Azure OpenAI applications.\nToday customers will call the endpoint provided during deployment for both\ndeployments and inference. These operations are stateless, so no data is lost in the case\nthat a region becomes unavailable.\nIf a region is non-operational customers must take steps to ensure service continuity.\nThe following set of instructions applies both customers using default endpoints and\nthose using custom endpoints.\nIf you're using a default endpoint, you should configure your client code to monitor\nerrors, and if the errors persist, be prepared to redirect to another region of your choice\nwhere you have an Azure OpenAI subscription.\nFollow these steps to configure your client to monitor errors:\n1. Use the models page  to identify the list of available regions for Azure OpenAI.Best practices\nBusiness continuity\nDefault endpoint recovery", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b55e7e625f56-0", "text": "2. Select a primary and one secondary/backup regions from the list.\n3. Create Azure OpenAI resources for each region selected.\n4. For the primary region and any backup regions your code will need to know:\na. Base URI for the resource\nb. Regional access key or Azure Active Directory access\n5. Configure your code so that you monitor connectivity errors (typically connection\ntimeouts and service unavailability errors).\na. Given that networks yield transient errors, for single connectivity issue\noccurrences, the suggestion is to retry.\nb. For persistence redirect traffic to the backup resource in the region you've\ncreated.\nThe recovery from regional failures for this usage type can be performed\ninstantaneously and at a very low cost. This does however, require custom development\nof this functionality on the client side of your application.BCDR requires custom code", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "177a439f810a-0", "text": "Monitoring Azure OpenAI Service\nArticle \u202203/15/2023\nWhen you have critical applications and business processes relying on Azure resources, you want to\nmonitor those resources for their availability, performance, and operation.\nThis article describes the monitoring data generated by Azure OpenAI Service. Azure OpenAI is part of\nCognitive Services, which uses Azure Monitor . If you're unfamiliar with the features of Azure Monitor\ncommon to all Azure services that use it, read Monitoring Azure resources with Azure Monitor .\nAzure OpenAI collects the same kinds of monitoring data as other Azure resources that are described in\nMonitoring data from Azure resources .\nPlatform metrics and the Activity log are collected and stored automatically, but can be routed to other\nlocations by using a diagnostic setting.\nResource Logs aren't collected and stored until you create a diagnostic setting and route them to one or\nmore locations.\nSee Create diagnostic setting to collect platform logs and metrics in Azure  for the detailed process for\ncreating a diagnostic setting using the Azure portal, CLI, or P owerShell. When you create a diagnostic\nsetting, you specify which categories of logs to collect.\nKeep in mind that using diagnostic settings and sending data to Azure Monitor Logs has additional costs\nassociated with it. T o understand more, consult the Azure Monitor cost calculation guide .\nThe metrics and logs you can collect are discussed in the following sections.\nYou can analyze metrics for Azure OpenAI  by opening Metrics  which can be found underneath the\nMonit oring  section when viewing your Azure OpenAI resource in the Azure portal. See Getting started\nwith Azure Metrics Explorer  for details on using this tool.\nAzure OpenAI is a part of Cognitive Services. For a list of all platform metrics collected for Cognitive\nServices and Azure OpenAI, see Cognitive Services supported metrics .\nFor the current subset of metrics available in Azure OpenAI:Monitoring data\nCollection and routing\nAnalyzing metrics", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "177a439f810a-1", "text": "Collection and routing\nAnalyzing metrics\nAzure OpenAI Metrics", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "47164153d23b-0", "text": "Metr ic Expor table\nvia\nDiagnostic\nSettings?Metr ic\nDisplay\nNameUnit Aggr egation\nTypeDescr iption Dimensions Metr ic Expor table\nvia\nDiagnostic\nSettings?Metr ic\nDisplay\nNameUnit Aggr egation\nTypeDescr iption Dimensions\nBlockedCalls Yes Blocked\nCallsCount Total Number of\ncalls that\nexceeded\nrate or\nquota limit.ApiName,\nOperationName, R egion,\nRatelimitK ey\nClientErrors Yes Client\nErrorsCount Total Number of\ncalls with\nclient side\nerror (HT TP\nresponse\ncode 4xx).ApiName,\nOperationName, R egion,\nRatelimitK ey\nDataIn Yes Data In Bytes Total Size of\nincoming\ndata in\nbytes.ApiName,\nOperationName, R egion\nDataOut Yes Data Out Bytes Total Size of\noutgoing\ndata in\nbytes.ApiName,\nOperationName, R egion\nFineTunedT rainingHours Yes Processed\nFineTuned\nTraining\nHoursCount Total Number of\nTraining\nHours\nProcessed\non an\nOpenAI\nFineTuned\nModelApiName,\nModelDeploymentName,\nFeatureName,\nUsageChannel, R egion\nLatency Yes Latency MilliSeconds Average Latency in\nmilliseconds.ApiName,\nOperationName, R egion,\nRatelimitK ey\nRatelimit Yes Ratelimit Count Total The current\nratelimit of\nthe ratelimit\nkey.Region, RatelimitK ey\nServerErrors Yes Server\nErrorsCount Total Number of\ncalls with\nservice\ninternal\nerror (HT TP\nresponse\ncode 5xx).ApiName,\nOperationName, R egion,\nRatelimitK ey\nSuccessfulCalls Yes Successful\nCallsCount Total Number of\nsuccessful\ncalls.ApiName,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "47164153d23b-1", "text": "SuccessfulCalls Yes Successful\nCallsCount Total Number of\nsuccessful\ncalls.ApiName,\nOperationName, R egion,\nRatelimitK ey", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "223316b2c171-0", "text": "Metr ic Expor table\nvia\nDiagnostic\nSettings?Metr ic\nDisplay\nNameUnit Aggr egation\nTypeDescr iption Dimensions\nTokenT ransaction Yes Processed\nInference\nTokensCount Total Number of\nInference\nTokens\nProcessed\non an\nOpenAI\nModelApiName,\nModelDeploymentName,\nFeatureName,\nUsageChannel, R egion\nTotalCalls Yes Total Calls Count Total Total\nnumber of\ncalls.ApiName,\nOperationName, R egion,\nRatelimitK ey\nTotalErrors Yes Total\nErrorsCount Total Total\nnumber of\ncalls with\nerror\nresponse\n(HTTP\nresponse\ncode 4xx or\n5xx).ApiName,\nOperationName, R egion,\nRatelimitK ey\nData in Azure Monitor Logs is stored in tables where each table has its own set of unique properties.\nAll resource logs in Azure Monitor have the same fields followed by service-specific fields. The common\nschema is outlined in Azure Monitor resource log schema .\nThe Activity log  is a type of platform log in Azure that provides insight into subscription-level events. Y ou\ncan view it independently or route it to Azure Monitor Logs, where you can do much more complex\nqueries using Log Analytics.\nFor a list of the types of resource logs available for Azure OpenAI and other Cognitive Services, see\nResource provider operations for Cognitive ServicesAnalyzing logs\nKusto queries\n\uff09 Impor tant\nWhen you select Logs  from the Azure OpenAI menu, Log Analytics is opened with the query scope\nset to the current Azure OpenAI resource. This means that log queries will only include data from that\nresource. If you want to run a query that includes data from other resources or data from other Azure\nservices, select Logs  from the Azur e Monit or menu. See Log quer y scope and time range in Azur e", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "223316b2c171-1", "text": "Monit or Log Analy tics for details.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e871da0b1cff-0", "text": "To explore and get a sense of what type of information is available for your Azure OpenAI resource a\nuseful query to start with once you have deployed a model and sent some completion calls through the\nplayground is as follows:\nKusto\nHere we return a sample of 100 entries and are displaying a subset of the available columns of data in the\nlogs. The results are as follows:\nIf you wish to see all available columns of data, you can remove the scoping that is provided by the |\nproject line:\nKusto\nYou can also select the arrow next to the table name to view all available columns and associated data\ntypes.\nTo examine AzureMetrics run:\nKustoAzureDiagnostics  \n| take 100 \n| project TimeGenerated, _ResourceId, Category,OperationName, DurationMs, ResultSignature,  \nproperties_s  \n\uf80a\nAzureDiagnostics  \n| take 100 \nAzureMetrics  \n| take 100 \n| project TimeGenerated, MetricName, Total, Count, TimeGrain, UnitName", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "317874ff3763-0", "text": "Azure Monitor alerts proactively notify you when important conditions are found in your monitoring data.\nThey allow you to identify and address issues in your system before your customers notice them. Y ou can\nset alerts on metrics , logs, and the activity log . Different types of alerts have different benefits and\ndrawbacks.\nEvery organization's alerting needs are going to vary, and will also evolve over time. Generally all alerts\nshould be actionable, with a specific intended response if the alert occurs. If there's no action for someone\nto take, then it might be something you want to capture in a report, but not in an alert. Some use cases\nmay require alerting anytime certain error conditions exist. But in many environments, it might only be in\ncases where errors exceed a certain threshold for a period of time where sending an alert is warranted.\nErrors below certain thresholds can often be evaluated through regular analysis of data in Azure Monitor\nLogs. As you analyze your log data over time, you may also find that a certain condition not occurring for\na long enough period of time might be valuable to track with alerts. Sometimes the absence of an event in\na log is just as important a signal as an error.\nDepending on what type of application you're developing in conjunction with your use of Azure OpenAI,\nAzure Monitor Application Insights  may offer additional monitoring benefits at the application layer.\nSee Monitoring Azure resources with Azure Monitor  for details on monitoring Azure resources.\nRead Understand log searches in Azure Monitor logs .\n\uf80a\nAlerts\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2834a3e4c409-0", "text": "Plan to manage costs for Azure OpenAI\nService\nArticle \u202204/05/2023\nThis article describes how you plan for and manage costs for Azure OpenAI Service.\nBefore you deploy the service, you can use the Azure pricing calculator to estimate costs\nfor Azure OpenAI. Later, as you deploy Azure resources, review the estimated costs.\nAfter you've started using Azure OpenAI resources, use Cost Management features to\nset budgets and monitor costs. Y ou can also review forecasted costs and identify\nspending trends to identify areas where you might want to act. Costs for Azure OpenAI\nService are only a portion of the monthly costs in your Azure bill. Although this article\nexplains how to plan for and manage costs for Azure OpenAI, you're billed for all Azure\nservices and resources used in your Azure subscription, including the third-party\nservices.\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data . To view cost data, you need at least read access for an Azure\naccount. For information about assigning access to Azure Cost Management data, see\nAssign access to data .\nUse the Azure pricing calculator  to estimate the costs of using Azure OpenAI.\nAzure OpenAI Service runs on Azure infrastructure that accrues costs when you deploy\nnew resources. It's important to understand that there could be other additional\ninfrastructure costs that might accrue.Prerequisites\nEstimate costs before using Azure OpenAI\nUnderstand the full billing  model for Azure\nOpenAI Service\nHow you're charged for Azure OpenAI Service\nBase series and Codex series models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "029c7cc2a5ee-0", "text": "Azure OpenAI base series and Codex series models are charged per 1,000 tokens. Costs\nvary depending on which model series you choose: Ada, Babbage, Curie, Davinci, or\nCode-Cushman.\nOur models understand and process text by breaking it down into tokens. For reference,\neach token is roughly four characters for typical English text.\nToken costs are for both input and output. For example, if you have a 1,000 token\nJavaScript code sample that you ask an Azure OpenAI model to convert to Python. Y ou\nwould be charged approximately 1,000 tokens for the initial input request sent, and\n1,000 more tokens for the output that is received in response for a total of 2,000 tokens.\nIn practice, for this type of completion call the token input/output wouldn't be perfectly\n1:1. A conversion from one programming language to another could result in a longer\nor shorter output depending on many different factors including the value assigned to\nthe max_tokens parameter.\nAzure OpenAI fine-tuned models are charged based on three factors:\nTraining hours\nHosting hours\nInference per 1,000 tokens\nThe hosting hours cost is important to be aware of since once a fine-tuned model is\ndeployed it continues to incur an hourly cost regardless of whether you're actively using\nit. Fine-tuned model costs should be monitored closely.Base Series and Codex series fine-tuned models\n\uff09 Impor tant\nAfter a customized model is deployed, if at any time the deployment remains\ninactive for greater than fifteen (15) days, the deployment will automatically be\ndeleted. The deployment of a customized model is \u201cinactive\u201d if the model was\ndeployed more than fifteen (15) days ago and no completions or chat completions\ncalls were made to it during a continuous 15-day period. The deletion of an inactive", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "029c7cc2a5ee-1", "text": "calls were made to it during a continuous 15-day period. The deletion of an inactive\ndeployment does NOT delete or affect the underlying customized model, and the\ncustomized model can be redeployed at any time. As described in Azur e OpenAI\nService pricing , each customized (fine-tuned) model that is deployed incurs an\nhourly hosting cost regardless of whether completions or chat completions calls are\nbeing made to the model. T o learn more about planning and managing costs with\nAzure OpenAI, refer to our cost management guide .", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c0c2485ed33b-0", "text": "Keep in mind that enabling capabilities like sending data to Azure Monitor Logs,\nalerting, etc. incurs additional costs for those services. These costs are visible under\nthose other services and at the subscription level, but aren't visible when scoped just to\nyour Azure OpenAI resource.\nYou can pay for Azure OpenAI Service charges with your Azure Prepayment credit.\nHowever, you can't use Azure Prepayment credit to pay for charges for third party\nproducts and services including those from the Azure Marketplace.\nAs you use Azure resources with Azure OpenAI, you incur costs. Azure resource usage\nunit costs vary by time intervals (seconds, minutes, hours, and days) or by unit usage\n(bytes, megabytes, and so on.) As soon as Azure OpenAI use starts, costs can be\nincurred and you can see the costs in cost analysis .\nWhen you use cost analysis, you view Azure OpenAI costs in graphs and tables for\ndifferent time intervals. Some examples are by day, current and prior month, and year.\nYou also view costs against budgets and forecasted costs. S witching to longer views\nover time can help you identify spending trends. And you see where overspending\nmight have occurred. If you've created budgets, you can also easily see where they're\nexceeded.\nTo view Azure OpenAI costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Select one of your Azure OpenAI resources.\n3. Under Resour ce Management  select Cost analysis\n4. By default cost analysis is scoped to the individual Azure OpenAI resource.Other costs that might accrue with Azure OpenAI Service\nUsing Azure Prepayment with Azure OpenAI Service\nMonitor costs", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b357679bd467-0", "text": "To understand the breakdown of what makes up that cost, it can help to modify Group\nby to Meter and in this case switching the chart type to Line. You can now see that for\nthis particular resource the source of the costs is from three different model series with\nText-Davinci T okens representing the bulk of the costs.\nIt's important to understand scope when evaluating costs associated with Azure OpenAI.\nIf your resources are part of the same resource group you can scope Cost Analysis at\nthat level to understand the effect on costs. If your resources are spread across multiple\nresource groups you can scope to the subscription level.\nHowever, when scoped at a higher level you often need to add additional filters to be\nable to zero in on Azure OpenAI usage. When scoped at the subscription level we see a\nnumber of other resources that we may not care about in the context of Azure OpenAI\ncost management. When scoping at the subscription level, we recommend navigating to\nthe full Cost analysis t ool under the Cost Management  service. Search for \"Cost\nManagement\"  in the top Azure search bar to navigate to the full service experience,\nwhich includes more options like creating budgets.\n\uf80a\n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "75470e82d4e9-0", "text": "If you try to add a filter by service, you'll find that you can't find Azure OpenAI in the list.\nThis is because technically Azure OpenAI is part of Cognitive Services so the service level\nfilter is Cognitiv e Ser vices , but if you want to see all Azure OpenAI resources across a\nsubscription without any other type of Cognitive Services resources you need to instead\nscope to Service tier : Azur e OpenAI :\nYou can create budgets  to manage costs and create alerts  that automatically notify\nstakeholders of spending anomalies and overspending risks. Alerts are based on\nspending compared to budget and cost thresholds. Budgets and alerts are created for\nAzure subscriptions and resource groups, so they're useful as part of an overall cost\nmonitoring strategy.\n\uf80a\n\uf80a\nCreate budgets", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "aa7a37b7d756-0", "text": "Budgets can be created with filters for specific resources or services in Azure if you want\nmore granularity present in your monitoring. Filters help ensure that you don't\naccidentally create new resources that cost you additional money. For more information\nabout the filter options available when you create a budget, see Group and filter\noptions .\nYou can also export your cost data  to a storage account. This is helpful when you need\nor others to do additional data analysis for costs. For example, a finance team can\nanalyze the data using Excel or P ower BI. Y ou can export your costs on a daily, weekly, or\nmonthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.\uff09 Impor tant\nWhile OpenAI has an option for hard limits that will prevent you from going over\nyour budget, Azure OpenAI does not currently provide this functionality. Y ou are\nable to kick off automation from action groups as part of your budget notifications\nto take more advanced actions, but this requires additional custom development\non your part.\nExport cost data\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "dd7fe808dfa6-0", "text": "Tutorial: Explore Azu re OpenAI Service\nembeddings and document search\nArticle \u202206/14/2023\nThis tutorial will walk you through using the Azure OpenAI embeddings  API to perform\ndocument sear ch where you'll query a knowledge base to find the most relevant\ndocument.\nIn this tutorial, you learn how to:\nAn Azure subscription - Create one for free\nAccess granted to Azure OpenAI in the desired Azure subscription Currently,\naccess to this service is granted only by application. Y ou can apply for access to\nAzure OpenAI by completing the form at https://aka.ms/oai/access . Open an\nissue on this repo to contact us if you have an issue.\nPython 3.7.1 or later version\nThe following Python libraries: openai, num2words, matplotlib, plotly, scipy, scikit-\nlearn, pandas, tiktoken.\nJupyter NotebooksInstall Azure OpenAI and other dependent Python libraries.\uff02\nDownload the BillSum dataset and prepare it for analysis.\uff02\nCreate environment variables for your resources endpoint and API key.\uff02\nUse the text-embedding-ada-002 (V ersion 2)  model \uff02\nUse cosine similarity  to rank search results. \uff02\n\uff09 Impor tant\nWe strongly recommend using text-embedding-ada-002 (Version 2). This\nmodel/version provides parity with OpenAI's text-embedding-ada-002. To learn\nmore about the improvements offered by this model, please refer to OpenAI' s blog\npost . Even if you are currently using V ersion 1 you should migrate to V ersion 2 to\ntake advantage of the latest weights/updated token limit. V ersion 1 and V ersion 2\nare not interchangeable, so document embedding and document search must be\ndone using the same version of the model.\nPrerequisites", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c1c586b08d3a-0", "text": "An Azure OpenAI resource with the text-embedding-ada-002 (V ersion 2)  model\ndeployed. This model is currently only available in certain regions . If you don't have\na resource the process of creating one is documented in our resource deployment\nguide .\nIf you haven't already, you need to install the following libraries:\nWindows Command Prompt\nBillSum is a dataset of United S tates Congressional and California state bills. For\nillustration purposes, we'll look only at the US bills. The corpus consists of bills from the\n103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills\nand 3,269 test bills. The BillSum corpus focuses on mid-length legislation from 5,000 to\n20,000 characters in length. More information on the project and the original academic\npaper where this dataset is derived from can be found on the BillSum project's GitHub\nrepository\nThis tutorial uses the bill_sum_data.csv file that can be downloaded from our GitHub\nsample data .\nYou can also download the sample data by running the following command on your\nlocal machine:\nWindows Command Prompt\nTo successfully make a call against Azure OpenAI, you'll need an endpoint  and a key.Set up\nPython libraries\npip install openai num2words matplotlib plotly scipy scikit-learn pandas  \ntiktoken  \nDownload the BillSum dataset\ncurl \"https://raw.githubusercontent.com/Azure-Samples/Azure-OpenAI-Docs-\nSamples/main/Samples/Tutorials/Embeddings/data/bill_sum_data.csv\" --output  \nbill_sum_data.csv  \nRetrieve key and endpoint", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4841e86aa921-0", "text": "Variable\nnameValue\nENDPOINTThis value can be found in the Keys & Endpoint  section when examining your\nresource from the Azure portal. Alternatively, you can find the value in Azur e OpenAI\nStudio  > Playground  > Code View . An example endpoint is: https://docs-test-\n001.openai.azure.com.\nAPI-KEY This value can be found in the Keys & Endpoint  section when examining your\nresource from the Azure portal. Y ou can use either KEY1 or KEY2.\nGo to your resource in the Azure portal. The Endpoint and K eys can be found in the\nResour ce Management  section. Copy your endpoint and access key as you'll need both\nfor authenticating your API calls. Y ou can use either KEY1 or KEY2. Always having two\nkeys allows you to securely rotate and regenerate keys without causing a service\ndisruption.\nCreate and assign persistent environment variables for your key and endpoint.\nCMD\nCMD\nAfter setting the environment variables, you may need to close and reopen Jupyter\nnotebooks or whatever IDE you're using in order for the environment variables to be\naccessible. While we strongly recommend using Jupyter Notebooks, if for some reason\nyou cannot you'll need to modify any code that is returning a pandas dataframe by\nusing print(dataframe_name) rather than just calling the dataframe_name directly as is\noften done at the end of a code block.\nRun the following code in your preferred Python IDE:Environment variables\nCommand Line\nsetx AZURE_OPENAI_API_KEY \"REPLACE_WITH_YOUR_KEY_VALUE_HERE\"  \nsetx AZURE_OPENAI_ENDPOINT \"REPLACE_WITH_YOUR_ENDPOINT_HERE\"  \nImport libraries and list models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bc215efee56d-0", "text": "Python\nOutputimport openai \nimport os \nimport re \nimport requests  \nimport sys \nfrom num2words import num2words  \nimport os \nimport pandas as pd \nimport numpy as np \nfrom openai.embeddings_utils import get_embedding, cosine_similarity  \nimport tiktoken  \nAPI_KEY = os.getenv( \"AZURE_OPENAI_API_KEY\" )  \nRESOURCE_ENDPOINT = os.getenv( \"AZURE_OPENAI_ENDPOINT\" )  \nopenai.api_type = \"azure\" \nopenai.api_key = API_KEY  \nopenai.api_base = RESOURCE_ENDPOINT  \nopenai.api_version = \"2022-12-01\"\nurl = openai.api_base + \"/openai/deployments?api-version=2022-12-01\"   \nr = requests.get(url, headers={ \"api-key\" : API_KEY})  \nprint(r.text)  \n{ \n  \"data\": [  \n    { \n      \"scale_settings\": {  \n        \"scale_type\": \"standard\"  \n      },  \n      \"model\": \"text-embedding-ada-002\",  \n      \"owner\": \"organization-owner\",  \n      \"id\": \"text-embedding-ada-002\",  \n      \"status\": \"succeeded\",  \n      \"created_at\": 1657572678,  \n      \"updated_at\": 1657572678,  \n      \"object\": \"deployment\"  \n    }, \n    { \n      \"scale_settings\": {  \n        \"scale_type\": \"standard\"  \n      },  \n      \"model\": \"code-cushman-001\",  \n      \"owner\": \"organization-owner\",  \n      \"id\": \"code-cushman-001\",  \n      \"status\": \"succeeded\",", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bc215efee56d-1", "text": "\"status\": \"succeeded\",  \n      \"created_at\": 1657572712,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ac4c3972fa70-0", "text": "The output of this command will vary based on the number and type of models you've\ndeployed. In this case, we need to confirm that we have an entry for text-embedding-\nada-002 . If you find that you're missing this model, you'll need to deploy the model  to\nyour resource before proceeding.\nNow we need to read our csv file and create a pandas DataFrame. After the initial\nDataFrame is created, we can view the contents of the table by running df.\nPython\nOutput:      \"updated_at\": 1657572712,  \n      \"object\": \"deployment\"  \n    }, \n    { \n      \"scale_settings\": {  \n        \"scale_type\": \"standard\"  \n      },  \n      \"model\": \"text-search-curie-doc-001\",  \n      \"owner\": \"organization-owner\",  \n      \"id\": \"text-search-curie-doc-001\",  \n      \"status\": \"succeeded\",  \n      \"created_at\": 1668620345,  \n      \"updated_at\": 1668620345,  \n      \"object\": \"deployment\"  \n    }, \n    { \n      \"scale_settings\": {  \n        \"scale_type\": \"standard\"  \n      },  \n      \"model\": \"text-search-curie-query-001\",  \n      \"owner\": \"organization-owner\",  \n      \"id\": \"text-search-curie-query-001\",  \n      \"status\": \"succeeded\",  \n      \"created_at\": 1669048765,  \n      \"updated_at\": 1669048765,  \n      \"object\": \"deployment\"  \n    } \n  ], \n  \"object\": \"list\"  \n}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ac4c3972fa70-1", "text": "} \n  ], \n  \"object\": \"list\"  \n} \ndf=pd.read_csv(os.path.join(os.getcwd(), 'bill_sum_data.csv' )) # This assumes  \nthat you have placed the bill_sum_data.csv in the same directory you are  \nrunning Jupyter Notebooks  \ndf", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "23056a4c26fc-0", "text": "The initial table has more columns than we need we'll create a new smaller DataFrame\ncalled df_bills which will contain only the columns for text, summary, and title.\nPython\nOutput:\nNext we'll perform some light data cleaning by removing redundant whitespace and\ncleaning up the punctuation to prepare the data for tokenization.\n\uf80a\ndf_bills = df[[ 'text', 'summary' , 'title']] \ndf_bills  \n\uf80a", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b7ab931472be-0", "text": "Python\nNow we need to remove any bills that are too long for the token limit (8192 tokens).\nPython\nOutput\nWe'll once again examine df_bills .\nPythonpd.options.mode.chained_assignment = None #https://pandas.pydata.org/pandas-\ndocs/stable/user_guide/indexing.html#evaluation-order-matters  \n# s is input text  \ndef normalize_text (s, sep_token = \" \\n \"): \n    s = re.sub( r'\\s+',  ' ', s).strip()  \n    s = re.sub( r\". ,\",\"\",s) \n    # remove all instances of multiple spaces  \n    s = s.replace( \"..\",\".\") \n    s = s.replace( \". .\",\".\") \n    s = s.replace( \"\\n\", \"\") \n    s = s.strip()  \n     \n    return s \ndf_bills[ 'text']= df_bills[ \"text\"].apply( lambda x : normalize_text(x))  \ntokenizer = tiktoken.get_encoding( \"cl100k_base\" ) \ndf_bills[ 'n_tokens' ] = df_bills[ \"text\"].apply( lambda x: \nlen(tokenizer.encode(x)))  \ndf_bills = df_bills[df_bills.n_tokens< 8192] \nlen(df_bills)  \n20 \n\uff17 Note\nIn this case all bills are under the embedding model input token limit, but you can\nuse the technique above to remove entries that would otherwise cause embedding\nto fail. When faced with content that exceeds the embedding limit, you can also\nchunk the content into smaller pieces and then embed those one at a time.\ndf_bills", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9796e8a2b1c5-0", "text": "Output:\nTo understand the n_tokens column a little more as well how text ultimately is tokenized,\nit can be helpful to run the following code:\nPython\nFor our docs we're intentionally truncating the output, but running this command in\nyour environment will return the full text from index zero tokenized into chunks. Y ou can\nsee that in some cases an entire word is represented with a single token whereas in\nothers parts of words are split across multiple tokens.\nOutput\n\uf80a\nsample_encode = tokenizer.encode(df_bills.text[ 0])  \ndecode = tokenizer.decode_tokens_bytes(sample_encode)  \ndecode \n[b'SECTION',  \n b' ', \n b'1', \n b'.', \n b' SHORT',  \n b' TITLE',  \n b'.', \n b' This',  \n b' Act',  \n b' may',  \n b' be',  \n b' cited',", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e5358f3cfefc-0", "text": "If you then check the length of the decode variable, you'll find it matches the first\nnumber in the n_tokens column.\nPython\nOutput\nNow that we understand more about how tokenization works we can move on to\nembedding. It is important to note, that we haven't actually tokenized the documents b' as',  \n b' the',  \n b' ``',  \n b'National',  \n b' Science',  \n b' Education',  \n b' Tax',  \n b' In',  \n b'cent',  \n b'ive',  \n b' for',  \n b' Businesses',  \n b' Act',  \n b' of',  \n b' ', \n b'200',  \n b'7', \n b\"''.\",  \n b' SEC',  \n b'.', \n b' ', \n b'2', \n b'.', \n b' C', \n b'RED',  \n b'ITS',  \n b' FOR',  \n b' CERT',  \n b'AIN',  \n b' CONTRIBUT',  \n b'IONS',  \n b' BEN',  \n b'EF', \n b'IT', \n b'ING',  \n b' SC',  \nlen(decode)  \n1466", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d9545362c531-0", "text": "yet. The n_tokens column is simply a way of making sure none of the data we pass to\nthe model for tokenization and embedding exceeds the input token limit of 8,192. When\nwe pass the documents to the embeddings model, it will break the documents into\ntokens similar (though not necessarily identical) to the examples above and then convert\nthe tokens to a series of floating point numbers that will be accessible via vector search.\nThese embeddings can be stored locally or in an Azure Database. As a result, each bill\nwill have its own corresponding embedding vector in the new ada_v2 column on the\nright side of the DataFrame.\nPython\nPython\nOutput:\nAs we run the search code block below, we'll embed the search query \"Can I get\ninformation on cable c ompany tax revenue?\"  with the same text-embedding-ada-002\n(Version 2)  model. Next we'll find the closest bill embedding to the newly embedded\ntext from our query ranked by cosine similarity .\nPythondf_bills[ 'ada_v2' ] = df_bills[ \"text\"].apply( lambda x : get_embedding(x,  \nengine = 'text-embedding-ada-002' )) # engine should be set to the deployment  \nname you chose when you deployed the text-embedding-ada-002 (Version 2)  \nmodel \ndf_bills  \n\uf80a\n# search through the reviews for a specific product  \ndef search_docs (df, user_query, top_n= 3, to_print=True) :", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "332f85d68eb2-0", "text": "Output :\nFinally, we'll show the top result from document search based on user query against the\nentire knowledge base. This returns the top result of the \"T axpayer's Right to View Act of\n1993\". This document has a cosine similarity score of 0.76 between the query and the\ndocument:\nPython\nOutput    embedding = get_embedding(  \n        user_query,  \n        engine= \"text-embedding-ada-002\"  # engine should be set to the  \ndeployment name you chose when you deployed the text-embedding-ada-002  \n(Version 2) model  \n    ) \n    df[\"similarities\" ] = df.ada_v2.apply( lambda x: cosine_similarity(x,  \nembedding))  \n    res = (  \n        df.sort_values( \"similarities\" , ascending= False) \n        .head(top_n)  \n    ) \n    if to_print:  \n        display(res)  \n    return res \nres = search_docs(df_bills, \"Can I get information on cable company tax  \nrevenue?\" , top_n= 4) \n\uf80a\nres[\"summary\" ][9] \n\"Taxpayer's Right to View Act of 1993 - Amends the Communications Act of  \n1934 to prohibit a cable operator from assessing separate charges for any  \nvideo programming of a sporting, theatrical, or other entertainment event if  \nthat event is performed at a facility constructed, renovated, or maintained  \nwith tax revenues or by an organization that receives public financial  \nsupport. Authorizes the Federal Communications Commission and local  \nfranchising authorities to make determinations concerning the applicability  \nof such prohibition. Sets forth conditions under which a facility is  \nconsidered to have been constructed, maintained, or renovated with tax", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "332f85d68eb2-1", "text": "considered to have been constructed, maintained, or renovated with tax  \nrevenues. Considers events performed by nonprofit or public organizations  \nthat receive tax subsidies to be subject to this Act if the event is", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bb38bf784668-0", "text": "Using this approach, you can use embeddings as a search mechanism across documents\nin a knowledge base. The user can then take the top search result and use it for their\ndownstream task, which prompted their initial query.\nIf you created an OpenAI resource solely for completing this tutorial and want to clean\nup and remove an OpenAI resource, you'll need to delete your deployed models, and\nthen delete the resource or associated resource group if it's dedicated to your test\nresource. Deleting the resource group also deletes any other resources associated with\nit.\nPortal\nAzure CLI\nLearn more about Azure OpenAI's models:sponsored by, or includes the participation of a team that is part of, a tax  \nexempt organization.\"  \nClean up resources\nNext steps\nAzur e OpenAI Ser vice models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6d9a500f2323-0", "text": "Azure OpenAI speech to speech chat\nArticle \u202204/19/2023\nReference documentation  | Package (NuGet)  | Additional Samples on GitHub\nIn this how-to guide, you can use Azure Cognitive Services Speech  to converse with\nAzure OpenAI Service . The text recognized by the Speech service is sent to Azure\nOpenAI. The text response from Azure OpenAI is then synthesized by the Speech\nservice.\nSpeak into the microphone to start a conversation with Azure OpenAI.\nThe Speech service recognizes your speech and converts it into text (speech to\ntext).\nYour request as text is sent to Azure OpenAI.\nThe Speech service text to speech (T TS) feature synthesizes the response from\nAzure OpenAI to the default speaker.\nAlthough the experience of this example is a back-and-forth exchange, Azure OpenAI\ndoesn't remember the context of your conversation.\nThe Speech SDK is available as a NuGet package  and implements .NET S tandard 2.0.\nYou install the Speech SDK later in this guide, but first check the SDK installation guide\n\uff09 Impor tant\nTo complete the steps in this guide, access must be granted to Microsoft Azure\nOpenAI Service in the desired Azure subscription. Currently, access to this service is\ngranted only by application. Y ou can apply for access to Azure OpenAI by\ncompleting the form at https://aka.ms/o ai/access .\nPrerequisites\nAzure subscription - Create one for free \uff02\nCreate a Speech resource  in the Azure portal. \uff02\nGet the Speech resource key and region. After your Speech resource is deployed,\nselect Go to resour ce to view and manage keys. For more information about\nCognitive Services resources, see Get the keys for your resource .\uff02\nSet up the environment", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e7860253e48c-0", "text": "for any more requirements.\nThis example requires environment variables named OPEN_AI_KEY, OPEN_AI_ENDPOINT,\nSPEECH_KEY, and SPEECH_REGION.\nYour application must be authenticated to access Cognitive Services resources. For\nproduction, use a secure way of storing and accessing your credentials. For example,\nafter you get a key  for your Speech resource , write it to a new environment variable\non the local machine running the application.\nTo set the environment variables, open a console window, and follow the instructions for\nyour operating system and development environment.\nTo set the OPEN_AI_KEY environment variable, replace your-openai-key with one of\nthe keys for your resource.\nTo set the OPEN_AI_ENDPOINT environment variable, replace your-openai-endpoint\nwith one of the regions for your resource.\nTo set the SPEECH_KEY environment variable, replace your-speech-key with one of\nthe keys for your resource.\nTo set the SPEECH_REGION environment variable, replace your-speech-region with\none of the regions for your resource.\nConsoleSet environment variables\n\uea80 Tip\nDon't include the key directly in your code, and never post it publicly. See the\nCognitive Services security  article for more authentication options like Azur e Key\nVault.\nWindows\nsetx OPEN_AI_KEY your-openai-key  \nsetx OPEN_AI_ENDPOINT your-openai-endpoint  \nsetx SPEECH_KEY your-speech-key  \nsetx SPEECH_REGION your-speech-region  \n\uff17 Note", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2392af715489-0", "text": "After you add the environment variables, you may need to restart any running\nprograms that will need to read the environment variable, including the console\nwindow. For example, if you are using Visual S tudio as your editor, restart Visual\nStudio before running the example.\nFollow these steps to create a new console application.\n1. Open a command prompt where you want the new project, and create a console\napplication with the .NET CLI. The Program.cs file should be created in the project\ndirectory.\n.NET CLI\n2. Install the Speech SDK in your new project with the .NET CLI.\n.NET CLI\n3. Install the Azure OpenAI SDK (prerelease) in your new project with the .NET CLI.\n.NET CLI\n4. Replace the contents of Program.cs with the following code.\nC#If you only need to access the environment variable in the current running\nconsole, you can set the environment variable with set instead of setx.\nRecognize speech from a microphone\ndotnet new console  \ndotnet add package Microsoft.CognitiveServices.Speech  \ndotnet add package Azure.AI.OpenAI  --prerelease   \nusing System;  \nusing System.IO;  \nusing System.Threading.Tasks;  \nusing Microsoft.CognitiveServices.Speech;  \nusing Microsoft.CognitiveServices.Speech.Audio;  \nusing Azure; \nusing Azure.AI.OpenAI;", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e951d3afb070-0", "text": "using static System.Environment;  \nclass Program  \n{ \n    // This example requires environment variables named \"OPEN_AI_KEY\"  \nand \"OPEN_AI_ENDPOINT\"  \n    // Your endpoint should look like the following  \nhttps://YOUR_OPEN_AI_RESOURCE_NAME.openai.azure.com/  \n    static string openAIKey =  \nEnvironment.GetEnvironmentVariable( \"OPEN_AI_KEY\" ); \n    static string openAIEndpoint =  \nEnvironment.GetEnvironmentVariable( \"OPEN_AI_ENDPOINT\" ); \n    // Enter the deployment name you chose when you deployed the model.  \n    static string engine = \"text-davinci-002\" ; \n    // This example requires environment variables named \"SPEECH_KEY\"  \nand \"SPEECH_REGION\"  \n    static string speechKey =  \nEnvironment.GetEnvironmentVariable( \"SPEECH_KEY\" ); \n    static string speechRegion =  \nEnvironment.GetEnvironmentVariable( \"SPEECH_REGION\" ); \n    // Prompts Azure OpenAI with a request and synthesizes the  \nresponse.  \n    async static Task AskOpenAI (string prompt) \n    { \n        // Ask Azure OpenAI  \n        OpenAIClient client = new(new Uri(openAIEndpoint), new \nAzureKeyCredential(openAIKey));  \n        var completionsOptions = new CompletionsOptions()  \n        {  \n            Prompts = { prompt },\n            MaxTokens = 100, \n        };  \n        Response<Completions> completionsResponse =  \nclient.GetCompletions(engine, completionsOptions);  \n        string text = completionsResponse.Value.Choices[ 0].Text.Trim();  \n        Console.WriteLine( $\"Azure OpenAI response: {text}\"); \n        var speechConfig = SpeechConfig.FromSubscription(speechKey,  \nspeechRegion);", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e951d3afb070-1", "text": "var speechConfig = SpeechConfig.FromSubscription(speechKey,  \nspeechRegion);  \n        // The language of the voice that speaks.  \n        speechConfig.SpeechSynthesisVoiceName = \"en-US-\nJennyMultilingualNeural\" ;  \n        var audioOutputConfig = AudioConfig.FromDefaultSpeakerOutput();  \n        using (var speechSynthesizer = new \nSpeechSynthesizer(speechConfig, audioOutputConfig))  \n        {  \n            var speechSynthesisResult = await \nspeechSynthesizer.SpeakTextAsync(text).ConfigureAwait( true); \n            if (speechSynthesisResult.Reason ==  \nResultReason.SynthesizingAudioCompleted)", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "efa1cbf88300-0", "text": "{  \n                Console.WriteLine( $\"Speech synthesized to speaker for  \ntext: [{text}]\"); \n            }  \n            else if (speechSynthesisResult.Reason ==  \nResultReason.Canceled)  \n            {  \n                var cancellationDetails =  \nSpeechSynthesisCancellationDetails.FromResult(speechSynthesisResult);  \n                Console.WriteLine( $\"Speech synthesis canceled:  \n{cancellationDetails.Reason} \"); \n                if (cancellationDetails.Reason ==  \nCancellationReason.Error)  \n                {  \n                    Console.WriteLine( $\"Error details:  \n{cancellationDetails.ErrorDetails} \"); \n                }  \n            }  \n        }  \n    } \n    // Continuously listens for speech input to recognize and send as  \ntext to Azure OpenAI  \n    async static Task ChatWithOpenAI () \n    { \n        // Should be the locale for the speaker's language.  \n        var speechConfig = SpeechConfig.FromSubscription(speechKey,  \nspeechRegion);         \n        speechConfig.SpeechRecognitionLanguage = \"en-US\"; \n        using var audioConfig =  \nAudioConfig.FromDefaultMicrophoneInput();  \n        using var speechRecognizer = new SpeechRecognizer(speechConfig,  \naudioConfig);  \n        var conversationEnded = false; \n        while(!conversationEnded)\n        {  \n            Console.WriteLine( \"Azure OpenAI is listening. Say 'Stop' or  \npress Ctrl-Z to end the conversation.\" ); \n            // Get audio from the microphone and then send it to the  \nTTS service.  \n            var speechRecognitionResult = await \nspeechRecognizer.RecognizeOnceAsync();            \n            switch (speechRecognitionResult.Reason)  \n            {  \n                case ResultReason.RecognizedSpeech:  \n                    if (speechRecognitionResult.Text == \"Stop.\")", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "efa1cbf88300-1", "text": "if (speechRecognitionResult.Text == \"Stop.\") \n                    {  \n                        Console.WriteLine( \"Conversation ended.\" ); \n                        conversationEnded = true; \n                    }", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "795472bf5116-0", "text": "5. To increase or decrease the number of tokens returned by Azure OpenAI, change\nthe MaxTokens property in the CompletionsOptions class instance. For more\ninformation tokens and cost implications, see Azure OpenAI tokens  and Azure\nOpenAI pricing .\nRun your new console application to start speech recognition from a microphone:\nConsole                    else \n                    {  \n                        Console.WriteLine( $\"Recognized speech:  \n{speechRecognitionResult.Text} \"); \n                        await \nAskOpenAI(speechRecognitionResult.Text).ConfigureAwait( true); \n                    }  \n                    break; \n                case ResultReason.NoMatch:  \n                    Console.WriteLine( $\"No speech could be recognized:  \n\"); \n                    break; \n                case ResultReason.Canceled:  \n                    var cancellationDetails =  \nCancellationDetails.FromResult(speechRecognitionResult);  \n                    Console.WriteLine( $\"Speech Recognition canceled:  \n{cancellationDetails.Reason} \"); \n                    if (cancellationDetails.Reason ==  \nCancellationReason.Error)  \n                    {  \n                        Console.WriteLine( $\"Error details=\n{cancellationDetails.ErrorDetails} \"); \n                    }  \n                    break; \n            }  \n        }  \n    } \n    async static Task Main(string[] args) \n    { \n        try \n        {  \n            await ChatWithOpenAI().ConfigureAwait( true); \n        }  \n        catch (Exception ex)  \n        {  \n            Console.WriteLine(ex.Message);  \n        }  \n    } \n} \ndotnet run", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a9e2c2eaae5f-0", "text": "Speak into your microphone when prompted. The console output includes the prompt\nfor you to begin speaking, then your request as text, and then the response from Azure\nOpenAI as text. The response from Azure OpenAI should be converted from text to\nspeech and then output to the default speaker.\nConsole\nNow that you've completed the quickstart, here are some more considerations:\nTo change the speech recognition language, replace en-US with another supported\nlanguage . For example, es-ES for Spanish (Spain). The default language is en-US if\uff09 Impor tant\nMake sure that you set the OPEN_AI_KEY, OPEN_AI_ENDPOINT, SPEECH__KEY and\nSPEECH__REGION environment variables as described previously . If you don't set\nthese variables, the sample will fail with an error message.\nPS C:\\dev\\openai\\csharp> dotnet run  \nAzure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the  \nconversation.  \nRecognized speech:Make a comma separated list of all continents.  \nAzure OpenAI response:Africa, Antarctica, Asia, Australia, Europe, North  \nAmerica, South America  \nSpeech synthesized to speaker for text [Africa, Antarctica, Asia, Australia,  \nEurope, North America, South America]  \nAzure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the  \nconversation.  \nRecognized speech: Make a comma separated list of 1 Astronomical observatory  \nfor each continent. A list should include each continent name in  \nparentheses.  \nAzure OpenAI response:Mauna Kea Observatories (North America), La Silla  \nObservatory (South America), Tenerife Observatory (Europe), Siding Spring  \nObservatory (Australia), Beijing Xinglong Observatory (Asia), Naukluft", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a9e2c2eaae5f-1", "text": "Observatory (Australia), Beijing Xinglong Observatory (Asia), Naukluft  \nPlateau Observatory (Africa), Rutherford Appleton Laboratory (Antarctica)  \nSpeech synthesized to speaker for text [Mauna Kea Observatories (North  \nAmerica), La Silla Observatory (South America), Tenerife Observatory  \n(Europe), Siding Spring Observatory (Australia), Beijing Xinglong  \nObservatory (Asia), Naukluft Plateau Observatory (Africa), Rutherford  \nAppleton Laboratory (Antarctica)]\nAzure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the  \nconversation.  \nConversation ended.  \nPS C:\\dev\\openai\\csharp>  \nRemarks", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0bf207b1237b-0", "text": "you don't specify a language. For details about how to identify one of multiple\nlanguages that might be spoken, see language identification .\nTo change the voice that you hear, replace en-US-JennyMultilingualNeural with\nanother supported voice . If the voice doesn't speak the language of the text\nreturned from Azure OpenAI, the Speech service doesn't output synthesized audio.\nTo use a different model , replace text-davinci-002 with the ID of another\ndeployment . Keep in mind that the deployment ID isn't necessarily the same as the\nmodel name. Y ou named your deployment when you created it in Azure OpenAI\nStudio .\nAzure OpenAI also performs content moderation on the prompt inputs and\ngenerated outputs. The prompts or responses may be filtered if harmful content is\ndetected. For more information, see the content filtering  article.\nYou can use the Azure portal  or Azure Command Line Interface (CLI)  to remove the\nSpeech resource you created.\nLearn more about Speech\nLearn more about Azure OpenAI\nClean up resources\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ba283c1cac01-0", "text": "Overview of Responsible AI practices for\nAzure OpenAI models\nArticle \u202205/19/2023\nMany of the Azure OpenAI models are generative AI models that have demonstrated\nimprovements in advanced capabilities such as content and code generation,\nsummarization, and search. With many of these improvements also come increased\nresponsible AI challenges related to harmful content, manipulation, human-like\nbehavior, privacy, and more. For more information about the capabilities, limitations and\nappropriate use cases for these models, please review the Transparency Note .\nIn addition to the T ransparency Note, we have created technical recommendations and\nresources to help customers design, develop, deploy, and use AI systems that\nimplement the Azure OpenAI models responsibly. Our recommendations are grounded\nin the Microsoft R esponsible AI S tandard , which sets policy requirements that our own\nengineering teams follow. Much of the content of the S tandard follows a pattern, asking\nteams to Identify, Measure, and Mitigate potential harms, and plan for how to Operate\nthe AI system as well. In alignment with those practices, these recommendations are\norganized into four stages:\n1. Identif y : Identify and prioritize potential harms that could result from your AI\nsystem through iterative red-teaming, stress-testing, and analysis.\n2. Measur e : Measure the frequency and severity of those harms by establishing clear\nmetrics, creating measurement test sets, and completing iterative, systematic\ntesting (both manual and automated).\n3. Mitigat e : Mitigate harms by implementing tools and strategies such as prompt\nengineering  and using our content filters . Repeat measurement to test\neffectiveness after implementing mitigations.\n4. Operat e : Define and execute a deployment and operational readiness plan.\nIn addition to their correspondence to the Microsoft R esponsible AI S tandard, these\nstages correspond closely to the functions in the NIST AI Risk Management", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ba283c1cac01-1", "text": "stages correspond closely to the functions in the NIST AI Risk Management\nFramework .\nIdentifying potential harms that could occur in or be caused by an AI system is the first\nstage of the R esponsible AI lifecycle. The earlier you begin to identify potential harms,\nthe more effective you can be at mitigating the harms. When assessing potential harms,\nit is important to develop an understanding of the types of harms that could result from\nIdentify", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "eb897abcf629-0", "text": "using the Azure OpenAI Service in your specific context(s). In this section, we provide\nrecommendations and resources you can use to identify harms through an impact\nassessment, iterative red team testing, stress-testing, and analysis. R ed teaming and\nstress-testing are approaches where a group of testers come together and intentionally\nprobe a system to identify its limitations, risk surface, and vulnerabilities.\nThese steps have the goal of producing a prioritized list of potential harms for each\nspecific scenario.\n1. Identif y harms that ar e relevant for your specific model, application, and\ndeployment scenario.\na. Identify potential harms associated with the model and model capabilities (for\nexample, GPT-3 model vs GPT-4 model) that you're using in your system. This is\nimportant to consider because each model has different capabilities, limitations,\nand risks, as described more fully in the sections above.\nb. Identify any other harms or increased scope of harm presented by the intended\nuse of the system you're developing. Consider using a Responsible AI Impact\nAssessment  to identify potential harms.\ni. For example, let's consider an AI system that summarizes text. Some uses of\ntext generation are lower risk than others. For example, if the system is to be\nused in a healthcare domain for summarizing doctor's notes, the risk of harm\narising from inaccuracies is higher than if the system is summarizing online\narticles.\n2. Prioritize harms b ased on elements o f risk such as fr equency and sev erity. Assess\nthe level of risk for each harm and the likelihood of each risk occurring in order to\nprioritize the list of harms you've identified. Consider working with subject matter\nexperts and risk managers within your organization and with relevant external\nstakeholders when appropriate.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "eb897abcf629-1", "text": "experts and risk managers within your organization and with relevant external\nstakeholders when appropriate.\n3. Conduct r ed team t esting and str ess testing  starting with the highest priority\nharms, to develop a better understanding of whether and how the identified harms\nare actually occurring in your scenario, as well as to identify new harms you didn't\ninitially anticipate.\n4. Share this information with r elevant stak eholder s using your organization's\ninternal compliance processes.\nAt the end of this Identify stage, you should have a documented, prioritized list of\nharms. When new harms and new instances of harms emerge through further testing\nand use of the system, you can update and improve this list by following the above\nprocess again.\nMeasure", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3fd9e40aa62c-0", "text": "Once a list of prioritized harms has been identified, the next stage involves developing\nan approach for systematic measurement of each harm and conducting evaluations of\nthe AI system. There are manual and automated approaches to measurement. W e\nrecommend you do both, starting with manual measurement.\nManual measurement is useful for:\n1. Measuring progress on a small set of priority issues. When mitigating specific\nharms, it's often most productive to keep manually checking progress against a\nsmall dataset until the harm is no longer observed before moving to automated\nmeasurement.\n2. Defining and reporting metrics until automated measurement is reliable enough to\nuse alone.\n3. Spot-checking periodically to measure the quality of automatic measurement.\nAutomated measurement is useful for:\n1. Measuring at a large scale with increased coverage to provide more\ncomprehensive results.\n2. Ongoing measurement to monitor for any regression as the system, usage, and\nmitigations evolve.\nBelow, we provide specific recommendations to measure your AI system for potential\nharms. W e recommend you first complete this process manually and then develop a\nplan to automate the process:\n1. Create inputs that ar e likely to produce each prioritized harm:  Create\nmeasurement set(s) by generating many diverse examples of targeted inputs that\nare likely to produce each prioritized harm.\n2. Generat e System Outputs:  Pass in the examples from the measurement sets as\ninputs to the system to generate system outputs. Document the outputs.\n3. Evaluat e System Outputs and R epor t Results t o Relevant S takeholder s\na. Define clear metric(s).  For each intended use of your system, establish metrics\nthat measure the frequency and degree of severity of each potentially harmful\noutput. Create clear definitions to classify outputs that will be considered\nharmful or problematic in the context of your system and scenario, for each", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3fd9e40aa62c-1", "text": "harmful or problematic in the context of your system and scenario, for each\ntype of prioritized harm you identified.\nb. Assess the outputs  against the clear metric definitions and record and quantify\nthe occurrences of harmful outputs. R epeat the measurements periodically, to\nassess mitigations and monitor for any regression.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "409208270da4-0", "text": "c. Share this information with r elevant stak eholder s using your organization's\ninternal compliance processes.\nAt the end of this measurement stage, you should have a defined measurement\napproach to benchmark how your system performs for each potential harm as well as an\ninitial set of documented results. As you continue implementing and testing mitigations,\nthe metrics and measurement sets should continue to be refined (for example, to add\nmetrics for new harms that were initially unanticipated) and the results updated.\nMitigating harms presented by large language models such as the Azure OpenAI\nmodels requires an iterative, layered approach that includes experimentation and\ncontinual measurement. W e recommend developing a mitigation plan that\nencompasses four layers of mitigations for the harms identified in the earlier stages of\nthis process:\n1. At the model lev el, it's important to understand the model(s) you'll be using and\nwhat fine-tuning steps may have been taken by the model developers to align the\nmodel towards its intended uses and to reduce the risk of potentially harmful uses\nand outcomes.\na. For example, for GPT-4, model developers have been able to use reinforcement\nlearning methods as a responsible AI tool to better align the model towards the\ndesigners' intended goals.\n2. At the safety syst em lev el, you should understand the platform level mitigations\nthat have been implemented. such as the Azure OpenAI content filters  which helpMitigate", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e60e541216b8-0", "text": "to block the output of harmful content.\n3. At the application lev el, application developers can implement metaprompt and\nuser-centered design and user experience mitigations. Metaprompts are\ninstructions provided to the model to guide its behavior; their use can make a\ncritical difference in guiding the system to behave in accordance with your\nexpectations. User-centered design and user experience (UX) interventions are also\nkey mitigation tools to prevent misuse and overreliance on AI.\n4. At the positioning lev el, there are many ways to educate the people who will use\nor be affected by your system about its capabilities and limitations.\nBelow, we provide specific recommendations to implement mitigations at the different\nlayers. Not all of these mitigations are appropriate for every scenario, and conversely,\nthese mitigations may be insufficient for some scenarios. Give careful consideration to\nyour scenario and the prioritized harms you identified, and as you implement\nmitigations, develop a process to measur e and document their effectiv eness  for your\nsystem and scenario.\n1. Model lev el Mitigations:  Review and identify which Azure OpenAI base model is\nbest suited for the system you're building and educate yourself about its\ncapabilities, limitations, and any measures taken to reduce the risk of the potential\nharms you've identified. For example, if you're using GPT-4, in addition to reading\nthis T ransparency Note, you can review OpenAI's GPT-4 S ystem Card  explaining\nthe safety challenges presented by the model and the safety processes that\nOpenAI adopted to prepare GPT-4 for deployment. It may be worth experimenting\nwith different versions of the model(s) (including through red teaming and\nmeasuring) to see how the harms present differently.\n2. Safety S ystem Lev el Mitigations:  Identify and evaluate the effectiveness of", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e60e541216b8-1", "text": "2. Safety S ystem Lev el Mitigations:  Identify and evaluate the effectiveness of\nplatform level solutions such as the Azure OpenAI content filters  to help mitigate\nthe potential harms that you have identified.\n3. Application Lev el Mitigations:  Prompt engineering, including metapr ompt tuning,\ncan be an effectiv e mitigation  for many different types of harm. R eview and\nimplement metaprompt (also called the \"system message\" or \"system prompt\")\nguidance and best practices documented here.\nWe recommend implementing the following user-centered design and user\nexperience (UX) interventions, guidance, and best practices to guide users to use\nthe system as intended and to prevent overreliance on the AI system:\na. Review and edit int erventions:  Design the user experience (UX) to encourage\npeople who use the system to review and edit the AI-generated outputs before\naccepting them (see HAX G9 : Support efficient correction).", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "73164623539d-0", "text": "b. Highlight pot ential inaccuracies in the AI-generat ed outputs  (see HAX G2 :\nMake clear how well the system can do what it can do), both when users first\nstart using the system and at appropriate times during ongoing use. In the first\nrun experience (FRE), notify users that AI-generated outputs may contain\ninaccuracies and that they should verify information. Throughout the\nexperience, include reminders to check AI-generated output for potential\ninaccuracies, both overall and in relation to specific types of content the system\nmay generate incorrectly. For example, if your measurement process has\ndetermined that your system has lower accuracy with numbers, mark numbers\nin generated outputs to alert the user and encourage them to check the\nnumbers or seek external sources for verification.\nc. User r esponsibility . Remind people that they are accountable for the final\ncontent when they're reviewing AI-generated content. For example, when\noffering code suggestions, remind the developer to review and test suggestions\nbefore accepting.\nd. Disclose AI' s role in the int eraction.  Make people aware that they are\ninteracting with an AI system (as opposed to another human). Where\nappropriate, inform content consumers that content has been partly or fully\ngenerated by an AI model; such notices may be required by law or applicable\nbest practices, and can reduce inappropriate reliance on AI-generated outputs\nand can help consumers use their own judgment about how to interpret and act\non such content.\ne. Prevent the syst em fr om anthr opomorphizing.  AI models may output content\ncontaining opinions, emotive statements, or other formulations that could imply\nthat they're human-like, that could be mistaken for a human identity, or that\ncould mislead people to think that a system has certain capabilities when it\ndoesn't. Implement mechanisms that reduce the risk of such outputs or", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "73164623539d-1", "text": "doesn't. Implement mechanisms that reduce the risk of such outputs or\nincorporate disclosures to help prevent misinterpretation of outputs.\nf. Cite references and information sour ces. If your system generates content\nbased on references sent to the model, clearly citing information sources helps\npeople understand where the AI-generated content is coming from.\ng. Limit the length o f inputs and outputs, wher e appr opriat e. Restricting input\nand output length can reduce the likelihood of producing undesirable content,\nmisuse of the system beyond its intended uses, or other harmful or unintended\nuses.\nh. Structur e inputs and/or syst em outputs.  Use prompt engineering  techniques\nwithin your application to structure inputs to the system to prevent open-ended\nresponses. Y ou can also limit outputs to be structured in certain formats or\npatterns. For example, if your system generates dialog for a fictional character in", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "94e4b13af94d-0", "text": "response to queries, limit the inputs so that people can only query for a\npredetermined set of concepts.\ni. Prepare pre-det ermined r esponses.  There are certain queries to which a model\nmay generate offensive, inappropriate, or otherwise harmful responses. When\nharmful or offensive queries or responses are detected, you can design your\nsystem to deliver a predetermined response to the user. Predetermined\nresponses should be crafted thoughtfully. For example, the application can\nprovide prewritten answers to questions such as \"who/what are you?\" to avoid\nhaving the system respond with anthropomorphized responses. Y ou can also\nuse predetermined responses for questions like, \"What are your terms of use?\"\nto direct people to the correct policy.\nj. Restrict aut omatic posting on social media.  Limit how people can automate\nyour product or service. For example, you may choose to prohibit automated\nposting of AI-generated content to external sites (including social media), or to\nprohibit the automated execution of generated code.\nk. Bot det ection.  Devise and implement a mechanism to prohibit users from\nbuilding an API on top of your product.\n4. Positioning Lev el Mitigations:\na. Be appr opriat ely transp arent. It's important to provide the right level of\ntransparency to people who use the system, so that they can make informed\ndecisions around the use of the system.\nb. Provide syst em documentation.  Produce and provide educational materials for\nyour system, including explanations of its capabilities and limitations. For\nexample, this could be in the form of a \"learn more\" page accessible via the\nsystem.\nc. Publish user guidelines and best practices.  Help users and stakeholders use the\nsystem appropriately by publishing best practices, for example on prompt", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "94e4b13af94d-1", "text": "system appropriately by publishing best practices, for example on prompt\ncrafting, reviewing generations before accepting them, etc. Such guidelines can\nhelp people understand how the system works. When possible, incorporate the\nguidelines and best practices directly into the UX.\nAs you implement mitigations to address potential identified harms, it's important to\ndevelop a process for ongoing measurement of the effectiveness of such mitigations, to\ndocument measurement results, and to review those measurement results to continually\nimprove the system.\nOnce measurement and mitigation systems are in place, we recommend that you define\nand execute a deployment and operational readiness plan. This stage includesOperate", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6a197efa18fa-0", "text": "completing appropriate reviews of your system and mitigation plans with relevant\nstakeholders, establishing pipelines to collect telemetry and feedback, and developing\nan incident response and rollback plan.\nSome recommendations for how to deploy and operate a system that uses the Azure\nOpenAI service with appropriate, targeted harms mitigations include:\n1. Work with compliance teams within your organization to understand what types of\nreviews are required for your system and when they are required (for example,\nlegal review, privacy review, security review, accessibility review, etc.).\n2. Develop and implement the following:\na. Develop a phased deliv ery plan.  We recommend you launch systems using the\nAzure OpenAI service gradually using a \"phased delivery\" approach. This gives a\nlimited set of people the opportunity to try the system, provide feedback, report\nissues and concerns, and suggest improvements before the system is released\nmore widely. It also helps to manage the risk of unanticipated failure modes,\nunexpected system behaviors, and unexpected concerns being reported.\nb. Develop an incident r esponse plan.  Develop an incident response plan and\nevaluate the time needed to respond to an incident.\nc. Develop a r ollback plan  Ensure you can roll back the system quickly and\nefficiently in case an unanticipated incident occurs.\nd. Prepare for immediat e action for unanticip ated harms.  Build the necessary\nfeatures and processes to block problematic prompts and responses as they're\ndiscovered and as close to real-time as possible. When unanticipated harms do\noccur, block the problematic prompts and responses as quickly as possible,\ndevelop and deploy appropriate mitigations, investigate the incident, and\nimplement a long-term solution.\ne. Develop a mechanism t o block people who ar e misusing y our syst em.\nDevelop a mechanism to identify users who violate your content policies (for", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6a197efa18fa-1", "text": "Develop a mechanism to identify users who violate your content policies (for\nexample, by generating hate speech) or are otherwise using your system for\nunintended or harmful purposes, and take action against further abuse. For\nexample, if a user frequently uses your system to generate content that is\nblocked or flagged by content safety systems, consider blocking them from\nfurther use of your system. Implement an appeal mechanism where appropriate.\nf. Build effectiv e user feedb ack channels.  Implement feedback channels through\nwhich stakeholders (and the general public, if applicable) can submit feedback\nor report issues with generated content or that otherwise arise during their use\nof the system. Document how such feedback is processed, considered, and\naddressed. Evaluate the feedback and work to improve the system based on\nuser feedback. One approach could be to include buttons with generated\ncontent that would allow users to identify content as \"inaccurate,\" \"harmful\" or", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1a45c312d2f6-0", "text": "\"incomplete.\" This could provide a more widely used, structured and feedback\nsignal for analysis.\ng. Telemetr y data.  Identify and record (consistent with applicable privacy laws,\npolicies, and commitments) signals that indicate user satisfaction or their ability\nto use the system as intended. Use telemetry data to identify gaps and improve\nthe system.\nThis document is not intended to be, and should not be construed as providing, legal\nadvice. The jurisdiction in which you're operating may have various regulatory or legal\nrequirements that apply to your AI system. Consult a legal specialist if you are uncertain\nabout laws or regulations that might apply to your system, especially if you think those\nmight impact these recommendations. Be aware that not all of these recommendations\nand resources are appropriate for every scenario, and conversely, these\nrecommendations and resources may be insufficient for some scenarios.\nMicrosoft AI principles\nMicrosoft responsible AI resources\nMicrosoft Azure Learning courses on responsible AI\nLimited access to Azure OpenAI Service - Azure Cognitive Services | Microsoft\nLearn\nCode of Conduct for the Azure OpenAI Service | Microsoft Learn\nData, privacy, and security for Azure OpenAI Service - Azure Cognitive Services |\nMicrosoft LearnLearn more about responsible AI\nLearn more about Azure OpenAI", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a23f566d20f8-0", "text": "Transparency Note for Azure OpenAI\nService\nArticle \u202205/19/2023\nAn AI system includes not only the technology, but also the people who use it, the\npeople who are affected by it, and the environment in which it's deployed. Creating a\nsystem that is fit for its intended purpose requires an understanding of how the\ntechnology works, what its capabilities and limitations are, and how to achieve the best\nperformance. Microsoft's T ransparency Notes are intended to help you understand how\nour AI technology works, the choices system owners can make that influence system\nperformance and behavior, and the importance of thinking about the whole system,\nincluding the technology, the people, and the environment. Y ou can use T ransparency\nNotes when developing or deploying your own system, or share them with the people\nwho will use or be affected by your system.\nMicrosoft\u2019s T ransparency Notes are part of a broader effort at Microsoft to put our AI\nPrinciples into practice. T o find out more, see the Microsoft's AI principles .\nAzure OpenAI provides customers with a fully managed AI service that lets developers\nand data scientists apply OpenAI's powerful models including models that can generate\nnatural language, code, and images. Within the Azure OpenAI Service, the OpenAI\nmodels are integrated with Microsoft-developed content filtering and abuse detection\nmodels. Learn more about content filtering here and abuse detection here.\nSelect the tabs to see content for the relevant model type.\nAs part of the fully managed Azure OpenAI Service, the GPT-3 models analyze and\ngenerate natural language, Codex models analyze and generate code and plain text\ncode commentary, and the GPT-4 models can understand and generate natural\nlanguage and code. These models use an autoregressive architecture, meaning theyWhat is a Transparency Note?\nThe basics of the Azure OpenAI Models\nIntroduction\nText and code models", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "52e195bbb92e-0", "text": "use data from prior observations to predict the most probable next word. This\nprocess is then repeated by appending the newly generated content to the original\ntext to produce the complete generated response. Because the response is\nconditioned on the input text, these models can be applied to various tasks simply\nby changing the input text.\nThe GPT-3 series of models are pretrained on a wide body of publicly available free\ntext data. This data is sourced from a combination of web crawling (specifically, a\nfiltered version of Common Crawl , which includes a broad range of text from the\ninternet and comprises 60 percent of the weighted pretraining dataset) and higher-\nquality datasets, including an expanded version of the W ebText dataset, two\ninternet-based books corpora and English-language Wikipedia. The GPT-4 base\nmodel was trained using publicly available data (such as internet data) and data\nthat was licensed by OpenAI. The model was fine-tuned using reinforcement\nlearning with human feedback (RLHF).\nLearn more about the training and modeling techniques in OpenAI's GPT-3 , GPT-\n4, and Codex  research papers. The guidance below is also drawn from OpenAI's\nsafety best practices .\nTerm Definition\nPrompt The text you send to the service in the API call. This text is then input into the\nmodel. For example, one might input the following prompt:  \nConvert the questions to a command: \nQ: Ask Constance if we need some bread \nA: send-msg 'find constance' Do we need some bread? \nQ: Send a message to Greg to figure out if things are ready for\nWednesday. \nA:\nCompletion\nor\nGenerationThe text Azure OpenAI outputs in response. For example, the service may\nrespond with the following answer to the above prompt: send-msg 'find", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "52e195bbb92e-1", "text": "respond with the following answer to the above prompt: send-msg 'find\ngreg' figure out if things are ready for Wednesday.\nToken Azure OpenAI processes text by breaking it down into tokens. T okens can be\nwords or just chunks of characters. For example, the word hamburger gets\nbroken up into the tokens ham, bur and ger, while a short and common word\nlike pear is a single token. Many tokens start with a whitespace, for example\nhello and bye.\nKey terms", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6fbe72a026a6-0", "text": "The GPT-4, GPT-3, and Codex Azure OpenAI Service models use natural language\ninstructions and examples in the prompt to identify the task. The model then\ncompletes the task by predicting the most probable next text. This technique is\nknown as \"in-context\" learning. These models are not retrained during this step but\ninstead give predictions based on the context you include in the prompt.\nThere are three main approaches for in-context learning. These approaches vary\nbased on the amount of task-specific data that is given to the model:\nFew-shot  : In this case, a user includes several examples in the prompt that\ndemonstrate the expected answer format and content. The following example\nshows a few-shot prompt providing multiple examples:\nThe number of examples typically ranges from 0 to 100 depending on how many\ncan fit in the maximum input length for a single prompt. Few-shot learning enables\na major reduction in the amount of task-specific data required for accurate\npredictions.\nOne-shot  : This case is the same as the few-shot approach except only one example\nis provided. The following example shows a one-shot prompt:Capabilities\nText and code models\nConvert the questions to a command:  \nQ: Ask Constance if we need some bread  \nA: send-msg `find constance` Do we need some bread?  \nQ: Send a message to Greg to figure out if things areready for  \nWednesday.  \nA: send-msg `find greg` Is everything ready forWednesday?  \nQ: Ask Ilya if we're still having our meeting thisevening  \nA: send-msg `find ilya` Are we still having a meetingthis evening?  \nQ: Contact the ski store and figure out if I can getmy skis fixed before  \nI leave on Thursday", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6fbe72a026a6-1", "text": "I leave on Thursday  \nA: send-msg `find ski store` Would it be possible toget my skis fixed  \nbefore I leave on Thursday?  \nQ: Thank Nicolas for lunch  \nA: send-msg `find nicolas` Thank you for lunch!  \nQ: Tell Constance that I won't be home before 19:30tonight \u2014 unmovable  \nmeeting.  \nA: send-msg `find constance` I won't be home before19:30 tonight. I have  \na meeting I can't move.  \nQ: Tell John that I need to book an appointment at10:30  \nA:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "66dbddf6b358-0", "text": "Zero-shot : In this case, no examples are provided to the model and only the task\nrequest is provided. The following example shows a zero-shot prompt:\nThe GPT-4, GPT-3, and Codex models in the Azure OpenAI service can be used in\nmultiple scenarios. The following list isn't comprehensive, but it illustrates the\ndiversity of tasks that can be supported with appropriate mitigations:\nChat and conv ersation int eraction  : Users can interact with a conversational\nagent that responds with responses drawn from trusted documents such as\ninternal company documentation or tech support documentation.\nConversations must be limited to answering scoped questions.\nChat and conv ersation cr eation  : Users can create a conversational agent that\nresponds with responses drawn from trusted documents such as internal\ncompany documentation or tech support documentation. Conversations must\nbe limited to answering scoped questions.\nCode generation or transformation scenarios  : For example, converting one\nprogramming language to another, generating docstrings for functions,\nconverting natural language to SQL.Convert the questions to a command:  \nQ: Ask Constance if we need some bread  \nA: send-msg `find constance` Do we need some bread?  \nQ: Send a message to Greg to figure out if things are ready for  \nWednesday.  \nA: \nConvert the question to a command:  \nQ: Ask Constance if we need some bread  \nA: \nUse cases\nText and code models\nIntended uses", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d646093fa1dd-0", "text": "Journalistic cont ent : For use to create new journalistic content or to rewrite\njournalistic content submitted by the user as a writing aid for predefined\ntopics. Users cannot use the application as a general content creation tool for\nall topics. May not be used to generate content for political campaigns.\nQuestion-answ ering  : Users can ask questions and receive answers from\ntrusted source documents such as internal company documentation. The\napplication does not generate answers ungrounded in trusted source\ndocumentation.\nReason ov er structur ed and unstructur ed data  : Users can analyze inputs\nusing classification, sentiment analysis of text, or entity extraction. Examples\ninclude analyzing product feedback sentiment, analyzing support calls and\ntranscripts, and refining text-based search with embeddings.\nSearch : Users can search trusted source documents such as internal company\ndocumentation. The application does not generate results ungrounded in\ntrusted source documentation.\nSummarization  : Users can submit content to be summarized for predefined\ntopics built into the application and cannot use the application as an open-\nended summarizer. Examples include summarization of internal company\ndocumentation, call center transcripts, technical reports, and product reviews.\nWriting assistance on specific t opics  : Users can create new content or rewrite\ncontent submitted by the user as a writing aid for business content or pre-\ndefined topics. Users can only rewrite or create content for specific business\npurposes or predefined topics and cannot use the application as a general\ncontent creation tool for all topics. Examples of business content include\nproposals and reports. For journalistic use, see above Journalistic cont ent use\ncase.\nWe encourage customers to use the Azure OpenAI GPT-4, GPT-3, and Codex\nmodels in their innovative solutions or applications as approved in their Limited\nAccess registration form . However, here are some considerations when choosing a\nuse case:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d646093fa1dd-1", "text": "Access registration form . However, here are some considerations when choosing a\nuse case:\nNot suitable for open-ended, unconstrained cont ent generation.  Scenarios\nwhere users can generate content on any topic are more likely to produce\noffensive or harmful text. The same is true of longer generations.\nNot suitable for scenarios wher e up-t o-dat e, factually accurat e information\nis crucial  unless you have human reviewers or are using the models to search\nyour own documents and have verified suitability for your scenario. The\nservice does not have information about events that occur after its trainingConsiderations when choosing a use case", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3bc23a6e096c-0", "text": "date, likely has missing knowledge about some topics, and may not always\nproduce factually accurate information.\nAvoid scenarios wher e use or misuse o f the syst em could r esult in significant\nphysical or psy chological injur y to an individual.  For example, scenarios that\ndiagnose patients or prescribe medications have the potential to cause\nsignificant harm.\nAvoid scenarios wher e use or misuse o f the syst em could hav e a\nconsequential imp act on life oppor tunities or legal status.  Examples include\nscenarios where the AI system could affect an individual's legal status, legal\nrights, or their access to credit, education, employment, healthcare, housing,\ninsurance, social welfare benefits, services, opportunities, or the terms on\nwhich they're provided.\nAvoid high stak es scenarios that could lead t o harm.  The models hosted by\nAzure OpenAI service reflect certain societal views, biases, and other\nundesirable content present in the training data or the examples provided in\nthe prompt. As a result, we caution against using the models in high-stakes\nscenarios where unfair, unreliable, or offensive behavior might be extremely\ncostly or lead to harm.\nCarefully consider use cases in high stak es domains or industr y: Examples\ninclude but are not limited to healthcare, medicine, finance, or legal.\nCarefully consider w ell-scoped chatbot scenarios.  Limiting the use of the\nservice in chatbots to a narrow domain reduces the risk of generating\nunintended or undesirable responses.\nCarefully consider all generativ e use cases.  Content generation scenarios may\nbe more likely to produce unintended outputs and these scenarios require\ncareful consideration and mitigations.\nWhen it comes to large-scale natural language models and image models, there are\nparticular fairness and responsible AI issues to consider. P eople use language and", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "3bc23a6e096c-1", "text": "particular fairness and responsible AI issues to consider. P eople use language and\nimages to describe the world and to express their beliefs, assumptions, attitudes, and\nvalues. As a result, publicly available text and image data typically used to train large-\nscale natural language processing and image generation models contains societal biases\nrelating to race, gender, religion, age, and other groups of people, as well as other\nundesirable content. These societal biases are reflected in the distributions of words,\nphrases, and syntactic structures.Limitations\nTechnical limitations, operational factors and ranges", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7f766c2ac4f9-0", "text": "Large-scale natural language and image models trained with such data can potentially\nbehave in ways that are unfair, unreliable, or offensive, in turn causing harms. Some of\nthe ways are listed here. W e emphasize that these types of harms aren't mutually\nexclusive. A single model can exhibit more than one type of harm, potentially relating to\nmultiple different groups of people. For example:\nAllocation:  These models can be used in ways that lead to unfair allocation of\nresources or opportunities. For example, automated resume screening systems can\nwithhold employment opportunities from one gender if they're trained on resume\ndata that reflects the existing gender imbalance in a particular industry. Or the\nDALL\u00b7E 2 model could be used to create imagery in the style of a known artist,\nwhich could affect the value of the artist's work or the artist's life opportunities.\nQuality o f service: The Azure OpenAI models are trained primarily on English text\nand images with English text descriptions. Languages other than English will\nexperience worse performance. English language varieties with less representation\nin the training data might experience worse performance. The publicly available\nimages used to train the D ALL\u00b7E models might reinforce public bias and other\nundesirable content. The models are also unable to generate consistent\nphotorealistic images and comprehensive text at this time.\nStereotyping:  These models can reinforce stereotypes. For example, when\ntranslating \"He is a nurse\" and \"She is a doctor\" into a genderless language such as\nTurkish and then back into English, many machine translation systems yield the\nstereotypical (and incorrect) results of \"She is a nurse\" and \"He is a doctor.\" With\nDALL\u00b7E 2, when generating an image based on the prompt \"F atherless children,\"\nthe model could generate images of Black children only, reinforcing harmful", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "7f766c2ac4f9-1", "text": "the model could generate images of Black children only, reinforcing harmful\nstereotypes that may exist in publicly available images.\nDemeaning:  These models can demean people. For example, an open-ended\ncontent generation system with inappropriate or insufficient mitigations might\nproduce offensive or demeaning to a particular group of people.\nOverrepresentation and underr epresentation:  These models can over- or under-\nrepresent groups of people, or even erase their representation entirely. For\nexample, if text prompts that contain the word \"gay\" are detected as potentially\nharmful or offensive, this could to the underrepresentation or even erasure of\nlegitimate image generations by or about the L GBTQIA+ community.\uff35 Caution\nPlease be advised that this section contains illustrative examples which include\nterms and language that some individuals may find offensive.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e6b8e6bd1312-0", "text": "Inappr opriat e or o ffensiv e cont ent: These models can produce other types of\ninappropriate or offensive content. Examples include the ability to create images\nthat potentially contain harmful artifacts such as hate symbols; images that illicit\nharmful connotations; images that relate to contested, controversial, or\nideologically polarizing topics; images that are manipulative; images that contain\nsexually charged content that isn't caught by sexual-related content filters; and\nimages that relate to sensitive or emotionally charged topics. For example, a well-\nintentioned text prompt aimed to create an image of the New Y ork skyline with\nclouds and airplanes flying over it might unintentionally generate images that illicit\nsentiments related to the events surrounding 9/11.\nDisinformation and misinformation about sensitiv e topics:  Because D ALL\u00b7E 2 is a\npowerful image generation model, it can be used to produce disinformation and\nmisinformation that can be extremely harmful. For example, the model might\ngenerate an image of a political leader engaging in activity of a violent or sexual\n(or simply inaccurate) nature that might lead to defamation and other\nconsequential harms, including but not limited to public protests, political change,\nor fake news.\nInformation r eliability:  Language model responses can fabricate content that may\nsound reasonable but is nonsensical or inaccurate with respect to external\nvalidation sources. Even when drawing responses from trusted source information,\nresponses may misrepresent that content.\nFalse information:  Azure OpenAI doesn't fact-check or verify content that is\nprovided by customers or users. Depending on how you've developed your\napplication, it might produce false information unless you've built in mitigations\n(see Best practices for impr oving syst em per formance  below).\nIn many AI systems, performance is often defined in relation to accuracy\u2014that is, how", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e6b8e6bd1312-1", "text": "In many AI systems, performance is often defined in relation to accuracy\u2014that is, how\noften the AI system offers a correct prediction or output. With large-scale natural\nlanguage models and image models, two different users might look at the same output\nand have different opinions of how useful or relevant it is, which means that\nperformance for these systems must be defined more flexibly. Here, we broadly consider\nperformance to mean that the application performs as you and your users expect,\nincluding not generating harmful outputs.\nAzure OpenAI service can support a wide range of applications like search, classification,\ncode generation, and image generation, each with different performance metrics and\nmitigation strategies. There are several steps you can take to mitigate some of the\nconcerns listed under \"Limitations\" and to improve performance. Other importantSystem performance", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "98b38d15db36-0", "text": "mitigation techniques are outlined in\u202fthe section Evaluating and integrating Azure\nOpenAI for your use  below.\nShow and t ell when designing pr ompts.  \u202fWith text and code models, make it clear\nto the model what kind of outputs you expect through instructions, examples, or a\ncombination of the two. If you want the model to rank a list of items in\nalphabetical order or to classify a paragraph by sentiment, show it that's what you\nwant.\nKeep y our application on t opic.  Carefully structure prompts and image inputs to\nreduce the chance of producing undesired content, even if a user tries to use it for\nthis purpose. For instance, you might indicate in your prompt that a chatbot only\nengages in conversations about mathematics and otherwise responds \"I'm sorry.\nI'm afraid I can't answer that.\" Adding adjectives like \"polite\" and examples in your\ndesired tone to your prompt can also help steer outputs. With image models, you\nmight indicate in your prompt or image input that your application generates only\nconceptual images. It might otherwise generate a pop-up notification that explains\nthat the application is not for photorealistic use or to portray reality. Consider\nnudging users toward acceptable queries and image inputs, either by listing such\nexamples up front or by offering them as suggestions upon receiving an off-topic\nrequest. Consider training a classifier to determine whether an input (prompt or\nimage) is on topic or off topic.\nProvide quality data.  With text and code models,\u202fif you're trying to build a\nclassifier or get the model to follow a pattern, make sure that there are enough\nexamples. Be sure to proofread your examples\u2014the model is usually smart enough\nto see through basic spelling mistakes and give you a response, but it also might\nassume this is intentional and it could affect the response. Providing quality data", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "98b38d15db36-1", "text": "assume this is intentional and it could affect the response. Providing quality data\nalso includes giving your model reliable data to draw responses from in chat and\nquestion answering systems.\nMeasur e model quality . \u202fAs part of general model quality, consider measuring and\nimproving fairness-related metrics and other metrics related to responsible AI in\naddition to traditional accuracy measures for your scenario. Consider resources\nlike\u202fthis checklist\u202fwhen you measure the fairness of the system. These\nmeasurements come with limitations, which you should acknowledge and\ncommunicate to stakeholders along with evaluation results.\nLimit the length, structur e, and rat e of inputs and outputs.  Restricting the length\nor structure of inputs and outputs can increase the likelihood that the application\nwill stay on task and mitigate, at least in part, any potentially unfair, unreliable, or\noffensive behavior. Other options to reduce the risk of misuse include (i) restrictingBest practices for improving system performance", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b0031e260532-0", "text": "the source of inputs (for example, limiting inputs to a particular domain or to\nauthenticated users rather than being open to anyone on the internet) and (ii)\nimplementing usage rate limits.\nEncourage human r eview o f outputs prior t o publication or dissemination.  With\ngenerative AI, there is potential for generating content that might be offensive or\nnot related to the task at hand, even with mitigations in place. T o ensure that the\ngenerated output meets the task of the customer, consider building ways to\nremind customers to review their outputs for quality prior to sharing widely. This\ncan reduce many different harms, including offensive material, disinformation, and\nmore.\nImplement additional scenario-specific mitigations.  Refer to the mitigations\noutlined in Evaluating and integrating Azure OpenAI for your use  including content\nmoderation strategies. These do not represent every mitigation that might be\nrequired for your application, but they point to the general minimum baseline we\ncheck for when approving use cases for Azure OpenAI Service.\nFor additional information on how to evaluate and integrate these models responsibly,\nplease see the RAI Overview document .\nMicrosoft AI principles\nMicrosoft responsible AI resources\nMicrosoft Azure Learning courses on responsible AI\nLimited access to Azure OpenAI Service - Azure Cognitive Services | Microsoft\nLearn\nCode of Conduct for the Azure OpenAI Service | Microsoft Learn\nData, privacy, and security for Azure OpenAI Service - Azure Cognitive Services |\nMicrosoft LearnEvaluating and integrating Azure OpenAI for\nyour use\nLearn more about responsible AI\nLearn more about Azure OpenAI", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ef92a067f3e3-0", "text": "Limited access to Azure OpenAI Service\nArticle \u202204/28/2023\nAs part of Microsoft's commitment to responsible AI, we are designing and releasing\nAzure OpenAI Service with the intention of protecting the rights of individuals and\nsociety and fostering transparent human-computer interaction. For this reason, we\ncurrently limit the access and use of Azure OpenAI, including limiting access to the\nability to modify content filters and/or abuse monitoring.\nAzure OpenAI requires registration and is currently only available to approved enterprise\ncustomers and partners. Customers who wish to use Azure OpenAI are required to\nsubmit a registration form .\nCustomers must attest to any and all use cases for which they will use the service (the\nuse cases from which customers may select will populate in the form after selection of\nthe desired model(s) in Question 22 in the initial registration form). Customers who wish\nto add additional use cases after initial onboarding must submit the additional use cases\nusing this form . The use of Azure OpenAI is limited to use cases that have been\nselected in a registration form. Microsoft may require customers to re-verify this\ninformation. R ead more about example use cases and use cases to avoid here.\nCustomers who wish to modify content filters and modify abuse monitoring after they\nhave onboarded to the service are subject to additional eligibility criteria and scenario\nrestrictions. At this time, modified content filters and/or modified abuse monitoring for\nAzure OpenAI Service are only available to managed customers and partners working\nwith Microsoft account teams and have additional use case restrictions. Customers\nmeeting these requirements can register here .\nAccess to the Azure OpenAI Service is subject to Microsoft's sole discretion based on\neligibility criteria and a vetting process, and customers must acknowledge that they\nhave read and understand the Azure terms of service for Azure OpenAI Service.\nAzure OpenAI Service is made available to customers under the terms governing their", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "ef92a067f3e3-1", "text": "Azure OpenAI Service is made available to customers under the terms governing their\nsubscription to Microsoft Azure Services, including the Azure OpenAI section of the\nMicrosoft Product T erms . Please review these terms carefully as they contain\nimportant conditions and obligations governing your use of Azure OpenAI Service.Registration process\nImportant links", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "067a6301baac-0", "text": "Register to use Azure OpenAI\nAdd additional use cases  (if needed)\nRegister to modify content filters and abuse monitoring  (if needed)\nFAQ about Limited Access can be found here. If you need help with Azure OpenAI, find\nsupport here. Report abuse of Azure OpenAI here .\nReport problematic content to cscraireport@microsoft.com .\nCode of conduct for Azure OpenAI Service integrations\nTransparency note for Azure OpenAI Service\nCharacteristics and limitations for Azure OpenAI Service\nData, privacy, and security for Azure OpenAI Service\nHelp and support\nSee also", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bef77639514f-0", "text": "Code of conduct for Azure OpenAI\nService\nArticle \u202203/13/2023\nThe following Code of Conduct defines the requirements that all Azure OpenAI Service\nimplementations must adhere to in good faith. This code of conduct is in addition to the\nAcceptable Use P olicy in the Microsoft Online Services T erms .\nAzure OpenAI Service is a Limited Access service that requires registration and is only\navailable to approved enterprise customers and partners. Customers who wish to use\nthis service are required to register through this form . To learn more, see Limited\nAccess to Azure OpenAI Service .\nIntegrations with Azure OpenAI Service must:\nImplement meaningful human oversight\nImplement strong technical limits on inputs and outputs to reduce the likelihood\nof misuse beyond the application's intended purpose\nTest applications thoroughly to find and mitigate undesirable behaviors\nEstablish feedback channels\nImplement additional scenario-specific mitigations\nTo learn more, see the Azure OpenAI transparency note .\nbe used in any way that violates Microsoft\u2019s Acceptable Use P olicy , including but\nnot limited to any use prohibited by law, regulation, government order, or decree,\nor any use that violates the rights of others;\nbe used in any way that is inconsistent with this code of conduct, including the\nLimited Access requirements, the R esponsible AI mitigation requirements, and the\nContent requirements;\nAccess requirements\nResponsible AI mitigation requirements\nIntegrations with Azure OpenAI Service must\nnot:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a4406b55772c-0", "text": "exceed the use case(s) you identified to Microsoft in connection with your request\nto use the service;\ninteract with individuals under the age of consent in any way that could result in\nexploitation or manipulation or is otherwise prohibited by law or regulation;\ngenerate or interact with content prohibited in this Code of Conduct;\nbe presented alongside or monetize content prohibited in this Code of Conduct;\nmake decisions without appropriate human oversight if your application may have\na consequential impact on any individual\u2019s legal position, financial position, life\nopportunities, employment opportunities, human rights, or result in physical or\npsychological injury to an individual;\ninfer sensitive information about people without their explicit consent unless if\nused in a lawful manner by a law enforcement entity, court, or government official\nsubject to judicial oversight in a jurisdiction that maintains a fair and independent\njudiciary; or\nbe used for chatbots that (i) are erotic, romantic, or used for companionship\npurposes, or which are otherwise prohibited by this Code of Conduct; (ii) are\npersonas of specific people without their explicit consent; (iii) claim to have special\nwisdom/insight/knowledge, unless very clearly labeled as being for entertainment\npurposes only; or (iv) enable end users to create their own chatbots without\noversight.\nWe prohibit the use of our service for generating content that can inflict harm on\nindividuals or society. Our content policies are intended to improve the safety of our\nplatform.\nThese content requirements apply to the output of all models developed by OpenAI and\nhosted in Azure OpenAI, such as GPT-3, GPT-4, Codex models, and D ALL\u00b7E 2, and\nincludes content provided as input to the service and content generated as output from\nthe service.\nAzure OpenAI Service prohibits content that describes, features, or promotes child", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a4406b55772c-1", "text": "the service.\nAzure OpenAI Service prohibits content that describes, features, or promotes child\nsexual exploitation or abuse, whether or not prohibited by law. This includes sexual\ncontent involving a child or that sexualizes a child.Content requirements\nExploitation and Abuse\nChild sexual exploitation and abuse", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f5a091ca858c-0", "text": "Azure OpenAI Service prohibits content that describes or is used for purposes of\ngrooming of children. Grooming is the act of an adult building a relationship with a\nchild for the purposes of exploitation, especially sexual exploitation. This includes\ncommunicating with a child for the purpose of sexual exploitation, trafficking, or other\nforms of exploitation.\nAzure OpenAI Service prohibits content that describes, features, or promotes non-\nconsensual intimate activity.\nAzure OpenAI Service prohibits content that describes, features, or promotes, or is used\nfor, purposes of solicitation of commercial sexual activity and sexual services. This\nincludes encouragement and coordination of real sexual activity.\nAzure OpenAI Service prohibits content describing or used for purposes of human\ntrafficking. This includes the recruitment of individuals, facilitation of transport, and\npayment for, and the promotion of, exploitation of people such as forced labor,\ndomestic servitude, sexual slavery, forced marriages, and forced medical procedures.\nAzure OpenAI Service prohibits content that describes, praises, supports, promotes,\nglorifies, encourages and/or instructs individual(s) on self-injury or to take their life.\nAzure OpenAI Service prohibits content that describes, features, or promotes graphic\nviolence or gore.Groom ing\nNon-consensual intimate content\nSexual solicitation\nTrafficking\nSuicide and Self-Injury\nViolent Content and Conduct\nGraphic violence and gore\nTerrorism and Violent Extremism", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "05a35f25de75-0", "text": "Azure OpenAI Service prohibits content that depicts an act of terrorism; praises, or\nsupports a terrorist organization, terrorist actor, or violent terrorist ideology; encourages\nterrorist activities; offers aid to terrorist organizations or terrorist causes; or aids in\nrecruitment to a terrorist organization.\nAzure OpenAI Service prohibits content advocating or promoting violence toward\nothers through violent threats or incitement.\nAzure OpenAI Service prohibits content that attacks, denigrates, intimidates, degrades,\ntargets, or excludes individuals or groups on the basis of traits such as actual or\nperceived race, ethnicity, national origin, gender, gender identity, sexual orientation,\nreligious affiliation, age, disability status, caste, or any other characteristic that is\nassociated with systemic prejudice or marginalization.\nAzure OpenAI Service prohibits content that targets individual(s) or group(s) with\nthreats, intimidation, insults, degrading or demeaning language or images, promotion of\nphysical harm, or other abusive behavior such as stalking.\nAzure OpenAI Service prohibits content that is intentionally deceptive and likely to\nadversely affect the public interest, including deceptive or untrue content relating to\nhealth, safety, election integrity, or civic participation. Azure OpenAI Service also\nprohibits inauthentic interactions, such as fake accounts, automated inauthentic activity,\nimpersonation to gain unauthorized information or privileges, and claims to be from any\nperson, company, government body, or entity without explicit permission to make that\nrepresentation.\nContent that directly supports unlawful active attacks or malware campaigns that cause\ntechnical harms, such as delivering malicious executables, organizing denial of serviceViolent Threats, Incitement, and Glorification of Violence\nHarmful Content\nHate speech and discrimination\nBullying and harassment\nDeception, disinformation, and inauthentic activity\nActive malware or exploits", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "fbcfb1e7a111-0", "text": "attacks, or managing command and control servers.\nWe prohibit the use of our Azure OpenAI Service for scenarios in which the system is\nlikely to generate undesired content due to limitations in the models or scenarios in\nwhich the system cannot be applied in a way that properly manages potential negative\nconsequences to people and society. Without limiting the foregoing restriction,\nMicrosoft reserves the right to revise and expand the above Content requirements to\naddress specific harms to people and society.\nThis includes prohibiting content that is sexually graphic, including consensual\npornographic content and intimate descriptions of sexual acts, as well as content that\nmay influence the political process, such as an election, passage of legislation, and\ncontent for campaigning purposes.\nWe may at times limit our service's ability to respond to particular topics, such as\nprobing for personal information or seeking opinions on sensitive topics or current\nevents.\nWe prohibit the use of Azure OpenAI Service for activities that significantly harm other\nindividuals, organizations, or society, including but not limited to use of the service for\npurposes in conflict with the applicable Azure Legal T erms  and the Microsoft Product\nTerms .\nIf you suspect that Azure OpenAI Service is being used in a manner that is abusive or\nillegal, infringes on your rights or the rights of other people, or violates these policies,\nyou can report it at the Report Abuse P ortal .\nIf Azure OpenAI Service outputs problematic content that you believe should have been\nfiltered, report it at cscraireport@microsoft.com .\nLimited access to Azure OpenAI Service\nTransparency note for Azure OpenAI Service\nData, privacy, and security for Azure OpenAI ServiceAdditional content policies\nReport abuse\nReport problematic content\nSee also", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "62558937eb87-0", "text": "Data, privacy, and security for Azure\nOpenAI Service\nArticle \u202206/23/2023\nThis article provides details regarding how data provided by you to the Azure OpenAI\nservice is processed, used, and stored. Azure OpenAI stores and processes data to\nprovide the service and to monitor for uses that violate the applicable product terms.\nPlease also see the Microsoft Products and Services Data Protection Addendum ,\nwhich governs data processing by the Azure OpenAI Service except as otherwise\nprovided in the applicable Product T erms .\nAzure OpenAI processes the following types of data:\nPrompts and generat ed cont ent. Prompts are submitted by the user, and content\nis generated by the service, via the completions, chat completions, images and\nembeddings operations.\n\uff09 Impor tant\nYour prompts (inputs) and completions (outputs), your embeddings, and your\ntraining data:\nare NO T available to other customers.\nare NO T available to OpenAI.\nare NO T used to improve OpenAI models.\nare NO T used to improve any Microsoft or 3rd party products or services.\nare NO T used for automatically improving Azure OpenAI models for your use\nin your resource (The models are stateless, unless you explicitly fine-tune\nmodels with your training data).\nYour fine-tuned Azure OpenAI models are available exclusively for your use.\nThe Azure OpenAI Service is fully controlled by Microsoft; Microsoft hosts the\nOpenAI models in Microsoft\u2019s Azure environment and the Service does NO T\ninteract with any services operated by OpenAI (e.g. ChatGPT, or the OpenAI API).\nWhat data does the Azure OpenAI Service\nprocess?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b258423fcd8e-0", "text": "Augment ed data included with pr ompts . When using the \"on your data\" feature,\nthe service retrieves relevant data from a configured data store and augments the\nprompt to produce generations that are grounded with your data.\nTraining & v alidation data . You can provide your own training data consisting of\nprompt-completion pairs for the purposes of fine-tuning an OpenAI model .\nThe diagram below illustrates how your data is processed. This diagram covers three\ndifferent types of processing:\n1. How the Azure OpenAI Service processes your prompts to generate content\n(including when additional data from a connected data source is added to a\nprompt using Azure OpenAI on your data).\n2. How the Azure OpenAI Service creates a fine-tuned (custom) model with your\ntraining data.\n3. How the Azure OpenAI Service and Microsoft personnel analyze prompts,\ncompletions and images for harmful content and for patterns suggesting the use\nof the service in a manner that violates the Code of Conduct or other applicable\nproduct terms\nAs depicted in the diagram above, managed customers may apply to modify abuse\nmonitoring .How does the Azure OpenAI Service process\ndata?\nGenerating completions, images or embeddings", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d92019319490-0", "text": "Models (base or fine-tuned) deployed in your resource process your input prompts and\ngenerate responses with text, images or embeddings. The service is configured to\nsynchronously evaluate the prompt and completion data in real time to check for\nharmful content types and stops generating content that exceeds the configured\nthresholds. Learn more at Azure OpenAI Service content filtering .\nThe models ar e stat eless: no pr ompts or generations ar e stored in the model.\nAdditionally , prompts and generations ar e not used t o train, r etrain, or impr ove the\nbase models.\nThe Azure OpenAI \"on your data\" feature lets you connect data sources to ground the\ngenerated results with your data. The data remains stored in the data source and\nlocation you designate. No data is copied int o the Azur e OpenAI ser vice. When a user\nprompt is received, the service retrieves relevant data from the connected data source\nand augments the prompt. The model processes this augmented prompt and the\ngenerated content is returned as described above.\nAs depicted in the diagram above, managed customers may apply to modify abuse\nmonitoring .\nCustomers can upload their training data to the service to fine tune a model. Uploaded\ntraining data is stored in the Azure OpenAI resource in the customer\u2019s Azure tenant.Augmenting prompts with data retrieved from your data sources to\n\"ground\" the generated results\nCreating a customized (fine-tuned) model with your data:", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "814cded75493-0", "text": "Training data and fine-tuned models:\nAre available exclusively for use by the customer.\nAre stored within the same region as the Azure OpenAI resource.\nCan be double encrypted at rest  (by default with Microsoft's AES-256 encryption\nand optionally with a customer managed key).\nCan be deleted by the customer at any time.\nTraining data uploaded for fine-tuning is not used to train, retrain, or improve any\nMicrosoft or 3rd party base models.\nTo reduce the risk of harmful use of the Azure OpenAI Service, the Azure OpenAI Service\nincludes both content filtering and abuse monitoring features. T o learn more about\ncontent filtering , see Azure OpenAI Service content filtering. T o learn more about abuse\nmonitoring, see abuse monitoring .\nContent filtering occurs synchronously as the service processes prompts to generate\ncontent as described above and here. No prompts or generated results are stored in the\ncontent classifier models, and prompts and results are not used to train, retrain, or\nimprove the classifier models.\nAzure OpenAI abuse monitoring detects and mitigates instances of recurring content\nand/or behaviors that suggest use of the service in a manner that may violate the code\nof conduct  or other applicable product terms. T o detect and mitigate abuse, Azure\nOpenAI stores all prompts and generated content securely for up to thirty (30) days. (No\nprompts or completions are stored if the customer is approved for and elects to\nconfigure abuse monitoring off, as described below.)\nThe data store where prompts and completions are stored is logically separated by\ncustomer resource (each request includes the resource ID of the customer\u2019s Azure\nOpenAI resource). A separate data store is located in each region  in which the Azure\nOpenAI Service is available, and a customer\u2019s prompts and generated content are stored\nin the Azure region where the customer\u2019s Azure OpenAI service resource is deployed,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "814cded75493-1", "text": "in the Azure region where the customer\u2019s Azure OpenAI service resource is deployed,\nwithin the Azure OpenAI service boundary. Human reviewers assessing potential abuse\ncan access prompts and completions data only when that data has been flagged by the\nabuse monitoring system. The human reviewers are authorized Microsoft employees\nwho access the data via point wise queries using request IDs, Secure Access\nWorkstations (SA Ws), and Just-In-Time (JIT) request approval granted by team\nmanagers. For Azure OpenAI Service deployed in the European Economic Area, the\nauthorized Microsoft employees are located in the European Economic Area.Preventing abuse and harmful content generation", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "c1d29ddb2cb2-0", "text": "Some customers may want to use the Azure OpenAI Service for a use case that involves\nthe processing of sensitive, highly confidential, or legally-regulated input data but where\nthe likelihood of harmful outputs and/or misuse is low. These customers may conclude\nthat they do not want or do not have the right to permit Microsoft to process such data\nfor abuse detection, as described above, due to their internal policies or applicable legal\nregulations. T o address these concerns, Microsoft allows customers who meet additional\nLimited Access eligibility criteria and attest to specific use cases to apply to modify the\nAzure OpenAI content management features by completing this form .\nIf Microsoft approves a customer's request to modify abuse monitoring, then Microsoft\ndoes not store any prompts and completions associated with the approved Azure\nsubscription for which abuse monitoring is configured off. In this case, because no\nprompts and completions are stored at rest in the Service R esults S tore, the human\nreview process is not possible and is not performed. See Abuse monitoring  for more\ninformation.\nThere are two ways for customers, once approved to turn off abuse monitoring, to verify\nthat data storage for abuse monitoring has been turned off in their approved Azure\nsubscription:\nUsing the Azure portal, or\nAzure CLI (or any management API).\n1. Sign into Azure\n2. Select the Azure Subscription which hosts the Azure OpenAI Service resource.\n3. Navigate to the Overview  page of the Azure OpenAI Service resource.How can customers get an exemption from\nabuse monitoring and human review?\nHow can a customer verify if data storage for abuse\nmonitoring is off?\n\uff17 Note\nThe value of \"false\" for the \"ContentLogging\" attribute appears only if data storage\nfor abuse monitoring is turned off. Otherwise, this property will not appear in either\nAzure portal or Azure CLI's output.\nPrerequisites", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "88a019bb548e-0", "text": "Logging status v erification using the Azur e por tal:\n1. Go to the resource Overview page\n2. Click on the JSON view  link on the top right corner as shown in the image below.\nThere will be a value in the Capabilities list called \"ContentLogging\" which will appear\nand be set to F ALSE when logging for abuse monitoring is off.\nJSON\nLogging status v erification using the Azur e CLI (or other management API):\nExecute the following command in Azure CLI to see the same JSON data as shown in the\nAzure portal above.\nAzure CLI\nTo learn more about Microsoft's privacy and security commitments see the Microsoft\nTrust Center .{  \n    \"name\":\"ContentLogging\" , \n    \"value\":\"false\" \n} \naz cognitiveservices account show -n resource\\_name -g resource \\_group  \nChange log", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "013e89ea8de9-0", "text": "DateChanges DateChanges\n23\nJune\n2023Added information about data processing for new Azure on your data feature; removed\ninformation about abuse monitoring which is now available at Azure OpenAI Service\nabuse monitoring . Added summary note. Updated and streamlined content and updated\ndiagrams for additional clarity. added change log\nLimited access to Azure OpenAI Service\nCode of conduct for Azure OpenAI Service integrations\nTransparency note and use cases for Azure OpenAI Service\nCharacteristics and limitations for Azure OpenAI Service\nReport abuse of Azure OpenAI Service through the Report Abuse P ortal\nReport problematic content to cscraireport@microsoft.comSee also", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "be6856591b6d-0", "text": "Microsoft.CognitiveServices accounts\nArticle \u202206/18/2023\nThe accounts resource type can be deployed with operations that target:\nResour ce gr oups  - See resource group deployment commands\nFor a list of changed properties in each API version, see change log .\nTo create a Microsoft.CognitiveServices/accounts resource, add the following Bicep to your template.\nBicepBicep resource definition\nResource format\nresource  symbolicname  'Microsoft.CognitiveServices/accounts@2023-05-01'  = { \n  name: 'string'  \n  location : 'string'  \n  tags: { \n    tagName1 : 'tagValue1'  \n    tagName2 : 'tagValue2'  \n  } \n  sku: { \n    capacity : int \n    family: 'string'  \n    name: 'string'  \n    size: 'string'  \n    tier: 'string'  \n  } \n  kind: 'string'  \n  identity : { \n    type: 'string'  \n    userAssignedIdentities : {} \n  } \n  properties : { \n    allowedFqdnList : [ \n      'string'  \n    ] \n    apiProperties : { \n      aadClientId : 'string'  \n      aadTenantId : 'string'  \n      eventHubConnectionString : 'string'  \n      qnaAzureSearchEndpointId : 'string'  \n      qnaAzureSearchEndpointKey : 'string'  \n      qnaRuntimeEndpoint : 'string'  \n      statisticsEnabled : bool \n      storageAccountConnectionString : 'string'  \n      superUser : 'string'  \n      websiteName : 'string'  \n    } \n    customSubDomainName : 'string'", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "be6856591b6d-1", "text": "} \n    customSubDomainName : 'string'  \n    disableLocalAuth : bool \n    dynamicThrottlingEnabled : bool \n    encryption : { \n      keySource : 'string'  \n      keyVaultProperties : { \n        identityClientId : 'string'  \n        keyName: 'string'  \n        keyVaultUri : 'string'  \n        keyVersion : 'string'  \n      } \n    } \n    locations : { \n      regions: [ \n        {  \n          customsubdomain : 'string'  \n          name: 'string'  \n          value: int \n        }  \n      ] \n      routingMethod : 'string'  \n    } \n    migrationToken : 'string'", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "26b60e7150c5-0", "text": "Name Descr iption Value\nname The resource name string (required)  \nCharacter limit: 2-64  \nValid characters:  \nAlphanumerics and hyphens.  \nStart and end with alphanumeric.\nlocation The geo-location where the resource lives string\ntags Resource tags. Dictionary of tag names and values. See Tags in templates\nsku The resource model definition representing SKU Sku\nkind The Kind of the resource. string\nidentity Identity for the resource. Identity\nproperties Properties of Cognitive Services account. AccountProperties\nName Descr iption\ntype The identity type.\nuserAssignedIdentities The list of user assigned identities associated with the resource. The user identity dictionary key references will be ARM resource ids in the for\n'/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identity\nName Descr iption Value\nallowedFqdnList string[]    networkAcls : { \n      defaultAction : 'string'  \n      ipRules: [ \n        {  \n          value: 'string'  \n        }  \n      ] \n      virtualNetworkRules : [ \n        {  \n          id: 'string'  \n          ignoreMissingVnetServiceEndpoint : bool \n          state: 'string'  \n        }  \n      ] \n    } \n    publicNetworkAccess : 'string'  \n    restore: bool \n    restrictOutboundNetworkAccess : bool \n    userOwnedStorage : [ \n      { \n        identityClientId : 'string'  \n        resourceId : 'string'  \n      } \n    ] \n  } \n} \nProperty values\naccounts\nIdentity\nAccountProperties", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "99d448f6e7d1-0", "text": "Name Descr iption Value\napiProperties The api properties for special APIs. ApiProperties\ncustomSubDomainName Optional subdomain name used for token-based authentication. string\ndisableLocalAuth bool\ndynamicThrottlingEnabled The flag to enable dynamic throttling. bool\nencryption The encryption properties for this resource. Encryption\nlocations The multiregion settings of Cognitive Services account. MultiR egionSettings\nmigrationT oken Resource migration token. string\nnetworkAcls A collection of rules governing the accessibility from specific network locations. NetworkRuleSet\npublicNetworkAccess Whether or not public endpoint access is allowed for this account. 'Disabled'  \n'Enabled'\nrestore bool\nrestrictOutboundNetworkAccess bool\nuserOwnedS torage The storage accounts for this resource. UserOwnedS torage []\nName Descr iption Value\naadClientId (Metrics Advisor Only) The Azure AD Client Id (Application Id). string\naadTenantId (Metrics Advisor Only) The Azure AD T enant Id. string\neventHubConnectionS tring (Personalization Only) The flag to enable statistics of Bing Search. string\nqnaAzureSearchEndpointId (QnAMaker Only) The Azure Search endpoint id of QnAMaker. string\nqnaAzureSearchEndpointK ey (QnAMaker Only) The Azure Search endpoint key of QnAMaker. string\nqnaRuntimeEndpoint (QnAMaker Only) The runtime endpoint of QnAMaker. string\nstatisticsEnabled (Bing Search Only) The flag to enable statistics of Bing Search. bool\nstorageAccountConnectionS tring (Personalization Only) The storage account connection string. string\nsuperUser (Metrics Advisor Only) The super user of Metrics Advisor. string\nwebsiteName (Metrics Advisor Only) The website name of Metrics Advisor. string\nName Descr iption Value", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "99d448f6e7d1-1", "text": "Name Descr iption Value\nkeySource Enumerates the possible value of keySource for Encryption 'Microsoft.CognitiveServices'  \n'Microsoft.K eyVault'\nkeyVaultProperties Properties of K eyVault KeyVaultProperties\nName Descr iption Value\nidentityClientId string\nkeyName Name of the K ey from K eyVault string\nkeyVaultUri Uri of K eyVault string\nkeyV ersion Version of the K ey from K eyVault stringApiProperties\nEncryption\nKeyVaultProperties\nMultiRegionSettings", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "258af6af7445-0", "text": "Name Descr iption Value\nregions RegionSetting []\nroutingMethod Multiregion routing methods. 'Performance'  \n'Priority'  \n'Weighted'\nName Descr iption Value\ncustomsubdomain Maps the region to the regional custom subdomain. string\nname Name of the region. string\nvalue A value for priority or weighted routing methods. int\nName Descr iption Value\ndefaultAction The default action when no rule from ipRules and from virtualNetworkRules match. This is only used after the\nbypass property has been evaluated.'Allow'  \n'Deny'\nipRules The list of IP address rules. IpRule []\nvirtualNetworkRules The list of virtual network rules. VirtualNetworkRule []\nName Descr iption Value\nvalue An IPv4 address range in CIDR notation, such as '124.56.78.91' (simple IP address) or '124.56.78.0/24' (all addresses that start with\n124.56.78).string\n(required)\nName Descr iption Value\nid Full resource id of a vnet subnet, such as\n'/subscriptions/subid/resourceGroups/rg1/providers/Microsoft.Network/virtualNetworks/test-\nvnet/subnets/subnet1'.string\n(required)\nignoreMissingV netServiceEndpoint Ignore missing vnet service endpoint or not. bool\nstate Gets the state of virtual network rule. string\nName Descr iption Value\nidentityClientId string\nresourceId Full resource id of a Microsoft.S torage resource. string\nName Descr iption Value\ncapacity If the SKU supports scale out/in then the capacity integer should be included. If scale out/in is not possible for the resource this\nmay be omitted.int\nfamily If the service has different generations of hardware, for the same SKU, then that can be captured here. string", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "258af6af7445-1", "text": "name The name of the SKU. Ex - P3. It is typically a letter+number code string\n(required)RegionSetting\nNetworkRuleSet\nIpRule\nVirtualNetworkRule\nUserOwnedStorage\nSku", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1d6215f9e99c-0", "text": "Name Descr iption Value\nsize The SKU size. When the name field is the combination of tier and some other value, this would be the standalone code. string\ntier This field is required to be implemented by the R esource Provider if the service has more than one tier, but is not required on a\nPUT.'Basic'  \n'Enterprise'  \n'Free'  \n'Premium'  \n'Standard'\nThe following quickstart templates deploy this resource type.\nTemplat e Descr iption\nDeploy a Cognitive Services Computer Vision API  A template for creating a new Cognitive Services Computer Vision API\nDeploy a Cognitive Services T ranslate API  This template deploys a Cognitive Services T ranslate API. Microsoft T ranslator\nAPI is a neural machine translation service that developers can easily\nintegrate into their applications websites, tools, or any solution requiring\nmulti-language support such as website localization, e-commerce, customer\nsupport, messaging applications, internal communication, and more.\nDeploy a Cognitive Service Universal key  This template deploys all the Cognitive Services APIs Cognitive Services bring\nAI within reach of every developer?without requiring machine-learning\nexpertise. All it takes is an API call to embed the ability to see, hear, speak,\nsearch, understand, and accelerate decision-making into your apps.Quickstart templates", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "cb5e5062a29e-0", "text": "Azure OpenAI Service REST API\nreference\nArticle \u202206/19/2023\nThis article provides details on the inference REST API endpoints for Azure OpenAI.\nAzure OpenAI provides two methods for authentication. you can use either API K eys or\nAzure Active Directory.\nAPI K ey authentication : For this type of authentication, all API requests must\ninclude the API K ey in the api-key HTTP header. The Quickstart  provides guidance\nfor how to make calls with this type of authentication.\nAzur e Activ e Dir ectory authentication : You can authenticate an API call using an\nAzure Active Directory token. Authentication tokens are included in a request as\nthe Authorization header. The token provided must be preceded by Bearer, for\nexample Bearer YOUR_AUTH_TOKEN. You can read our how-to guide on\nauthenticating with Azure Active Directory .\nThe service APIs are versioned using the api-version query parameter. All versions\nfollow the YYYY-MM-DD date structure. For example:\nHTTP\nWith the Completions operation, the model will generate one or more predicted\ncompletions based on a provided prompt. The service can also return the probabilities\nof alternative tokens at each position.\nCreate a completionAuth entication\nREST API versioning\nPOST \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYME\nNT_NAME/completions?api-version=2023-05-15  \nCompletions", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d98f5e884066-0", "text": "HTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-resource-\nnamestring Required The name of your Azure OpenAI R esource.\ndeployment-id string Required The deployment name you chose when you deployed the\nmodel.\napi-version string Required The API version to use for this operation. This follows the\nYYYY-MM-DD format.\nSuppor ted versions\n2023-03-15-preview Swagger spec\n2022-12-01 Swagger spec\n2023-05-15 Swagger spec\n2023-06-01-preview Swagger spec\nRequest body\nParamet er Type Requir ed? Default Descr iption\nprompt string\nor arrayOptional <\\|endoftext\\|>The prompt(s) to generate\ncompletions for, encoded as a\nstring, or array of strings. Note\nthat <\\|endoftext\\|> is the\ndocument separator that the\nmodel sees during training, so if\na prompt isn't specified the\nmodel will generate as if from\nthe beginning of a new\ndocument.POST https://{your-resource-\nname}.openai.azure.com/openai/deployments/{deployment-id}/completions?api-\nversion={api-version}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f0f1343968c2-0", "text": "Paramet er Type Requir ed? Default Descr iption\nmax_tokens integer Optional 16 The maximum number of tokens\nto generate in the completion.\nThe token count of your prompt\nplus max_tokens can't exceed the\nmodel's context length. Most\nmodels have a context length of\n2048 tokens (except for the\nnewest models, which support\n4096).\ntemperature number Optional 1 What sampling temperature to\nuse, between 0 and 2. Higher\nvalues means the model will take\nmore risks. T ry 0.9 for more\ncreative applications, and 0\n(argmax sampling) for ones with\na well-defined answer. W e\ngenerally recommend altering\nthis or top_p but not both.\ntop_p number Optional 1 An alternative to sampling with\ntemperature, called nucleus\nsampling, where the model\nconsiders the results of the\ntokens with top_p probability\nmass. So 0.1 means only the\ntokens comprising the top 10%\nprobability mass are considered.\nWe generally recommend\naltering this or temperature but\nnot both.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e59941e1c854-0", "text": "Paramet er Type Requir ed? Default Descr iption\nlogit_bias map Optional null Modify the likelihood of\nspecified tokens appearing in the\ncompletion. Accepts a json\nobject that maps tokens\n(specified by their token ID in the\nGPT tokenizer) to an associated\nbias value from -100 to 100. Y ou\ncan use this tokenizer tool (which\nworks for both GPT-2 and GPT-3)\nto convert text to token IDs.\nMathematically, the bias is added\nto the logits generated by the\nmodel prior to sampling. The\nexact effect will vary per model,\nbut values between -1 and 1\nshould decrease or increase\nlikelihood of selection; values like\n-100 or 100 should result in a\nban or exclusive selection of the\nrelevant token. As an example,\nyou can pass {\"50256\": -100} to\nprevent the <|endoftext|> token\nfrom being generated.\nuser string Optional A unique identifier representing\nyour end-user, which can help\nmonitoring and detecting abuse\nn integer Optional 1 How many completions to\ngenerate for each prompt. Note:\nBecause this parameter\ngenerates many completions, it\ncan quickly consume your token\nquota. Use carefully and ensure\nthat you have reasonable\nsettings for max_tokens and\nstop.\nstream boolean Optional False Whether to stream back partial\nprogress. If set, tokens will be\nsent as data-only server-sent\nevents as they become available,\nwith the stream terminated by a\ndata: [DONE] message.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f878758e3de2-0", "text": "Paramet er Type Requir ed? Default Descr iption\nlogprobs integer Optional null Include the log probabilities on\nthe logprobs most likely tokens,\nas well the chosen tokens. For\nexample, if logprobs is 10, the\nAPI will return a list of the 10\nmost likely tokens. the API will\nalways return the logprob of the\nsampled token, so there may be\nup to logprobs+1 elements in\nthe response. This parameter\ncannot be used with gpt-35-\nturbo.\nsuffix string Optional null The suffix that comes after a\ncompletion of inserted text.\necho boolean Optional False Echo back the prompt in\naddition to the completion. This\nparameter cannot be used with\ngpt-35-turbo.\nstop string\nor arrayOptional null Up to four sequences where the\nAPI will stop generating further\ntokens. The returned text won't\ncontain the stop sequence.\npresence_penalty number Optional 0 Number between -2.0 and 2.0.\nPositive values penalize new\ntokens based on whether they\nappear in the text so far,\nincreasing the model's likelihood\nto talk about new topics.\nfrequency_penaltynumber Optional 0 Number between -2.0 and 2.0.\nPositive values penalize new\ntokens based on their existing\nfrequency in the text so far,\ndecreasing the model's\nlikelihood to repeat the same\nline verbatim.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "4f828e0216bd-0", "text": "Paramet er Type Requir ed? Default Descr iption\nbest_of integer Optional 1 Generates best_of completions\nserver-side and returns the\n\"best\" (the one with the lowest\nlog probability per token).\nResults can't be streamed. When\nused with n, best_of controls the\nnumber of candidate\ncompletions and n specifies how\nmany to return \u2013 best_of must be\ngreater than n. Note: Because\nthis parameter generates many\ncompletions, it can quickly\nconsume your token quota. Use\ncarefully and ensure that you\nhave reasonable settings for\nmax_tokens and stop. This\nparameter cannot be used with\ngpt-35-turbo.\nConsole\nJSONExample request\ncurl \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYME\nNT_NAME/completions?api-version=2023-05-15\\  \n  -H \"Content-Type: application/json\" \\  \n  -H \"api-key: YOUR_API_KEY\" \\  \n  -d \"{ \n  \\\"prompt\\\": \\\"Once upon a time\\\",  \n  \\\"max_tokens\\\": 5  \n}\" \nExample response\n{ \n    \"id\": \"cmpl-4kGh7iXtjW4lc9eGhff6Hp8C7btdQ\" , \n    \"object\" : \"text_completion\" , \n    \"created\" : 1646932609 , \n    \"model\": \"ada\", \n    \"choices\" : [ \n        {  \n            \"text\": \", a dark line crossed\" ,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "88ef4dae24a9-0", "text": "In the example response, finish_reason equals stop. If finish_reason equals\ncontent_filter consult our content filtering guide  to understand why this is occurring.\nGet a vector representation of a given input that can be easily consumed by machine\nlearning models and other algorithms.\nCreate an embedding\nHTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-\nresource-\nnamestring Required The name of your Azure OpenAI R esource.\ndeployment-\nidstring Required The name of your model deployment. Y ou're required to first\ndeploy a model before you can make calls            \"index\": 0,\n            \"logprobs\" : null, \n            \"finish_reason\" : \"length\"  \n        }  \n    ] \n} \nEmbeddings\n\uff17 Note\nWe currently do not support batching of embeddings into a single API call. If you\nreceive the error InvalidRequestError: Too many inputs. The max number of inputs\nis 1. We hope to increase the number of inputs per request soon., this typically\noccurs when an array of embeddings is attempted to be passed as a batch rather\nthan a single string. The string can be up to 8191 tokens in length when using the\ntext-embedding-ada-002 (V ersion 2) model.\nPOST https://{your-resource-\nname}.openai.azure.com/openai/deployments/{deployment-id}/embeddings?api-\nversion={api-version}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d1b214ce6526-0", "text": "Paramet er Type Requir ed? Descr iption\napi-\nversionstring Required The API version to use for this operation. This follows the\nYYYY-MM-DD format.\nSuppor ted versions\n2023-03-15-preview Swagger spec\n2022-12-01 Swagger spec\n2023-05-15 Swagger spec\nRequest body\nParamet er Type Requir ed? Default Descr iption\ninput string Yes N/A Input text to get embeddings for, encoded as a\nstring. The number of input tokens varies\ndepending on what model you are using .  \nUnless you're embedding code, we suggest\nreplacing newlines (\\n) in your input with a single\nspace, as we have observed inferior results when\nnewlines are present.\nuser string No Null A unique identifier representing for your end-user.\nThis will help Azure OpenAI monitor and detect\nabuse. Do not p ass PII identifier s inst ead use\npseudo anonymized v alues such as GUIDs\nConsole\nJSON\nExample request\ncurl \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYME\nNT_NAME/embeddings?api-version=2023-05-15 \\  \n  -H \"Content-Type: application/json\" \\  \n  -H \"api-key: YOUR_API_KEY\" \\  \n  -d \"{\\\"input\\\": \\\"The food was delicious and the waiter...\\\"}\"  \nExample response\n{ \n  \"object\" : \"list\", \n  \"data\": [", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0749068e76cb-0", "text": "Create completions for chat messages with the ChatGPT and GPT-4 models.\nCreate chat completions\nHTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-\nresource-\nnamestring Required The name of your Azure OpenAI R esource.\ndeployment-\nidstring Required The name of your model deployment. Y ou're required to first\ndeploy a model before you can make calls\napi-\nversionstring Required The API version to use for this operation. This follows the\nYYYY-MM-DD format.\nSuppor ted versions\n2023-03-15-preview Swagger spec\n2023-05-15 Swagger spec\n2023-06-01-preview Swagger spec    { \n      \"object\" : \"embedding\" , \n      \"embedding\" : [ \n        0.018990106880664825 , \n        -0.0073809814639389515 , \n        .... ( 1024 floats total for ada)  \n        0.021276434883475304 , \n      ],  \n      \"index\": 0 \n    } \n  ], \n  \"model\": \"text-similarity-babbage:001\"  \n} \nChat completions\nPOST https://{your-resource-\nname}.openai.azure.com/openai/deployments/{deployment-id}/chat/completions?\napi-version={api-version}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "21e6829926e6-0", "text": "Console\nConsole\nIn the example response, finish_reason equals stop. If finish_reason equals\ncontent_filter consult our content filtering guide  to understand why this is occurring.\nOutput formatting adjusted for ease of reading, actual output is a single block of text\nwithout line breaks.\nParamet er Type Requir ed? Default Descr iption\nmessages array Required The messages to generate chat\ncompletions for, in the chat format.Example request\ncurl \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYME\nNT_NAME/chat/completions?api-version=2023-05-15 \\  \n  -H \"Content-Type: application/json\" \\  \n  -H \"api-key: YOUR_API_KEY\" \\  \n  -d '{\"messages\":[{\"role\": \"system\", \"content\": \"You are a helpful  \nassistant.\"},{\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer  \nmanaged keys?\"},{\"role\": \"assistant\", \"content\": \"Yes, customer managed keys  \nare supported by Azure OpenAI.\"},{\"role\": \"user\", \"content\": \"Do other Azure  \nCognitive Services support this too?\"}]}'  \nExample response\n{\"id\":\"chatcmpl-6v7mkQj980V1yBec6ETrKPRqFjNw9\",  \n\"object\":\"chat.completion\",\"created\":1679072642,  \n\"model\":\"gpt-35-turbo\",  \n\"usage\":{\"prompt_tokens\":58,  \n\"completion_tokens\":68,  \n\"total_tokens\":126},  \n\"choices\":[{\"message\":{\"role\":\"assistant\",  \n\"content\":\"Yes, other Azure Cognitive Services also support customer managed  \nkeys. Azure Cognitive Services offer multiple options for customers to", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "21e6829926e6-1", "text": "keys. Azure Cognitive Services offer multiple options for customers to  \nmanage keys, such as using Azure Key Vault, customer-managed keys in Azure  \nKey Vault or customer-managed keys through Azure Storage service. This helps  \ncustomers ensure that their data is secure and access to their services is  \ncontrolled.\"},\"finish_reason\":\"stop\",\"index\":0}]}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "9a3ba89e56f2-0", "text": "Paramet er Type Requir ed? Default Descr iption\ntemperature number Optional 1 What sampling temperature to use,\nbetween 0 and 2. Higher values like 0.8\nwill make the output more random, while\nlower values like 0.2 will make it more\nfocused and deterministic.\\nW e generally\nrecommend altering this or top_p but not\nboth.\nn integer Optional 1 How many chat completion choices to\ngenerate for each input message.\nstream boolean Optional false If set, partial message deltas will be sent,\nlike in ChatGPT. T okens will be sent as\ndata-only server-sent events as they\nbecome available, with the stream\nterminated by a data: [DONE] message.\"\nstop string\nor arrayOptional null Up to 4 sequences where the API will stop\ngenerating further tokens.\nmax_tokens integer Optional inf The maximum number of tokens allowed\nfor the generated answer. By default, the\nnumber of tokens the model can return\nwill be (4096 - prompt tokens).\npresence_penalty number Optional 0 Number between -2.0 and 2.0. P ositive\nvalues penalize new tokens based on\nwhether they appear in the text so far,\nincreasing the model's likelihood to talk\nabout new topics.\nfrequency_penaltynumber Optional 0 Number between -2.0 and 2.0. P ositive\nvalues penalize new tokens based on\ntheir existing frequency in the text so far,\ndecreasing the model's likelihood to\nrepeat the same line verbatim.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "6ca897fb143a-0", "text": "Paramet er Type Requir ed? Default Descr iption\nlogit_bias object Optional null Modify the likelihood of specified tokens\nappearing in the completion. Accepts a\njson object that maps tokens (specified by\ntheir token ID in the tokenizer) to an\nassociated bias value from -100 to 100.\nMathematically, the bias is added to the\nlogits generated by the model prior to\nsampling. The exact effect will vary per\nmodel, but values between -1 and 1\nshould decrease or increase likelihood of\nselection; values like -100 or 100 should\nresult in a ban or exclusive selection of\nthe relevant token.\nuser string Optional A unique identifier representing your\nend-user, which can help Azure OpenAI\nto monitor and detect abuse.\nExtensions for chat completions, for example Azure OpenAI on your data.\nUse chat completions ext ensions\nHTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-\nresource-\nnamestring Required The name of your Azure OpenAI R esource.\ndeployment-\nidstring Required The name of your model deployment. Y ou're required to first\ndeploy a model before you can make calls\napi-\nversionstring Required The API version to use for this operation. This follows the\nYYYY-MM-DD format.\nSuppor ted versionsCompletions extensions\nPOST {your-resource-name}/openai/deployments/{deployment-\nid}/extensions/chat/completions?api-version={api-version}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "720131a3c052-0", "text": "2023-06-01-preview Swagger spec\nConsole\nJSON\nExample request\ncurl -i -X POST  \nYOUR_RESOURCE_NAME/openai/deployments/YOUR_DEPLOYMENT_NAME/extensions/chat/c\nompletions?api-version=2023-06-01-preview \\  \n-H \"Content-Type: application/json\" \\  \n-H \"api-key: YOUR_API_KEY\" \\  \n-H \"chatgpt_url: YOUR_RESOURCE_URL\" \\  \n-H \"chatgpt_key: YOUR_API_KEY\" \\  \n-d \\ \n' \n{ \n    \"dataSources\": [  \n        {  \n            \"type\": \"AzureCognitiveSearch\",  \n            \"parameters\": {  \n                \"endpoint\": \"'YOUR_AZURE_COGNITIVE_SEARCH_ENDPOINT'\",  \n                \"key\": \"'YOUR_AZURE_COGNITIVE_SEARCH_KEY'\",  \n                \"indexName\": \"'YOUR_AZURE_COGNITIVE_SEARCH_INDEX_NAME'\"  \n            }  \n        }  \n    ], \n    \"messages\": [  \n        {  \n            \"role\": \"user\",  \n            \"content\": \"What are the differences between Azure Machine  \nLearning and Azure Cognitive Services?\"  \n        }  \n    ] \n} \n' \nExample response\n{ \n    \"id\": \"12345678-1a2b-3c4e5f-a123-12345678abcd\" , \n    \"model\": \"\", \n    \"created\" : 1684304924 , \n    \"object\" : \"chat.completion\" , \n    \"choices\" : [ \n        {  \n            \"index\": 0,\n            \"messages\" : [ \n                {", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f1e37a840a44-0", "text": "Paramet ersType Requir ed? Default Descr iption\nmessages array Required null The messages to generate chat completions for,\nin the chat format.\ndataSourcesarray Required The data sources to be used for the Azure\nOpenAI on your data feature.\ntemperaturenumber Optional 0 What sampling temperature to use, between 0\nand 2. Higher values like 0.8 will make the\noutput more random, while lower values like 0.2\nwill make it more focused and deterministic. W e\ngenerally recommend altering this or top_p but\nnot both.\ntop_p number Optional 1 An alternative to sampling with temperature,\ncalled nucleus sampling, where the model\nconsiders the results of the tokens with top_p\nprobability mass. So 0.1 means only the tokens\ncomprising the top 10% probability mass are\nconsidered. W e generally recommend altering\nthis or temperature but not both.                    \"role\": \"tool\", \n                    \"content\" : \"{\\\"citations\\\": [{\\\"content\\\":  \n\\\"\\\\nCognitive Services are cloud-based artificial intelligence (AI)  \nservices...\\\", \\\"id\\\": null, \\\"title\\\": \\\"What is Cognitive Services\\\",  \n\\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal  \ndocument size=250. Scores=0.4314117431640625 and 1.72564697265625.Org  \nHighlight count=4.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"Learn about  \nAzure Cognitive Services.\\\\\\\"]\\\"}\" , \n                    \"end_turn\" : false \n                },  \n                {  \n                    \"role\": \"assistant\" , \n                    \"content\" : \" \\nAzure Cognitive Services are cloud-based  \nartificial intelligence (AI) services that help developers build cognitive", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f1e37a840a44-1", "text": "artificial intelligence (AI) services that help developers build cognitive  \nintelligence into applications without having direct AI or data science  \nskills or knowledge. [doc1]. Azure Machine Learning is a cloud service for  \naccelerating and managing the machine learning project lifecycle. [doc1].\" , \n                    \"end_turn\" : true \n                }  \n            ]  \n        }  \n    ] \n}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "32f667fd9b35-0", "text": "Paramet ersType Requir ed? Default Descr iption\nstream boolean Optional false If set, partial message deltas will be sent, like in\nChatGPT. T okens will be sent as data-only\nserver-sent events as they become available,\nwith the stream terminated by a message\n\"messages\": [{\"delta\": {\"content\": \"[DONE]\"},\n\"index\": 2, \"end_turn\": true}]\nstop string\nor arrayOptional null Up to 2 sequences where the API will stop\ngenerating further tokens.\nmax_tokens integer Optional 1000 The maximum number of tokens allowed for the\ngenerated answer. By default, the number of\ntokens the model can return is 4096 -\nprompt_tokens.\nThe following parameters can be used inside of the parameters field inside of\ndataSources.\nParamet ers Type Requir ed? Default Descr iption\ntype string Required null The data source to be used for the\nAzure OpenAI on your data feature.\nFor Azure Cognitive search the\nvalue is AzureCognitiveSearch.\nendpoint string Required null The data source endpoint.\nkey string Required null One of the Azure Cognitive Search\nadmin keys for your service.\nindexName string Required null The search index to be used.\nfieldsMapping dictionary Optional null Index data column mapping.\ninScope boolean Optional true If set, this value will limit responses\nspecific to the grounding data\ncontent.\ntopNDocuments number Optional 5 Number of documents that need to\nbe fetched for document\naugmentation.\nqueryType string Optional simple Indicates which query option will be\nused for Azure Cognitive Search.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "573f14789473-0", "text": "Paramet ers Type Requir ed? Default Descr iption\nsemanticConfigurationstring Optional null The semantic search configuration.\nOnly available when queryType is\nset to semantic.\nroleInformation string Optional null Gives the model instructions about\nhow it should behave and the\ncontext it should reference when\ngenerating a response.\nCorresponds to the \u201cS ystem\nMessage\u201d in Azure OpenAI S tudio.\nThere\u2019s a 100 token limit, which\ncounts towards the overall token\nlimit.\nGenerate a batch of images from a text caption. Image generation is currently only\navailable with api-version=2023-06-01-preview.\nHTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-resource-\nnamestring Required The name of your Azure OpenAI R esource.\napi-version string Required The API version to use for this operation. This follows the\nYYYY-MM-DD format.\nSuppor ted versions\n2023-06-01-preview\nRequest bodyImage generation\nRequest a generated image\nPOST https://{your-resource-\nname}.openai.azure.com/openai/images/generations:submit?api-version={api-\nversion}", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "bf7bf6dfe2ed-0", "text": "Paramet er Type Requir ed? Default Descr iption Paramet er Type Requir ed? Default Descr iption\nprompt string Required A text description of the desired image(s). The\nmaximum length is 1000 characters.\nn integer Optional 1 The number of images to generate. Must be\nbetween 1 and 5.\nsize string Optional 1024x1024 The size of the generated images. Must be one\nof 256x256, 512x512, or 1024x1024.\nConsole\nThe operation returns a 202 status code and an GenerateImagesResponse JSON object\ncontaining the ID and status of the operation.\nJSON\nUse this API to retrieve the results of an image generation operation. Image generation\nis currently only available with api-version=2023-06-01-preview.\nHTTPExample request\ncurl -X POST  \nhttps://YOUR_RESOURCE_NAME.openai.azure.com/openai/images/generations:submit\n?api-version=2023-06-01-preview \\\n  -H \"Content-Type: application/json\" \\  \n  -H \"api-key: YOUR_API_KEY\" \\  \n  -d '{ \n\"prompt\": \"An avocado chair\",  \n\"size\": \"512x512\",  \n\"n\": 3 \n}' \nExample response\n{ \n  \"id\": \"f508bcf2-e651-4b4b-85a7-58ad77981ffa\" , \n  \"status\" : \"notRunning\"  \n} \nGet a generated image result", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "5855ea054984-0", "text": "Path p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-resource-\nnamestring Required The name of your Azure OpenAI R esource.\noperation-id string Required The GUID that identifies the original image generation\nrequest.\nSuppor ted versions\n2023-06-01-preview\nConsole\nUpon success the operation returns a 200 status code and an OperationResponse JSON\nobject. The status field can be \"notRunning\" (task is queued but hasn't started yet),\n\"running\", \"succeeded\", \"canceled\" (task has timed out), \"failed\", or \"deleted\". A\nsucceeded status indicates that the generated image is available for download at the\ngiven URL. If multiple images were generated, their URLs are all returned in the\nresult.data field.\nJSONGET https://{your-resource-\nname}.openai.azure.com/openai/operations/images/{operation-id}?api-version=\n{api-version}  \nExample request\ncurl -X GET \"https://{your-resource-\nname}.openai.azure.com/openai/operations/images/{operation-id}?api-\nversion=2023-06-01-preview\"  \n-H \"Content-Type: application/json\"  \n-H \"Api-Key: {api key}\"  \nExample response\n{ \n  \"created\" : 1685064331 , \n  \"expires\" : 1685150737 , \n  \"id\": \"4b755937-3173-4b49-bf3f-da6702a3971a\" , \n  \"result\" : {", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2cb2b904ba0d-0", "text": "You can use the operation ID returned by the request to delete the corresponding\nimage from the Azure server. Generated images are automatically deleted after 24 hours\nby default, but you can trigger the deletion earlier if you want to.\nHTTP\nPath p aramet ers\nParamet er Type Requir ed? Descr iption\nyour-resource-\nnamestring Required The name of your Azure OpenAI R esource.\noperation-id string Required The GUID that identifies the original image generation\nrequest.\nSuppor ted versions\n2023-06-01-preview\nConsole    \"data\": [ \n      { \n        \"url\": \"<URL_TO_IMAGE>\"  \n      },  \n      { \n        \"url\": \"<URL_TO_NEXT_IMAGE>\"  \n      },  \n      ...  \n    ] \n  }, \n  \"status\" : \"succeeded\"  \n} \nDelete a generated image from the server\nDELETE https://{your-resource-\nname}.openai.azure.com/openai/operations/images/{operation-id}?api-version=\n{api-version}  \nExample request\ncurl -X DELETE \"https://{your-resource-\nname}.openai.azure.com/openai/operations/images/{operation-id}?api-\nversion=2023-06-01-preview\"", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8b297b8e982d-0", "text": "The operation returns a 204 status code if successful. This API only succeeds if the\noperation is in an end state (not running).\nAzure OpenAI is deployed as a part of the Azure Cognitive Services. All Cognitive\nServices rely on the same set of management APIs for creation, update and delete\noperations. The management APIs are also used for deploying models within an OpenAI\nresource.\nManagement APIs r eference documentation\nLearn about managing deployments, models, and fine-tuning with the REST API . Learn\nmore about the underlying models that power Azure OpenAI .-H \"Content-Type: application/json\"  \n-H \"Api-Key: {api key}\"  \nResponse\nManagement APIs\nNext steps", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "f81d14aff59b-0", "text": "Fine Tunes\nReference\nService: Cognitive Services\nAPI V ersion: 2022-12-01\nCancel Cancels the processing of the fine-tune job specified by the\ngiven fine-tune-id.\nCreate Creates a job that fine-tunes a specified model from a given\ntraining file. R esponse includes details of the enqueued job\nincluding job status and hyper paramete...\nDelete Deletes the fine-tune job specified by the given fine-tune-id.\nGet Gets details for a single fine-tune job specified by the given fine-\ntune-id. The details contain the base model, training and\nvalidation files, hyper parameters,...\nGet Events Gets the events for the fine-tune job specified by the given fine-\ntune-id. Events are created when the job status changes, e.g.\nrunning or complete, and when res...\nList Gets a list of all fine-tune jobs owned by the Azure OpenAI\nresource. The details that are returned for each fine-tune job\ncontain besides its identifier the base...Operations", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "015feb90e104-0", "text": "Deployments - Create Or Update\nReference\nService: Cognitive Services\nAPI V ersion: 2021-10-01\nUpdate the state of specified deployments associated with the Cognitive Services account.\nHTTP\nName In Requir ed Type Descr iption\naccountName path True string The name of Cognitive Services account.\nRegex pattern: ^[a-zA-Z0-9][a-zA-Z0-9_.-]*$\ndeployment Name path True string The name of the deployment associated with the Cognitive Services Account\nresource GroupName path True string The name of the resource group. The name is case insensitive.\nsubscription Id path True string The ID of the target subscription.\napi-version query True string The API version to use for this operation.\nName Type Descr iption\nproperties Deployment Properties Properties of Cognitive Services account deployment.\nName Type Descr iption\n200 OK Deployment Create/Update the deployment successfully.\n201 Created Deployment Create the deployment successfully.\nOther S tatus\nCodesError Response Error response describing why the operation failed.\nSample RequestPUT \nhttps://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Cognitiv\neServices/accounts/{accountName}/deployments/{deploymentName}?api-version=2021-10-01\nURI Parameters\nRequest Body\nResponses\nExamples\nPutDeployment\nHTTP", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d9d31c53bff2-0", "text": "HTTP\nSample Response\nStatus code: 200\nJSON\nStatus code: 201\nJSON\nName Descr iptionPUT \nhttps://management.azure.com/subscriptions/subscriptionId/resourceGroups/resourceGroupName/providers/Microsoft.Cognitiv\neServices/accounts/accountName/deployments/deploymentName?api-version=2021-10-01  \n{ \n  \"properties\" : { \n    \"model\": { \n      \"format\": \"OpenAI\",  \n      \"name\": \"ada\",  \n      \"version\" : \"1\" \n    }, \n    \"scaleSettings\" : { \n      \"scaleType\" : \"Manual\",  \n      \"capacity\" : 1 \n    } \n  } \n} \n{ \n  \"id\": \n\"/subscriptions/subscriptionId/resourceGroups/resourceGroupName/providers/Microsoft.CognitiveServices/accounts/accountName/\ndeployments/deploymentName\" , \n  \"name\": \"deploymentName\" , \n  \"type\": \"Microsoft.CognitiveServices/accounts/deployments\" , \n  \"properties\" : { \n    \"model\": { \n      \"format\" : \"OpenAI\" , \n      \"name\": \"ada\", \n      \"version\" : \"1\" \n    }, \n    \"scaleSettings\" : { \n      \"scaleType\" : \"Manual\" , \n      \"capacity\" : 1 \n    }, \n    \"provisioningState\" : \"Succeeded\"  \n  } \n}\n{ \n  \"id\": \n\"/subscriptions/subscriptionId/resourceGroups/resourceGroupName/providers/Microsoft.CognitiveServices/accounts/accountName/\ndeployments/deploymentName\" , \n  \"name\": \"deploymentName\" , \n  \"type\": \"Microsoft.CognitiveServices/accounts/deployments\" ,", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d9d31c53bff2-1", "text": "\"type\": \"Microsoft.CognitiveServices/accounts/deployments\" , \n  \"properties\" : { \n    \"model\": { \n      \"format\" : \"OpenAI\" , \n      \"name\": \"ada\", \n      \"version\" : \"1\" \n    }, \n    \"scaleSettings\" : { \n      \"scaleType\" : \"Manual\" , \n      \"capacity\" : 1 \n    }, \n    \"provisioningState\" : \"Accepted\"  \n  } \n}\nDefinitio ns", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "2437c7735e69-0", "text": "created ByType The type of identity that created the resource.\nDeployment Cognitive Services account deployment.\nDeployment Model Properties of Cognitive Services account deployment model.\nDeployment Properties Properties of Cognitive Services account deployment.\nDeployment Provisioning State Gets the status of the resource at the time the operation was called.\nDeployment Scale Settings Properties of Cognitive Services account deployment model.\nDeployment Scale Type Deployment scale type.\nError Additional Info The resource management error additional info.\nError Detail The error detail.\nError Response Error response\nsystem Data Metadata pertaining to creation and last modification of the resource.\nThe type of identity that created the resource.\nName Type Descr iption\nApplication string\nKey string\nManagedIdentity string\nUser string\nCognitive Services account deployment.\nName Type Descr iption\netag string Resource Etag.\nid string Fully qualified resource ID for the resource. Ex -\n/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceT ype}/\nname string The name of the resource\nproperties Deployment\nPropertiesProperties of Cognitive Services account deployment.\nsystemData system Data Metadata pertaining to creation and last modification of the resource.\ntype string The type of the resource. E.g. \"Microsoft.Compute/virtualMachines\" or \"Microsoft.S torage/storageAccounts\"createdByType\nDeployment", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b4a19dc7015e-0", "text": "Properties of Cognitive Services account deployment model.\nName Type Descr iption\nformat string Deployment model format.\nname string Deployment model name.\nversion string Deployment model version.\nProperties of Cognitive Services account deployment.\nName Type Descr iption\nmodel Deployment\nModelProperties of Cognitive Services account deployment model.\nprovisioningS tate Deployment\nProvisioning StateGets the status of the resource at the time the operation was called.\nscaleSettings Deployment\nScale SettingsProperties of Cognitive Services account deployment model.\nGets the status of the resource at the time the operation was called.\nName Type Descr iption\nAccepted string\nCreating string\nDeleting string\nFailed string\nMoving string\nSucceeded string\nProperties of Cognitive Services account deployment model.\nName Type Descr iption\ncapacity integer Deployment capacity.\nscaleT ype Deployment\nScale TypeDeployment scale type.DeploymentModel\nDeploymentProperties\nDeploymentProvisioningState\nDeploymentScaleSettings\nDeploymentScaleType", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "b0950ee32e54-0", "text": "Deployment scale type.\nName Type Descr iption\nManual string\nStandard string\nThe resource management error additional info.\nName Type Descr iption\ninfo object The additional info.\ntype string The additional info type.\nThe error detail.\nName Type Descr iption\nadditionalInfo Error Additional\nInfo[]The error additional info.\ncode string The error code.\ndetails Error Detail [] The error details.\nmessage string The error message.\ntarget string The error target.\nError response\nName Type Descr iption\nerror Error Detail The error object.\nMetadata pertaining to creation and last modification of the resource.\nName Type Descr iption\ncreatedAt string The timestamp of resource creation (UT C).\ncreatedBy string The identity that created the resource.\ncreatedByT ype created ByType The type of identity that created the resource.\nlastModifiedAt string The timestamp of resource last modification (UT C)ErrorAdditionalInfo\nErrorDetail\nErrorResponse\nsystemData", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "a51b069a47ad-0", "text": "lastModifiedBy string The identity that last modified the resource.\nlastModifiedByT ype created ByType The type of identity that last modified the resource.", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0f2921527e5d-0", "text": "Azure.AI.OpenAI Namespace\nReference\nChatChoice The representation of a single completions result for a chat\ncompletions request.\nChatCompletions A representation of a full response to a chat completions\nrequest.\nChatCompletions Options The configuration information used for a chat completions\nrequest.\nChatMessage A single, role-attributed message within a chat completion\ninteraction.\nChoice Choice model within completion response.\nCompletions Expected response schema to completion request.\nCompletions LogProbability LogProbs model within completion choice.\nCompletions Options Post body schema to create a prompt completion from a\ndeployment.\nCompletions Usage Representation of the token counts processed for a completions\nrequest. Counts consider all tokens across prompts, choices,\nchoice alternates, best_of generations, and other consumers.\nEmbedding Item Expected response schema to embeddings object list item\nrequest.\nEmbeddings Expected response schema to embeddings request.\nEmbeddings Options Schema to create a prompt completion from a deployment.\nEmbeddings Usage Measurment of the amount of tokens used in this request and\nresponse.\uff09 Impor tant\nSome information relates to prerelease product that may be substantially modified\nbefore it\u2019s released. Microsoft makes no warranties, express or implied, with respect\nto the information provided here.\nClasses", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0cc36260e14d-0", "text": "Open AIClient Azure OpenAI APIs for completions and search.\nOpen AIClient Options Client options for OpenAIClient.\nStreaming ChatChoice\nStreaming ChatCompletions\nStreaming Choice\nStreaming Completions\nChatRole A description of the intended purpose of a message within a\nchat completions interaction.\nOpen AIClient Options. Service\nVersionThe version of the service to use.Structs\nEnum s", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "885fe22cf172-0", "text": "com.azure.ai.openai\nReference\nPackage: com.azure.ai.openai\nMaven Artifact: com.azure:azure-ai-openai:1.0.0-beta.2\nPackage containing the classes for OpenAI. Azure OpenAI APIs for completions and\nsearch.\nOpen AIAsync Client Initializes a new instance of the asynchronous Open AIClient type.\nOpen AIClient Initializes a new instance of the synchronous Open AIClient type.\nOpen AIClient Builder A builder for creating a new instance of the Open AIClient type.\nOpen AIService Version Service version of Open AIClient.\nClasses\nEnum s", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8985b253e419-0", "text": "@azure/openai package\nReference\nAzure KeyCredential A static-key-based credential that supports updating the\nunderlying key value.\nOpen AIClient A client for interacting with Azure OpenAI.\nThe client needs the endpoint of an OpenAI resource and an\nauthentication method such as an API key or token. The API key\nand endpoint can be found in the OpenAI resource page. They\nwill be located in the resource's K eys and Endpoint page.\nExamples for authentication:\nAPI Key\nJavaScript\nAzure Active Directory\nJavaScriptClasses\nimport { OpenAIClient } from \"@azure/openai\" ; \nimport { AzureKeyCredential } from \n\"@azure/core-auth\" ; \nconst endpoint = \"<azure endpoint>\" ; \nconst credential = new AzureKeyCredential( \"<api \nkey>\"); \nconst client = new OpenAIClient(endpoint,  \ncredential);  \nimport { OpenAIClient } from \"@azure/openai\" ; \nimport { DefaultAzureCredential } from \n\"@azure/identity\" ; \nconst endpoint = \"<azure endpoint>\" ; \nconst credential = new \nDefaultAzureCredential();", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d9c17587a8fb-0", "text": "Open AIKeyCredential The OpenAIK eyCredential class represents an OpenAI API key\nand is used to authenticate into an OpenAI client for an OpenAI\nendpoint.\nChatChoice The representation of a single prompt completion as part of an\noverall chat completions request. Generally, n choices are\ngenerated per provided prompt with a default value of 1. T oken\nlimits and other settings may limit the number of choices\ngenerated.\nChatCompletions Representation of the response data from a chat completions\nrequest. Completions support a wide variety of tasks and\ngenerate text that continues from or \"completes\" provided\nprompt data.\nChatMessage A single, role-attributed message within a chat completion\ninteraction.\nChoice The representation of a single prompt completion as part of an\noverall completions request. Generally, n choices are generated\nper provided prompt with a default value of 1. T oken limits and\nother settings may limit the number of choices generated.\nCompletions Representation of the response data from a completions request.\nCompletions support a wide variety of tasks and generate text\nthat continues from or \"completes\" provided prompt data.\nCompletions LogProbability\nModelRepresentation of a log probabilities model for a completions\ngeneration.\nCompletions Usage Representation of the token counts processed for a completions\nrequest. Counts consider all tokens across prompts, choices,\nchoice alternates, best_of generations, and other consumers.\nEmbedding Item Representation of a single embeddings relatedness comparison.\nEmbeddings Representation of the response data from an embeddings\nrequest. Embeddings measure the relatedness of text strings and\nare commonly used for search, clustering, recommendations,\nand other similar scenarios.\nEmbeddings Usage Measurement of the amount of tokens used in this request and\nresponse.const client = new OpenAIClient(endpoint,  \ncredential);  \nInterfaces", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "e27c2b0d54ae-0", "text": "GetChatCompletions Options\nGetCompletions Options\nGetEmbeddings Options\nOpen AIClient Options\nRequest Options\nChatRole \"system\", \"assistant\", \"user\"\nCompletions Finish Reason \"stopped\", \"tokenLimitR eached\", \"contentFiltered\"Type Aliases", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "8229afa9cf04-0", "text": "Azure Cognitive Services support and\nhelp options\nArticle \u202207/22/2022\nAre you just starting to explore the functionality of Azure Cognitive Services? P erhaps\nyou are implementing a new feature in your application. Or after using the service, do\nyou have suggestions on how to improve it? Here are options for where you can get\nsupport, stay up-to-date, give feedback, and report bugs for Cognitive Services.\nExplore the range of Azure support options and choose the plan  that best fits,\nwhether you're a developer just starting your cloud journey or a large organization\ndeploying business-critical, strategic applications. Azure customers can create and\nmanage support requests in the Azure portal.\nAzure portal\nAzure portal for the United S tates government\nFor quick and reliable answers on your technical product questions from Microsoft\nEngineers, Azure Most V aluable Professionals (MVPs), or our expert community, engage\nwith us on Microsoft Q&A , Azure's preferred destination for community support.\nIf you can't find an answer to your problem using search, submit a new question to\nMicrosoft Q&A. Use one of the following tags when you ask your question:\nCognitive Services\nVision\nComputer Vision\nCustom Vision\nFace\nForm R ecognizer\nVideo Indexer\nLanguageCreate an Azure support request\nPost a question on Microsoft Q&A", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "1730f9f66f87-0", "text": "Immersive R eader\nLanguage Understanding (L UIS)\nQnA Maker\nLanguage service\nTranslator\nSpeech\nSpeech service\nDecision\nAnomaly Detector\nContent Moderator\nMetrics Advisor\nPersonalizer\nAzur e OpenAI\nAzure OpenAI\nFor answers on your developer questions from the largest community developer\necosystem, ask your question on S tack Overflow.\nIf you do submit a new question to S tack Overflow, please use one or more of the\nfollowing tags when you create the question:\nCognitive Services\nVision\nComputer Vision\nCustom Vision\nFace\nForm R ecognizer\nVideo Indexer\nLanguage\nImmersive R eader\nLanguage Understanding (L UIS)Post a question to Stack Overflow", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "0d7a47c5da67-0", "text": "QnA Maker\nLanguage service\nTranslator\nSpeech\nSpeech service\nDecision\nAnomaly Detector\nContent Moderator\nMetrics Advisor\nPersonalizer\nAzur e OpenAI\nAzure OpenAI\nTo request new features, post them on https://feedback.azure.com . Share your ideas\nfor making Cognitive Services and its APIs work better for the applications you develop.\nCognitive Services\nVision\nComputer Vision\nCustom Vision\nFace\nForm R ecognizer\nVideo Indexer\nLanguage\nImmersive R eader\nLanguage Understanding (L UIS)\nQnA Maker\nLanguage service\nTranslator\nSpeech\nSpeech service\nSubmit feedback", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
{"id": "d841fc827693-0", "text": "Decision\nAnomaly Detector\nContent Moderator\nMetrics Advisor\nPersonalizer\nStaying informed about features in a new release or news on the Azure blog can help\nyou find the difference between a programming error, a service bug, or a feature not yet\navailable in Cognitive Services.\nLearn more about product updates, roadmap, and announcements in Azure\nUpdates .\nNews about Cognitive Services is shared in the Azure blog .\nJoin the conversation on R eddit  about Cognitive Services.\nStay informed\nNext steps\nWhat ar e Azur e Cognitiv e Ser vices?", "source": "https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview"}
